[
{
	"uri": "/vi/6-data-ingestion/1-prepare-data/",
	"title": "Chuẩn bị dữ liệu mẫu",
	"tags": [],
	"description": "",
	"content": "Chuẩn bị dữ liệu mẫu Bước đầu tiên trong việc nạp dữ liệu là chuẩn bị dữ liệu mẫu để upload vào data lake. Trong phần này, chúng ta sẽ tạo hai loại dữ liệu mẫu:\nDữ liệu legislators: Chứa thông tin về các nhà lập pháp, bao gồm ID, tên, đảng phái, tiểu bang, và nhiệm kỳ. Dữ liệu transactions: Chứa thông tin về các giao dịch, bao gồm ID giao dịch, ID khách hàng, số tiền, loại tiền tệ, ngày giao dịch và danh mục. Cả hai loại dữ liệu này sẽ được lưu trữ dưới dạng JSON Lines, mỗi dòng là một đối tượng JSON hợp lệ.\nCác bước thực hiện Bước 1: Tạo thư mục cho dữ liệu mẫu Đầu tiên, chúng ta cần tạo một thư mục để lưu trữ các file dữ liệu mẫu. CloudShell cung cấp một môi trường làm việc tạm thời để chúng ta có thể tạo và quản lý các file.\nMở AWS Management Console và tìm kiếm \u0026ldquo;CloudShell\u0026rdquo;\nKhi CloudShell đã khởi động, nhập lệnh sau để tạo thư mục cho dữ liệu mẫu:\nmkdir -p ~/environment/sample-data cd ~/environment/sample-data Nhấn Enter để thực hiện lệnh CloudShell là một terminal dựa trên trình duyệt được quản lý bởi AWS, cho phép bạn chạy các lệnh AWS CLI mà không cần cài đặt bất kỳ phần mềm nào trên máy tính của bạn.\nBước 2: Tạo dữ liệu mẫu cho legislators Tiếp theo, chúng ta sẽ tạo dữ liệu mẫu cho legislators. Dữ liệu này sẽ được lưu trữ dưới dạng JSON Lines, với mỗi dòng là một đối tượng JSON đại diện cho một nhà lập pháp.\nTrong CloudShell, đảm bảo bạn đang ở thư mục ~/environment/sample-data\nNhập lệnh sau để tạo file dữ liệu mẫu cho legislators:\ncat \u0026gt; legislators.json \u0026lt;\u0026lt; \u0026#39;EOL\u0026#39; {\u0026#34;id\u0026#34;: \u0026#34;001\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;John Smith\u0026#34;, \u0026#34;party\u0026#34;: \u0026#34;Democratic\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;CA\u0026#34;, \u0026#34;chamber\u0026#34;: \u0026#34;Senate\u0026#34;, \u0026#34;start_date\u0026#34;: \u0026#34;2020-01-01\u0026#34;, \u0026#34;end_date\u0026#34;: \u0026#34;2026-01-01\u0026#34;, \u0026#34;is_current\u0026#34;: true, \u0026#34;term_length_days\u0026#34;: 2191} {\u0026#34;id\u0026#34;: \u0026#34;002\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Jane Doe\u0026#34;, \u0026#34;party\u0026#34;: \u0026#34;Republican\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;TX\u0026#34;, \u0026#34;chamber\u0026#34;: \u0026#34;House\u0026#34;, \u0026#34;start_date\u0026#34;: \u0026#34;2022-01-01\u0026#34;, \u0026#34;end_date\u0026#34;: \u0026#34;2024-01-01\u0026#34;, \u0026#34;is_current\u0026#34;: true, \u0026#34;term_length_days\u0026#34;: 730} {\u0026#34;id\u0026#34;: \u0026#34;003\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Robert Johnson\u0026#34;, \u0026#34;party\u0026#34;: \u0026#34;Democratic\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;NY\u0026#34;, \u0026#34;chamber\u0026#34;: \u0026#34;Senate\u0026#34;, \u0026#34;start_date\u0026#34;: \u0026#34;2018-01-01\u0026#34;, \u0026#34;end_date\u0026#34;: \u0026#34;2024-01-01\u0026#34;, \u0026#34;is_current\u0026#34;: true, \u0026#34;term_length_days\u0026#34;: 2191} {\u0026#34;id\u0026#34;: \u0026#34;004\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Emily Wilson\u0026#34;, \u0026#34;party\u0026#34;: \u0026#34;Republican\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;FL\u0026#34;, \u0026#34;chamber\u0026#34;: \u0026#34;House\u0026#34;, \u0026#34;start_date\u0026#34;: \u0026#34;2022-01-01\u0026#34;, \u0026#34;end_date\u0026#34;: \u0026#34;2024-01-01\u0026#34;, \u0026#34;is_current\u0026#34;: true, \u0026#34;term_length_days\u0026#34;: 730} {\u0026#34;id\u0026#34;: \u0026#34;005\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Michael Brown\u0026#34;, \u0026#34;party\u0026#34;: \u0026#34;Independent\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;OH\u0026#34;, \u0026#34;chamber\u0026#34;: \u0026#34;Senate\u0026#34;, \u0026#34;start_date\u0026#34;: \u0026#34;2020-01-01\u0026#34;, \u0026#34;end_date\u0026#34;: \u0026#34;2026-01-01\u0026#34;, \u0026#34;is_current\u0026#34;: true, \u0026#34;term_length_days\u0026#34;: 2191} EOL Nhấn Enter để thực hiện lệnh Lệnh trên sẽ tạo một file có tên legislators.json với 5 bản ghi. Mỗi bản ghi chứa các trường sau:\nid: Mã định danh duy nhất của nhà lập pháp name: Tên của nhà lập pháp party: Đảng phái chính trị (Democratic, Republican, Independent) state: Tiểu bang (mã 2 ký tự của tiểu bang) chamber: Viện (Senate hoặc House) start_date: Ngày bắt đầu nhiệm kỳ (dạng YYYY-MM-DD) end_date: Ngày kết thúc nhiệm kỳ (dạng YYYY-MM-DD) is_current: Trạng thái hiện tại (true hoặc false) term_length_days: Độ dài nhiệm kỳ tính bằng ngày Bước 3: Tạo dữ liệu mẫu cho transactions Sau khi đã tạo dữ liệu cho legislators, chúng ta sẽ tiếp tục tạo dữ liệu mẫu cho transactions. Dữ liệu này mô phỏng các giao dịch tài chính với các thông tin như ID giao dịch, ID khách hàng, số tiền, và danh mục giao dịch.\nTrong CloudShell, đảm bảo bạn vẫn đang ở thư mục ~/environment/sample-data\nNhập lệnh sau để tạo file dữ liệu mẫu cho transactions:\ncat \u0026gt; transactions.json \u0026lt;\u0026lt; \u0026#39;EOL\u0026#39; {\u0026#34;transaction_id\u0026#34;: \u0026#34;tx001\u0026#34;, \u0026#34;customer_id\u0026#34;: \u0026#34;cust001\u0026#34;, \u0026#34;amount\u0026#34;: 100.50, \u0026#34;currency\u0026#34;: \u0026#34;USD\u0026#34;, \u0026#34;transaction_date\u0026#34;: \u0026#34;2024-01-15\u0026#34;, \u0026#34;merchant_category\u0026#34;: \u0026#34;retail\u0026#34;} {\u0026#34;transaction_id\u0026#34;: \u0026#34;tx002\u0026#34;, \u0026#34;customer_id\u0026#34;: \u0026#34;cust002\u0026#34;, \u0026#34;amount\u0026#34;: 75.25, \u0026#34;currency\u0026#34;: \u0026#34;USD\u0026#34;, \u0026#34;transaction_date\u0026#34;: \u0026#34;2024-01-15\u0026#34;, \u0026#34;merchant_category\u0026#34;: \u0026#34;food\u0026#34;} {\u0026#34;transaction_id\u0026#34;: \u0026#34;tx003\u0026#34;, \u0026#34;customer_id\u0026#34;: \u0026#34;cust001\u0026#34;, \u0026#34;amount\u0026#34;: 200.00, \u0026#34;currency\u0026#34;: \u0026#34;USD\u0026#34;, \u0026#34;transaction_date\u0026#34;: \u0026#34;2024-01-16\u0026#34;, \u0026#34;merchant_category\u0026#34;: \u0026#34;travel\u0026#34;} {\u0026#34;transaction_id\u0026#34;: \u0026#34;tx004\u0026#34;, \u0026#34;customer_id\u0026#34;: \u0026#34;cust003\u0026#34;, \u0026#34;amount\u0026#34;: 50.75, \u0026#34;currency\u0026#34;: \u0026#34;USD\u0026#34;, \u0026#34;transaction_date\u0026#34;: \u0026#34;2024-01-16\u0026#34;, \u0026#34;merchant_category\u0026#34;: \u0026#34;retail\u0026#34;} {\u0026#34;transaction_id\u0026#34;: \u0026#34;tx005\u0026#34;, \u0026#34;customer_id\u0026#34;: \u0026#34;cust002\u0026#34;, \u0026#34;amount\u0026#34;: 120.00, \u0026#34;currency\u0026#34;: \u0026#34;USD\u0026#34;, \u0026#34;transaction_date\u0026#34;: \u0026#34;2024-01-17\u0026#34;, \u0026#34;merchant_category\u0026#34;: \u0026#34;entertainment\u0026#34;} {\u0026#34;transaction_id\u0026#34;: \u0026#34;tx006\u0026#34;, \u0026#34;customer_id\u0026#34;: \u0026#34;cust004\u0026#34;, \u0026#34;amount\u0026#34;: 300.25, \u0026#34;currency\u0026#34;: \u0026#34;USD\u0026#34;, \u0026#34;transaction_date\u0026#34;: \u0026#34;2024-01-17\u0026#34;, \u0026#34;merchant_category\u0026#34;: \u0026#34;travel\u0026#34;} {\u0026#34;transaction_id\u0026#34;: \u0026#34;tx007\u0026#34;, \u0026#34;customer_id\u0026#34;: \u0026#34;cust001\u0026#34;, \u0026#34;amount\u0026#34;: 45.00, \u0026#34;currency\u0026#34;: \u0026#34;USD\u0026#34;, \u0026#34;transaction_date\u0026#34;: \u0026#34;2024-01-18\u0026#34;, \u0026#34;merchant_category\u0026#34;: \u0026#34;food\u0026#34;} {\u0026#34;transaction_id\u0026#34;: \u0026#34;tx008\u0026#34;, \u0026#34;customer_id\u0026#34;: \u0026#34;cust005\u0026#34;, \u0026#34;amount\u0026#34;: 500.00, \u0026#34;currency\u0026#34;: \u0026#34;USD\u0026#34;, \u0026#34;transaction_date\u0026#34;: \u0026#34;2024-01-18\u0026#34;, \u0026#34;merchant_category\u0026#34;: \u0026#34;electronics\u0026#34;} {\u0026#34;transaction_id\u0026#34;: \u0026#34;tx009\u0026#34;, \u0026#34;customer_id\u0026#34;: \u0026#34;cust002\u0026#34;, \u0026#34;amount\u0026#34;: 25.50, \u0026#34;currency\u0026#34;: \u0026#34;USD\u0026#34;, \u0026#34;transaction_date\u0026#34;: \u0026#34;2024-01-19\u0026#34;, \u0026#34;merchant_category\u0026#34;: \u0026#34;food\u0026#34;} {\u0026#34;transaction_id\u0026#34;: \u0026#34;tx010\u0026#34;, \u0026#34;customer_id\u0026#34;: \u0026#34;cust003\u0026#34;, \u0026#34;amount\u0026#34;: 150.75, \u0026#34;currency\u0026#34;: \u0026#34;USD\u0026#34;, \u0026#34;transaction_date\u0026#34;: \u0026#34;2024-01-19\u0026#34;, \u0026#34;merchant_category\u0026#34;: \u0026#34;retail\u0026#34;} EOL Nhấn Enter để thực hiện lệnh Lệnh trên sẽ tạo một file có tên transactions.json với 10 bản ghi. Mỗi bản ghi chứa các trường sau:\ntransaction_id: Mã định danh duy nhất của giao dịch customer_id: Mã định danh của khách hàng thực hiện giao dịch amount: Số tiền giao dịch currency: Loại tiền tệ (USD, EUR, v.v.) transaction_date: Ngày thực hiện giao dịch (dạng YYYY-MM-DD) merchant_category: Danh mục của giao dịch (retail, food, travel, v.v.) Dữ liệu transactions này sẽ được sử dụng để mô phỏng dữ liệu giao dịch thực tế trong hệ thống tài chính. Trong môi trường thực tế, dữ liệu này có thể đến từ nhiều nguồn khác nhau như hệ thống thanh toán, ứng dụng di động, hoặc các kênh giao dịch khác.\nBước 4: Kiểm tra dữ liệu mẫu Sau khi đã tạo các file dữ liệu mẫu, chúng ta cần kiểm tra để đảm bảo dữ liệu đã được tạo đúng định dạng và có đầy đủ các trường cần thiết.\nTrong CloudShell, nhập lệnh sau để kiểm tra nội dung của các file dữ liệu mẫu: # Kiểm tra file legislators.json cat legislators.json # Kiểm tra file transactions.json cat transactions.json Nhấn Enter để thực hiện từng lệnh Bạn sẽ thấy nội dung của các file dữ liệu mẫu\nĐể kiểm tra số lượng bản ghi trong mỗi file, bạn có thể sử dụng lệnh wc -l:\n# Đếm số dòng trong file legislators.json wc -l legislators.json # Đếm số dòng trong file transactions.json wc -l transactions.json Kết quả sẽ cho thấy file legislators.json có 5 dòng và file transactions.json có 10 dòng Kiểm tra dữ liệu trước khi upload là một bước quan trọng để đảm bảo quá trình ETL sẽ hoạt động đúng cách. Nếu dữ liệu không đúng định dạng, các bước xử lý sau này có thể gặp lỗi.\nBước 5: Tạo script để tạo dữ liệu mẫu lớn hơn (tùy chọn) Trong môi trường thực tế, dữ liệu thường có khối lượng lớn hơn nhiều so với các file mẫu đơn giản mà chúng ta đã tạo. Để mô phỏng tốt hơn môi trường sản xuất, chúng ta có thể tạo một lượng dữ liệu lớn hơn bằng cách sử dụng script Python.\nNếu bạn muốn tạo dữ liệu mẫu lớn hơn, bạn có thể sử dụng script Python sau. Script này sẽ tạo 100 bản ghi legislators và 1000 bản ghi transactions với dữ liệu ngẫu nhiên: cat \u0026gt; generate_data.py \u0026lt;\u0026lt; \u0026#39;EOL\u0026#39; import json import random from datetime import datetime, timedelta import uuid # Tạo dữ liệu legislators def generate_legislators(count=100): parties = [\u0026#34;Democratic\u0026#34;, \u0026#34;Republican\u0026#34;, \u0026#34;Independent\u0026#34;] states = [\u0026#34;AL\u0026#34;, \u0026#34;AK\u0026#34;, \u0026#34;AZ\u0026#34;, \u0026#34;AR\u0026#34;, \u0026#34;CA\u0026#34;, \u0026#34;CO\u0026#34;, \u0026#34;CT\u0026#34;, \u0026#34;DE\u0026#34;, \u0026#34;FL\u0026#34;, \u0026#34;GA\u0026#34;, \u0026#34;HI\u0026#34;, \u0026#34;ID\u0026#34;, \u0026#34;IL\u0026#34;, \u0026#34;IN\u0026#34;, \u0026#34;IA\u0026#34;, \u0026#34;KS\u0026#34;, \u0026#34;KY\u0026#34;, \u0026#34;LA\u0026#34;, \u0026#34;ME\u0026#34;, \u0026#34;MD\u0026#34;, \u0026#34;MA\u0026#34;, \u0026#34;MI\u0026#34;, \u0026#34;MN\u0026#34;, \u0026#34;MS\u0026#34;, \u0026#34;MO\u0026#34;, \u0026#34;MT\u0026#34;, \u0026#34;NE\u0026#34;, \u0026#34;NV\u0026#34;, \u0026#34;NH\u0026#34;, \u0026#34;NJ\u0026#34;, \u0026#34;NM\u0026#34;, \u0026#34;NY\u0026#34;, \u0026#34;NC\u0026#34;, \u0026#34;ND\u0026#34;, \u0026#34;OH\u0026#34;, \u0026#34;OK\u0026#34;, \u0026#34;OR\u0026#34;, \u0026#34;PA\u0026#34;, \u0026#34;RI\u0026#34;, \u0026#34;SC\u0026#34;, \u0026#34;SD\u0026#34;, \u0026#34;TN\u0026#34;, \u0026#34;TX\u0026#34;, \u0026#34;UT\u0026#34;, \u0026#34;VT\u0026#34;, \u0026#34;VA\u0026#34;, \u0026#34;WA\u0026#34;, \u0026#34;WV\u0026#34;, \u0026#34;WI\u0026#34;, \u0026#34;WY\u0026#34;] chambers = [\u0026#34;Senate\u0026#34;, \u0026#34;House\u0026#34;] legislators = [] for i in range(1, count + 1): id_str = f\u0026#34;{i:03d}\u0026#34; party = random.choice(parties) state = random.choice(states) chamber = random.choice(chambers) start_year = random.randint(2018, 2022) start_date = f\u0026#34;{start_year}-01-01\u0026#34; term_years = 6 if chamber == \u0026#34;Senate\u0026#34; else 2 end_year = start_year + term_years end_date = f\u0026#34;{end_year}-01-01\u0026#34; term_length_days = term_years * 365 legislator = { \u0026#34;id\u0026#34;: id_str, \u0026#34;name\u0026#34;: f\u0026#34;Legislator {id_str}\u0026#34;, \u0026#34;party\u0026#34;: party, \u0026#34;state\u0026#34;: state, \u0026#34;chamber\u0026#34;: chamber, \u0026#34;start_date\u0026#34;: start_date, \u0026#34;end_date\u0026#34;: end_date, \u0026#34;is_current\u0026#34;: True, \u0026#34;term_length_days\u0026#34;: term_length_days } legislators.append(legislator) return legislators # Tạo dữ liệu transactions def generate_transactions(count=1000): merchant_categories = [\u0026#34;retail\u0026#34;, \u0026#34;food\u0026#34;, \u0026#34;travel\u0026#34;, \u0026#34;entertainment\u0026#34;, \u0026#34;electronics\u0026#34;, \u0026#34;healthcare\u0026#34;, \u0026#34;education\u0026#34;, \u0026#34;utilities\u0026#34;] currencies = [\u0026#34;USD\u0026#34;, \u0026#34;EUR\u0026#34;, \u0026#34;GBP\u0026#34;, \u0026#34;JPY\u0026#34;, \u0026#34;CAD\u0026#34;, \u0026#34;AUD\u0026#34;] transactions = [] for i in range(1, count + 1): transaction_id = f\u0026#34;tx{i:06d}\u0026#34; customer_id = f\u0026#34;cust{random.randint(1, 100):03d}\u0026#34; amount = round(random.uniform(10, 1000), 2) currency = random.choice(currencies) # Ngày giao dịch trong khoảng 30 ngày gần đây days_ago = random.randint(0, 30) transaction_date = (datetime.now() - timedelta(days=days_ago)).strftime(\u0026#34;%Y-%m-%d\u0026#34;) merchant_category = random.choice(merchant_categories) transaction = { \u0026#34;transaction_id\u0026#34;: transaction_id, \u0026#34;customer_id\u0026#34;: customer_id, \u0026#34;amount\u0026#34;: amount, \u0026#34;currency\u0026#34;: currency, \u0026#34;transaction_date\u0026#34;: transaction_date, \u0026#34;merchant_category\u0026#34;: merchant_category } transactions.append(transaction) return transactions # Tạo và lưu dữ liệu def save_data(data, filename): with open(filename, \u0026#39;w\u0026#39;) as f: for item in data: f.write(json.dumps(item) + \u0026#39;\\n\u0026#39;) # Tạo dữ liệu legislators = generate_legislators(100) transactions = generate_transactions(1000) # Lưu dữ liệu save_data(legislators, \u0026#39;legislators_large.json\u0026#39;) save_data(transactions, \u0026#39;transactions_large.json\u0026#39;) print(f\u0026#34;Generated {len(legislators)} legislators and {len(transactions)} transactions\u0026#34;) EOL # Cài đặt Python nếu cần pip install --user datetime # Chạy script python generate_data.py Nhấn Enter để thực hiện lệnh Bạn sẽ thấy thông báo về số lượng bản ghi đã được tạo Bước 6: Kiểm tra dữ liệu mẫu lớn (nếu đã tạo) Sau khi chạy script Python để tạo dữ liệu mẫu lớn hơn, chúng ta cần kiểm tra các file đã được tạo để đảm bảo chúng có đúng số lượng bản ghi như mong đợi.\nNếu bạn đã tạo dữ liệu mẫu lớn, nhập lệnh sau để kiểm tra số lượng bản ghi: # Đếm số dòng trong file legislators_large.json wc -l legislators_large.json # Đếm số dòng trong file transactions_large.json wc -l transactions_large.json Nhấn Enter để thực hiện từng lệnh Bạn sẽ thấy kết quả hiển thị số lượng bản ghi trong mỗi file:\nlegislators_large.json: 100 bản ghi transactions_large.json: 1000 bản ghi Bạn cũng có thể kiểm tra kích thước của các file bằng lệnh:\n# Kiểm tra kích thước của các file ls -lh legislators*.json transactions*.json Lệnh này sẽ hiển thị kích thước của cả các file dữ liệu mẫu nhỏ và lớn Việc tạo dữ liệu mẫu lớn hơn giúp kiểm tra hiệu suất của ETL pipeline và khả năng xử lý của hệ thống với khối lượng dữ liệu lớn hơn. Tuy nhiên, để tiết kiệm thời gian trong workshop này, bạn có thể chỉ sử dụng các file dữ liệu mẫu nhỏ hơn đã tạo ở các bước trước.\nXử lý sự cố Vấn đề: Lỗi \u0026ldquo;command not found: python\u0026rdquo; Giải pháp:\nSử dụng python3 thay vì python: python3 generate_data.py Hoặc cài đặt Python trong CloudShell: sudo yum install -y python3 Vấn đề: Lỗi \u0026ldquo;ModuleNotFoundError: No module named \u0026lsquo;datetime\u0026rsquo;\u0026rdquo; Giải pháp:\nCài đặt thư viện datetime: pip3 install --user datetime Vấn đề: Lỗi khi tạo file dữ liệu mẫu Giải pháp:\nKiểm tra quyền truy cập thư mục: ls -la ~/environment/sample-data Đảm bảo bạn có đủ dung lượng trống: df -h Thử tạo file với nội dung ngắn hơn: echo \u0026#39;{\u0026#34;test\u0026#34;: \u0026#34;data\u0026#34;}\u0026#39; \u0026gt; test.json Vấn đề: Lỗi \u0026ldquo;Permission denied\u0026rdquo; khi chạy script Python Giải pháp:\nThêm quyền thực thi cho script: chmod +x generate_data.py Bạn có thể tùy chỉnh dữ liệu mẫu theo nhu cầu của mình. Các file JSON được tạo ra có định dạng JSON Lines, với mỗi dòng là một đối tượng JSON hợp lệ. Định dạng này rất phổ biến trong các hệ thống xử lý dữ liệu lớn vì nó cho phép xử lý từng bản ghi một cách độc lập.\nTổng kết Trong phần này, chúng ta đã hoàn thành các bước sau:\nTạo thư mục cho dữ liệu mẫu trong CloudShell Tạo dữ liệu mẫu cho legislators với 5 bản ghi Tạo dữ liệu mẫu cho transactions với 10 bản ghi Kiểm tra dữ liệu mẫu đã tạo (Tùy chọn) Tạo dữ liệu mẫu lớn hơn với script Python Dữ liệu mẫu này sẽ được sử dụng trong bước tiếp theo để upload vào data lake và kích hoạt ETL pipeline.\nBước tiếp theo Tiếp theo, chúng ta sẽ Upload dữ liệu vào data lake để kích hoạt ETL pipeline và theo dõi quá trình xử lý dữ liệu.\n"
},
{
	"uri": "/vi/2-foundations/1-clone-repository/",
	"title": "Clone repository SDLF",
	"tags": [],
	"description": "",
	"content": "Clone repository SDLF Bước đầu tiên để triển khai SDLF Foundations là clone repository chính thức của SDLF từ GitHub.\nCác bước thực hiện Bước 1: Tạo thư mục làm việc trong CloudShell Trong CloudShell, nhập các lệnh sau để tạo và di chuyển đến thư mục làm việc: mkdir -p ~/environment/sdlf-workshop cd ~/environment/sdlf-workshop Nhấn Enter để thực hiện lệnh Bước 2: Clone repository SDLF từ GitHub Nhập lệnh sau để clone repository SDLF: git clone https://github.com/awslabs/aws-serverless-data-lake-framework.git Nhấn Enter để thực hiện lệnh Đợi quá trình clone hoàn tất. Bạn sẽ thấy thông báo tương tự như sau: Cloning into \u0026#39;aws-serverless-data-lake-framework\u0026#39;... remote: Enumerating objects: 3397, done. remote: Counting objects: 100% (1180/1180), done. remote: Compressing objects: 100% (578/578), done. remote: Total 3397 (delta 695), reused 926 (delta 544), pack-reused 2217 Receiving objects: 100% (3397/3397), 2.30 MiB | 5.66 MiB/s, done. Resolving deltas: 100% (2018/2018), done. Bước 3: Di chuyển vào thư mục SDLF Nhập lệnh sau để di chuyển vào thư mục SDLF: cd aws-serverless-data-lake-framework Nhấn Enter để thực hiện lệnh Bước 4: Kiểm tra cấu trúc thư mục Nhập lệnh sau để xem cấu trúc thư mục: ls -la Nhấn Enter để thực hiện lệnh Bạn sẽ thấy danh sách các thư mục và file trong repository, bao gồm: sdlf-foundations/: Chứa mã nguồn cho SDLF Foundations sdlf-team/: Chứa mã nguồn cho quản lý team sdlf-dataset/: Chứa mã nguồn cho quản lý dataset sdlf-pipelines/: Chứa mã nguồn cho các ETL pipeline sdlf-utils/: Chứa các công cụ tiện ích sdlf-cicd/: Chứa mã nguồn cho CI/CD pipeline Bước 5: Kiểm tra phiên bản SDLF Nhập lệnh sau để xem phiên bản SDLF: cat VERSION Nhấn Enter để thực hiện lệnh Bạn sẽ thấy phiên bản hiện tại của SDLF, ví dụ: 2.0.0 Xử lý sự cố Vấn đề: Lỗi \u0026ldquo;git command not found\u0026rdquo; Giải pháp:\nGit đã được cài đặt sẵn trong CloudShell Nếu gặp lỗi này, hãy thử đóng và mở lại CloudShell Vấn đề: Lỗi khi clone repository Giải pháp:\nKiểm tra kết nối internet Đảm bảo URL repository là chính xác Thử lại sau vài phút Nếu bạn muốn tìm hiểu thêm về cấu trúc của SDLF, hãy dành thời gian để khám phá các thư mục con và đọc các file README.md trong mỗi thư mục.\nBước tiếp theo Tiếp theo, chúng ta sẽ Cấu hình parameters để chuẩn bị cho việc triển khai SDLF Foundations.\n"
},
{
	"uri": "/vi/4-team-dataset/1-register-team/",
	"title": "Đăng ký team",
	"tags": [],
	"description": "",
	"content": "Đăng ký team Bước đầu tiên trong việc thiết lập cấu trúc dữ liệu là đăng ký team trong SDLF.\nCác bước thực hiện Bước 1: Lấy thông tin Data Lake Bucket Trong CloudShell, nhập lệnh sau để lấy tên của Data Lake bucket: export DATA_LAKE_BUCKET=$(aws cloudformation describe-stacks \\ --stack-name sdlf-foundations-stack \\ --query \u0026#39;Stacks[0].Outputs[?OutputKey==`DataLakeBucketName`].OutputValue\u0026#39; \\ --output text) echo \u0026#34;Data Lake Bucket: $DATA_LAKE_BUCKET\u0026#34; Nhấn Enter để thực hiện lệnh Bạn sẽ thấy tên của Data Lake bucket, ví dụ: Data Lake Bucket: sdlf-datalake-123456789012-us-east-1 Bước 2: Đăng ký team trong DynamoDB Nhập lệnh sau để đăng ký team trong bảng DynamoDB sdlf-teams: aws dynamodb put-item \\ --table-name sdlf-teams \\ --item \u0026#39;{ \u0026#34;name\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;team-a\u0026#34;}, \u0026#34;bucket\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;\u0026#39;$DATA_LAKE_BUCKET\u0026#39;\u0026#34;}, \u0026#34;prefix\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;team-a\u0026#34;} }\u0026#39; Nhấn Enter để thực hiện lệnh Nếu lệnh thực hiện thành công, bạn sẽ không thấy output nào Bước 3: Kiểm tra team đã đăng ký Nhập lệnh sau để kiểm tra team đã được đăng ký: aws dynamodb scan --table-name sdlf-teams Nhấn Enter để thực hiện lệnh Bạn sẽ thấy kết quả JSON với thông tin về team đã đăng ký, ví dụ: { \u0026#34;Items\u0026#34;: [ { \u0026#34;name\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;team-a\u0026#34; }, \u0026#34;bucket\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;sdlf-datalake-123456789012-us-east-1\u0026#34; }, \u0026#34;prefix\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;team-a\u0026#34; } } ], \u0026#34;Count\u0026#34;: 1, \u0026#34;ScannedCount\u0026#34;: 1, \u0026#34;ConsumedCapacity\u0026#34;: null } Bước 4: Tạo IAM Role cho team Nhập lệnh sau để tạo IAM role cho team: # Tạo trust policy cat \u0026gt; team-role-trust-policy.json \u0026lt;\u0026lt; \u0026#39;EOL\u0026#39; { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;glue.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] } EOL # Tạo IAM role aws iam create-role \\ --role-name sdlf-team-a-role \\ --assume-role-policy-document file://team-role-trust-policy.json # Gán AmazonS3FullAccess policy aws iam attach-role-policy \\ --role-name sdlf-team-a-role \\ --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess # Gán AWSGlueServiceRole policy aws iam attach-role-policy \\ --role-name sdlf-team-a-role \\ --policy-arn arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole Nhấn Enter để thực hiện từng lệnh Bạn sẽ thấy kết quả JSON với thông tin về IAM role đã được tạo Bước 5: Tạo cấu trúc thư mục trong S3 Nhập lệnh sau để tạo cấu trúc thư mục trong S3 bucket: # Tạo thư mục raw aws s3api put-object \\ --bucket $DATA_LAKE_BUCKET \\ --key raw/team-a/ # Tạo thư mục stage aws s3api put-object \\ --bucket $DATA_LAKE_BUCKET \\ --key stage/team-a/ # Tạo thư mục analytics aws s3api put-object \\ --bucket $DATA_LAKE_BUCKET \\ --key analytics/team-a/ Nhấn Enter để thực hiện từng lệnh Bạn sẽ thấy kết quả JSON với thông tin về các objects đã được tạo, bao gồm ETag Bước 6: Kiểm tra cấu trúc thư mục Nhập lệnh sau để kiểm tra cấu trúc thư mục đã tạo: aws s3 ls s3://$DATA_LAKE_BUCKET/ --recursive Nhấn Enter để thực hiện lệnh Bạn sẽ thấy danh sách các thư mục đã tạo: analytics/team-a/ raw/team-a/ stage/team-a/ Xử lý sự cố Vấn đề: Lỗi \u0026ldquo;ResourceNotFoundException\u0026rdquo; khi truy vấn CloudFormation stack Giải pháp:\nKiểm tra tên stack chính xác Đảm bảo bạn đang ở đúng region Kiểm tra xem stack đã được triển khai thành công chưa Vấn đề: Lỗi \u0026ldquo;ResourceNotFoundException\u0026rdquo; khi thao tác với DynamoDB Giải pháp:\nKiểm tra tên bảng chính xác Đảm bảo bảng đã được tạo trong phần SDLF Foundations Kiểm tra region của bảng Bạn có thể tạo nhiều teams khác nhau bằng cách lặp lại các bước trên với tên team khác nhau (ví dụ: team-b, team-c).\nBước tiếp theo Tiếp theo, chúng ta sẽ Tạo dataset để thiết lập cấu trúc dữ liệu cho team.\n"
},
{
	"uri": "/vi/1-prerequisite/1-aws-console/",
	"title": "Đăng nhập vào AWS Management Console",
	"tags": [],
	"description": "",
	"content": "Đăng nhập vào AWS Management Console Bước đầu tiên là đăng nhập vào AWS Management Console để bắt đầu workshop.\nCác bước thực hiện Bước 1: Mở trình duyệt web Mở trình duyệt web của bạn (Chrome, Firefox, Safari hoặc Edge) Nhập địa chỉ: https://console.aws.amazon.com Bước 2: Nhập thông tin đăng nhập Nếu bạn đã có tài khoản AWS:\nNhập địa chỉ email hoặc ID tài khoản AWS Nhấp vào nút Tiếp theo Nhập mật khẩu của bạn Nhấp vào nút Đăng nhập Nếu bạn là người dùng IAM:\nNhập ID tài khoản AWS hoặc alias Nhấp vào nút Tiếp theo Nhập tên người dùng IAM Nhập mật khẩu Nhấp vào nút Đăng nhập Bước 3: Xác minh đăng nhập thành công Sau khi đăng nhập thành công, bạn sẽ thấy:\nAWS Management Console hiển thị với các dịch vụ AWS Tên tài khoản của bạn ở góc trên bên phải Region hiện tại ở góc trên bên phải Xử lý sự cố Vấn đề: Không thể đăng nhập vào AWS Console Giải pháp:\nKiểm tra lại email/ID tài khoản và mật khẩu Sử dụng tùy chọn \u0026ldquo;Quên mật khẩu\u0026rdquo; nếu cần Liên hệ với quản trị viên AWS của bạn nếu vẫn gặp vấn đề Vấn đề: Thông báo \u0026ldquo;Bạn không có quyền truy cập vào trang này\u0026rdquo; Giải pháp:\nĐảm bảo bạn đang sử dụng tài khoản có quyền AdministratorAccess Kiểm tra IAM permissions của bạn Liên hệ với quản trị viên AWS để được cấp thêm quyền Nếu bạn đang sử dụng AWS Organizations, có thể bạn cần chọn đúng tài khoản hoặc role trước khi tiếp tục.\nBước tiếp theo Tiếp theo, chúng ta sẽ Mở AWS CloudShell để thực hiện các lệnh AWS CLI.\n"
},
{
	"uri": "/vi/",
	"title": "Serverless Data Lake Framework Jump Start",
	"tags": [],
	"description": "",
	"content": "Serverless Data Lake Framework Jump Start Tổng quan Workshop này hướng dẫn người học cách triển khai các dịch vụ serverless của AWS để xây dựng kiến trúc data lake hiện đại trên nền tảng AWS, đảm bảo khả năng mở rộng linh hoạt và sẵn sàng cho tương lai.\nServerless Data Lake Framework (SDLF) là một bộ công cụ bao gồm các thành phần infrastructure có thể tái sử dụng, được thiết kế để tăng tốc việc triển khai hệ thống data lake doanh nghiệp trên AWS, giúp giảm thời gian triển khai vào production từ nhiều tháng xuống chỉ còn vài tuần. SDLF tuân thủ các nguyên tắc của AWS Well-Architected Framework và mang lại nhiều lợi ích khác cho doanh nghiệp, như được mô tả chi tiết trong tài liệu.\nLayer Mô tả storage Các layer lưu trữ data lake với S3 và Lake Formation catalog Glue data catalog (databases và crawlers) processing Lambda functions và Glue jobs được trigger bởi EventBridge để xử lý dữ liệu consumption Athena workgroups để query và sử dụng dữ liệu orchestration Step Functions và EventBridge để điều phối các workflow xử lý governance and security Lake Formation, KMS Keys, và IAM Roles cho quản trị và bảo mật Mục tiêu Mục tiêu của chúng ta là minh họa cách dữ liệu thô có thể được lưu trữ, phân loại, chuyển đổi (sử dụng các phương pháp transformation nhẹ và/hoặc nặng), và được sử dụng bởi các ứng dụng cũng như end users.\nWorkshop này sử dụng dataset được tải xuống từ đây. Dataset chứa thông tin định dạng JSON về các nhà lập pháp Hoa Kỳ và các vị trí họ đã nắm giữ trong Hạ viện và Thượng viện Hoa Kỳ, và đã được chỉnh sửa nhẹ và cung cấp trong GitHub repository cho mục đích của workshop này.\nTrong workshop này, sử dụng SDLF, chúng ta sẽ chuẩn hóa và xử lý dữ liệu bằng các phương pháp transformation nhẹ và nặng, và cuối cùng làm cho dữ liệu có thể query được bởi end users thông qua Amazon Athena.\nCác bước thực hiện Các bước chuẩn bị Triển khai SDLF Foundations Thiết lập CI/CD Pipeline Đăng ký Team và Dataset Triển khai ETL Pipeline Nạp và xử lý dữ liệu Truy vấn dữ liệu với Athena Giám sát và xử lý sự cố Dọn dẹp tài nguyên "
},
{
	"uri": "/vi/3-cicd-pipeline/1-create-repositories/",
	"title": "Tạo CodeCommit repositories",
	"tags": [],
	"description": "",
	"content": "Tạo CodeCommit repositories Bước đầu tiên để thiết lập CI/CD Pipeline là tạo các CodeCommit repositories để lưu trữ mã nguồn của SDLF.\nCác bước thực hiện Bước 1: Tạo repositories trong CloudShell Trong CloudShell, nhập các lệnh sau để tạo các CodeCommit repositories: # Tạo repository cho SDLF Foundations aws codecommit create-repository \\ --repository-name sdlf-foundations \\ --repository-description \u0026#34;SDLF Foundations Repository\u0026#34; # Tạo repository cho SDLF Team aws codecommit create-repository \\ --repository-name sdlf-team \\ --repository-description \u0026#34;SDLF Team Repository\u0026#34; # Tạo repository cho SDLF Dataset aws codecommit create-repository \\ --repository-name sdlf-dataset \\ --repository-description \u0026#34;SDLF Dataset Repository\u0026#34; Nhấn Enter để thực hiện từng lệnh Bạn sẽ thấy kết quả JSON cho mỗi lệnh, hiển thị thông tin về repository đã được tạo, bao gồm: repositoryName: Tên repository repositoryId: ID của repository cloneUrlHttp: URL để clone repository qua HTTP cloneUrlSsh: URL để clone repository qua SSH Bước 2: Kiểm tra repositories đã tạo Nhập lệnh sau để liệt kê các repositories đã tạo: aws codecommit list-repositories Nhấn Enter để thực hiện lệnh Bạn sẽ thấy danh sách các repositories đã tạo, bao gồm: sdlf-foundations sdlf-team sdlf-dataset Bước 3: Cấu hình Git trong CloudShell Nhập các lệnh sau để cấu hình Git: git config --global user.name \u0026#34;Your Name\u0026#34; git config --global user.email \u0026#34;your.email@example.com\u0026#34; Thay thế \u0026ldquo;Your Name\u0026rdquo; và \u0026ldquo;your.email@example.com\u0026rdquo; bằng tên và email của bạn Nhấn Enter để thực hiện từng lệnh Bước 4: Chuẩn bị mã nguồn cho sdlf-foundations Nhập các lệnh sau để chuẩn bị mã nguồn cho sdlf-foundations: # Tạo thư mục làm việc mkdir -p ~/environment/sdlf-repos/sdlf-foundations cd ~/environment/sdlf-repos/sdlf-foundations # Sao chép mã nguồn từ repository gốc cp -r ~/environment/sdlf-workshop/aws-serverless-data-lake-framework/sdlf-foundations/* . # Khởi tạo Git repository git init git add . git commit -m \u0026#34;Initial commit for sdlf-foundations\u0026#34; Nhấn Enter để thực hiện từng lệnh Bước 5: Đẩy mã nguồn lên CodeCommit Nhập các lệnh sau để đẩy mã nguồn lên CodeCommit: # Lấy region hiện tại export AWS_REGION=$(aws configure get region) # Thêm remote repository git remote add origin https://git-codecommit.$AWS_REGION.amazonaws.com/v1/repos/sdlf-foundations # Đẩy mã nguồn lên git push -u origin master Nhấn Enter để thực hiện từng lệnh Nếu được yêu cầu nhập thông tin đăng nhập, hãy sử dụng AWS credentials của bạn Bước 6: Chuẩn bị và đẩy mã nguồn cho sdlf-team Nhập các lệnh sau để chuẩn bị và đẩy mã nguồn cho sdlf-team: # Tạo thư mục làm việc mkdir -p ~/environment/sdlf-repos/sdlf-team cd ~/environment/sdlf-repos/sdlf-team # Sao chép mã nguồn từ repository gốc cp -r ~/environment/sdlf-workshop/aws-serverless-data-lake-framework/sdlf-team/* . # Khởi tạo Git repository git init git add . git commit -m \u0026#34;Initial commit for sdlf-team\u0026#34; # Thêm remote repository và đẩy mã nguồn git remote add origin https://git-codecommit.$AWS_REGION.amazonaws.com/v1/repos/sdlf-team git push -u origin master Nhấn Enter để thực hiện từng lệnh Bước 7: Chuẩn bị và đẩy mã nguồn cho sdlf-dataset Nhập các lệnh sau để chuẩn bị và đẩy mã nguồn cho sdlf-dataset: # Tạo thư mục làm việc mkdir -p ~/environment/sdlf-repos/sdlf-dataset cd ~/environment/sdlf-repos/sdlf-dataset # Sao chép mã nguồn từ repository gốc cp -r ~/environment/sdlf-workshop/aws-serverless-data-lake-framework/sdlf-dataset/* . # Khởi tạo Git repository git init git add . git commit -m \u0026#34;Initial commit for sdlf-dataset\u0026#34; # Thêm remote repository và đẩy mã nguồn git remote add origin https://git-codecommit.$AWS_REGION.amazonaws.com/v1/repos/sdlf-dataset git push -u origin master Nhấn Enter để thực hiện từng lệnh Bước 8: Kiểm tra repositories trong AWS Management Console Mở tab mới trong trình duyệt Truy cập AWS Management Console Tìm kiếm và chọn \u0026ldquo;CodeCommit\u0026rdquo; Bạn sẽ thấy danh sách các repositories đã tạo Nhấp vào từng repository để xem mã nguồn đã được đẩy lên Xử lý sự cố Vấn đề: Lỗi \u0026ldquo;fatal: unable to access\u0026rdquo; khi đẩy mã nguồn Giải pháp:\nKiểm tra kết nối internet Đảm bảo AWS credentials của bạn có quyền truy cập CodeCommit Kiểm tra region trong URL repository Vấn đề: Lỗi \u0026ldquo;fatal: repository not found\u0026rdquo; khi đẩy mã nguồn Giải pháp:\nKiểm tra tên repository đã chính xác chưa Đảm bảo repository đã được tạo thành công Kiểm tra URL remote repository Nếu bạn gặp vấn đề với xác thực Git, bạn có thể sử dụng AWS CLI credential helper bằng cách chạy lệnh: git config --global credential.helper '!aws codecommit credential-helper $@'\nBước tiếp theo Tiếp theo, chúng ta sẽ Cấu hình CodeBuild để tự động build và test mã nguồn.\n"
},
{
	"uri": "/vi/5-etl-pipeline/1-create-glue-jobs/",
	"title": "Tạo Glue jobs",
	"tags": [],
	"description": "",
	"content": "Tạo Glue jobs Bước đầu tiên trong việc triển khai ETL Pipeline là tạo các Glue jobs để xử lý và chuyển đổi dữ liệu.\nCác bước thực hiện Bước 1: Tạo thư mục cho Glue scripts Trong CloudShell, nhập các lệnh sau để tạo thư mục cho Glue scripts: mkdir -p ~/environment/glue-scripts cd ~/environment/glue-scripts Nhấn Enter để thực hiện lệnh Bước 2: Tạo script cho stage-a Nhập lệnh sau để tạo script cho stage-a: cat \u0026gt; stage_a_job.py \u0026lt;\u0026lt; \u0026#39;EOL\u0026#39; import sys from awsglue.transforms import * from awsglue.utils import getResolvedOptions from pyspark.context import SparkContext from awsglue.context import GlueContext from awsglue.job import Job # Lấy parameters args = getResolvedOptions(sys.argv, [\u0026#39;JOB_NAME\u0026#39;, \u0026#39;team_name\u0026#39;, \u0026#39;dataset_name\u0026#39;, \u0026#39;bucket_name\u0026#39;]) # Khởi tạo Spark và Glue context sc = SparkContext() glueContext = GlueContext(sc) spark = glueContext.spark_session job = Job(glueContext) job.init(args[\u0026#39;JOB_NAME\u0026#39;], args) # Đường dẫn input và output input_path = f\u0026#34;s3://{args[\u0026#39;bucket_name\u0026#39;]}/raw/{args[\u0026#39;team_name\u0026#39;]}/{args[\u0026#39;dataset_name\u0026#39;]}/\u0026#34; output_path = f\u0026#34;s3://{args[\u0026#39;bucket_name\u0026#39;]}/stage/{args[\u0026#39;team_name\u0026#39;]}/{args[\u0026#39;dataset_name\u0026#39;]}/\u0026#34; print(f\u0026#34;Input path: {input_path}\u0026#34;) print(f\u0026#34;Output path: {output_path}\u0026#34;) # Đọc dữ liệu từ raw layer try: # Đọc dữ liệu JSON datasource = glueContext.create_dynamic_frame.from_options( \u0026#34;s3\u0026#34;, {\u0026#34;paths\u0026#34;: [input_path]}, format=\u0026#34;json\u0026#34; ) # Chuyển đổi sang DataFrame để dễ xử lý dataframe = datasource.toDF() # Thực hiện transform đơn giản # Ví dụ: Thêm timestamp from pyspark.sql.functions import current_timestamp dataframe = dataframe.withColumn(\u0026#34;processing_time\u0026#34;, current_timestamp()) # Ghi dữ liệu ra stage layer dataframe.write.mode(\u0026#34;append\u0026#34;).json(output_path) print(f\u0026#34;Processed {dataframe.count()} records\u0026#34;) except Exception as e: print(f\u0026#34;Error processing data: {str(e)}\u0026#34;) raise e job.commit() EOL Nhấn Enter để thực hiện lệnh Bước 3: Tạo script cho stage-b Nhập lệnh sau để tạo script cho stage-b: cat \u0026gt; stage_b_job.py \u0026lt;\u0026lt; \u0026#39;EOL\u0026#39; import sys from awsglue.transforms import * from awsglue.utils import getResolvedOptions from pyspark.context import SparkContext from awsglue.context import GlueContext from awsglue.job import Job # Lấy parameters args = getResolvedOptions(sys.argv, [\u0026#39;JOB_NAME\u0026#39;, \u0026#39;team_name\u0026#39;, \u0026#39;dataset_name\u0026#39;, \u0026#39;bucket_name\u0026#39;]) # Khởi tạo Spark và Glue context sc = SparkContext() glueContext = GlueContext(sc) spark = glueContext.spark_session job = Job(glueContext) job.init(args[\u0026#39;JOB_NAME\u0026#39;], args) # Đường dẫn input và output input_path = f\u0026#34;s3://{args[\u0026#39;bucket_name\u0026#39;]}/stage/{args[\u0026#39;team_name\u0026#39;]}/{args[\u0026#39;dataset_name\u0026#39;]}/\u0026#34; output_path = f\u0026#34;s3://{args[\u0026#39;bucket_name\u0026#39;]}/analytics/{args[\u0026#39;team_name\u0026#39;]}/{args[\u0026#39;dataset_name\u0026#39;]}/\u0026#34; print(f\u0026#34;Input path: {input_path}\u0026#34;) print(f\u0026#34;Output path: {output_path}\u0026#34;) # Đọc dữ liệu từ stage layer try: # Đọc dữ liệu JSON datasource = glueContext.create_dynamic_frame.from_options( \u0026#34;s3\u0026#34;, {\u0026#34;paths\u0026#34;: [input_path]}, format=\u0026#34;json\u0026#34; ) # Chuyển đổi sang DataFrame để dễ xử lý dataframe = datasource.toDF() # Thực hiện transform nâng cao # Ví dụ: Thêm partition columns from pyspark.sql.functions import year, month, dayofmonth, col # Giả sử có cột transaction_date với định dạng yyyy-MM-dd if \u0026#34;transaction_date\u0026#34; in dataframe.columns: dataframe = dataframe.withColumn(\u0026#34;year\u0026#34;, year(col(\u0026#34;transaction_date\u0026#34;))) dataframe = dataframe.withColumn(\u0026#34;month\u0026#34;, month(col(\u0026#34;transaction_date\u0026#34;))) dataframe = dataframe.withColumn(\u0026#34;day\u0026#34;, dayofmonth(col(\u0026#34;transaction_date\u0026#34;))) # Ghi dữ liệu ra analytics layer dataframe.write.partitionBy(\u0026#34;year\u0026#34;, \u0026#34;month\u0026#34;, \u0026#34;day\u0026#34;).mode(\u0026#34;append\u0026#34;).parquet(output_path) print(f\u0026#34;Processed {dataframe.count()} records\u0026#34;) except Exception as e: print(f\u0026#34;Error processing data: {str(e)}\u0026#34;) raise e job.commit() EOL Nhấn Enter để thực hiện lệnh Bước 4: Lấy thông tin Data Lake Bucket Nhập lệnh sau để lấy tên của Data Lake bucket: export DATA_LAKE_BUCKET=$(aws cloudformation describe-stacks \\ --stack-name sdlf-foundations-stack \\ --query \u0026#39;Stacks[0].Outputs[?OutputKey==`DataLakeBucketName`].OutputValue\u0026#39; \\ --output text) echo \u0026#34;Data Lake Bucket: $DATA_LAKE_BUCKET\u0026#34; Nhấn Enter để thực hiện lệnh Bước 5: Upload scripts lên S3 Nhập lệnh sau để tạo thư mục artifacts/glue-scripts trong S3 bucket: aws s3api put-object --bucket $DATA_LAKE_BUCKET --key artifacts/glue-scripts/ Nhấn Enter để thực hiện lệnh\nNhập lệnh sau để upload scripts lên S3:\naws s3 cp stage_a_job.py s3://$DATA_LAKE_BUCKET/artifacts/glue-scripts/ aws s3 cp stage_b_job.py s3://$DATA_LAKE_BUCKET/artifacts/glue-scripts/ Nhấn Enter để thực hiện lệnh Bước 6: Tạo IAM Role cho Glue Nhập lệnh sau để tạo IAM role cho Glue: # Tạo trust policy cat \u0026gt; glue-trust-policy.json \u0026lt;\u0026lt; \u0026#39;EOL\u0026#39; { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;glue.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] } EOL # Tạo IAM role aws iam create-role \\ --role-name sdlf-glue-role \\ --assume-role-policy-document file://glue-trust-policy.json # Gán AmazonS3FullAccess policy aws iam attach-role-policy \\ --role-name sdlf-glue-role \\ --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess # Gán AWSGlueServiceRole policy aws iam attach-role-policy \\ --role-name sdlf-glue-role \\ --policy-arn arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole Nhấn Enter để thực hiện từng lệnh Bước 7: Tạo Glue job cho stage-a Mở AWS Management Console và tìm kiếm \u0026ldquo;Glue\u0026rdquo; Trong menu bên trái, chọn \u0026ldquo;ETL\u0026rdquo; \u0026gt; \u0026ldquo;Jobs\u0026rdquo; Nhấp vào nút \u0026ldquo;Create job\u0026rdquo; Chọn \u0026ldquo;Spark script editor\u0026rdquo; Nhập thông tin cơ bản: Name: team-a-dataset-a-stage-a IAM role: sdlf-glue-role Glue version: Spark 3.1, Python 3 Worker type: G.1X Number of workers: 2 Job timeout: 30 minutes Trong phần \u0026ldquo;Script\u0026rdquo;, chọn \u0026ldquo;S3\u0026rdquo; và nhập đường dẫn:\ns3://$DATA_LAKE_BUCKET/artifacts/glue-scripts/stage_a_job.py Trong phần \u0026ldquo;Job parameters\u0026rdquo;, thêm các parameters sau:\nKey: \u0026ndash;team_name, Value: team-a Key: \u0026ndash;dataset_name, Value: dataset-a Key: \u0026ndash;bucket_name, Value: $DATA_LAKE_BUCKET Nhấp vào \u0026ldquo;Save\u0026rdquo; để lưu job\nBước 8: Tạo Glue job cho stage-b Lặp lại các bước 3-8 ở trên, nhưng với các thông tin sau: Name: team-a-dataset-a-stage-b Script: s3://$DATA_LAKE_BUCKET/artifacts/glue-scripts/stage_b_job.py Các parameters khác giữ nguyên Bước 9: Kiểm tra Glue jobs Trong AWS Management Console, trang Glue, chọn \u0026ldquo;Jobs\u0026rdquo; từ menu bên trái Bạn sẽ thấy hai jobs đã tạo: team-a-dataset-a-stage-a team-a-dataset-a-stage-b Xử lý sự cố Vấn đề: Lỗi \u0026ldquo;AccessDeniedException\u0026rdquo; khi tạo Glue job Giải pháp:\nKiểm tra IAM permissions của bạn Đảm bảo IAM role đã được tạo thành công Kiểm tra trust relationship của IAM role Vấn đề: Lỗi khi upload scripts lên S3 Giải pháp:\nKiểm tra tên bucket chính xác Đảm bảo bạn có quyền truy cập vào S3 bucket Kiểm tra nội dung của scripts Bạn có thể kiểm tra logs của Glue jobs trong CloudWatch Logs để debug nếu có lỗi xảy ra khi chạy jobs.\nBước tiếp theo Tiếp theo, chúng ta sẽ Thiết lập Step Functions để orchestrate workflow của ETL pipeline.\n"
},
{
	"uri": "/vi/7-athena-query/1-setup-workgroup/",
	"title": "Thiết lập Athena Workgroup",
	"tags": [],
	"description": "",
	"content": "Thiết lập Athena Workgroup Workgroup trong Amazon Athena giúp bạn tổ chức người dùng, teams, applications và quản lý việc truy vấn. Mỗi workgroup có thể có cấu hình riêng về vị trí lưu kết quả truy vấn, giới hạn dữ liệu quét và theo dõi chi phí.\nCác bước thực hiện Bước 1: Mở CloudShell Đăng nhập vào AWS Management Console Nhấp vào biểu tượng CloudShell ở thanh menu phía trên Đợi CloudShell khởi động hoàn tất Bước 2: Lấy tên S3 bucket của data lake Trong CloudShell, chạy lệnh sau để lấy tên S3 bucket từ CloudFormation outputs: DATA_LAKE_BUCKET=$(aws cloudformation describe-stacks \\ --stack-name sdlf-foundations-stack \\ --query \u0026#39;Stacks[0].Outputs[?OutputKey==`DataLakeBucketName`].OutputValue\u0026#39; \\ --output text) echo \u0026#34;Data Lake Bucket: $DATA_LAKE_BUCKET\u0026#34; Nhấn Enter để thực hiện lệnh Ghi lại tên bucket được hiển thị, chúng ta sẽ sử dụng trong các bước tiếp theo Bước 3: Tạo Athena workgroup Trong CloudShell, chạy lệnh sau để tạo workgroup cho analytics team: aws athena create-work-group \\ --name analytics-team-workgroup \\ --configuration \u0026#39;{ \u0026#34;ResultConfiguration\u0026#34;: { \u0026#34;OutputLocation\u0026#34;: \u0026#34;s3://\u0026#39;$DATA_LAKE_BUCKET\u0026#39;/athena-results/analytics-team/\u0026#34; }, \u0026#34;EnforceWorkGroupConfiguration\u0026#34;: true, \u0026#34;PublishCloudwatchMetricsEnabled\u0026#34;: true, \u0026#34;BytesScannedCutoffPerQuery\u0026#34;: 1073741824 }\u0026#39; \\ --description \u0026#34;Workgroup for analytics team queries\u0026#34; Nhấn Enter để thực hiện lệnh Bước 4: Xác nhận workgroup đã được tạo Trong CloudShell, chạy lệnh sau để liệt kê các workgroup: aws athena list-work-groups Nhấn Enter để thực hiện lệnh Xác nhận \u0026ldquo;analytics-team-workgroup\u0026rdquo; xuất hiện trong danh sách Bước 5: Kiểm tra cấu hình workgroup Trong CloudShell, chạy lệnh sau để xem chi tiết cấu hình của workgroup: aws athena get-work-group --work-group analytics-team-workgroup Nhấn Enter để thực hiện lệnh Xác nhận các cấu hình như OutputLocation, EnforceWorkGroupConfiguration, v.v. Xác nhận trên AWS Console Bạn cũng có thể xác nhận workgroup đã được tạo thông qua AWS Console:\nMở AWS Management Console và tìm kiếm \u0026ldquo;Athena\u0026rdquo; Trong menu bên trái, chọn \u0026ldquo;Workgroups\u0026rdquo; Xác nhận \u0026ldquo;analytics-team-workgroup\u0026rdquo; xuất hiện trong danh sách Nhấp vào tên workgroup để xem chi tiết cấu hình Xử lý sự cố Vấn đề: Lỗi \u0026ldquo;Access Denied\u0026rdquo; khi tạo workgroup Giải pháp:\nKiểm tra IAM permissions của bạn Đảm bảo bạn có quyền \u0026ldquo;athena:CreateWorkGroup\u0026rdquo; Kiểm tra quyền truy cập vào S3 bucket Vấn đề: Không thể tìm thấy tên bucket từ CloudFormation Giải pháp:\nKiểm tra tên stack CloudFormation chính xác là \u0026ldquo;sdlf-foundations-stack\u0026rdquo; Kiểm tra stack đã được triển khai thành công Kiểm tra output key chính xác là \u0026ldquo;DataLakeBucketName\u0026rdquo; Workgroup giúp bạn quản lý và theo dõi chi phí truy vấn Athena. Mỗi team hoặc dự án nên có workgroup riêng để dễ dàng phân tích chi phí.\nBước tiếp theo Tiếp theo, chúng ta sẽ Tạo và chạy Glue Crawler để phát hiện schema của dữ liệu trong data lake.\n"
},
{
	"uri": "/vi/3-cicd-pipeline/2-configure-codebuild/",
	"title": "Cấu hình CodeBuild",
	"tags": [],
	"description": "",
	"content": "Cấu hình CodeBuild Sau khi tạo các CodeCommit repositories, chúng ta cần cấu hình CodeBuild để tự động build và test mã nguồn.\nCác bước thực hiện Bước 1: Tạo buildspec file cho sdlf-foundations Trong CloudShell, nhập các lệnh sau để tạo buildspec file: cd ~/environment/sdlf-repos/sdlf-foundations cat \u0026gt; buildspec.yml \u0026lt;\u0026lt; \u0026#39;EOL\u0026#39; version: 0.2 phases: install: runtime-versions: python: 3.8 pre_build: commands: - echo \u0026#34;Starting pre-build phase\u0026#34; - pip install -r requirements.txt build: commands: - echo \u0026#34;Starting build phase\u0026#34; - chmod +x deploy.sh - ./deploy.sh post_build: commands: - echo \u0026#34;Build completed successfully\u0026#34; artifacts: files: - parameters-*.json - deploy.sh - template.yaml discard-paths: no EOL # Commit và push buildspec file git add buildspec.yml git commit -m \u0026#34;Add buildspec.yml for CodeBuild\u0026#34; git push Nhấn Enter để thực hiện lệnh Bước 2: Tạo IAM Role cho CodeBuild Nhập lệnh sau để tạo IAM role cho CodeBuild: # Tạo trust policy cat \u0026gt; codebuild-trust-policy.json \u0026lt;\u0026lt; \u0026#39;EOL\u0026#39; { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;codebuild.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] } EOL # Tạo IAM role aws iam create-role \\ --role-name sdlf-codebuild-role \\ --assume-role-policy-document file://codebuild-trust-policy.json # Gán AdministratorAccess policy (chỉ cho workshop, không khuyến nghị cho production) aws iam attach-role-policy \\ --role-name sdlf-codebuild-role \\ --policy-arn arn:aws:iam::aws:policy/AdministratorAccess Nhấn Enter để thực hiện từng lệnh Bước 3: Tạo CodeBuild project cho sdlf-foundations Nhập lệnh sau để tạo CodeBuild project: # Lấy account ID và region export AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) export AWS_REGION=$(aws configure get region) # Tạo CodeBuild project aws codebuild create-project \\ --name sdlf-foundations-build \\ --description \u0026#34;Build project for SDLF Foundations\u0026#34; \\ --source type=CODECOMMIT,location=https://git-codecommit.$AWS_REGION.amazonaws.com/v1/repos/sdlf-foundations \\ --artifacts type=NO_ARTIFACTS \\ --environment type=LINUX_CONTAINER,image=aws/codebuild/amazonlinux2-x86_64-standard:3.0,computeType=BUILD_GENERAL1_SMALL \\ --service-role arn:aws:iam::$AWS_ACCOUNT_ID:role/sdlf-codebuild-role Nhấn Enter để thực hiện lệnh Bạn sẽ thấy kết quả JSON với thông tin về CodeBuild project đã được tạo Bước 4: Tạo buildspec file cho sdlf-team Nhập các lệnh sau để tạo buildspec file cho sdlf-team: cd ~/environment/sdlf-repos/sdlf-team cat \u0026gt; buildspec.yml \u0026lt;\u0026lt; \u0026#39;EOL\u0026#39; version: 0.2 phases: install: runtime-versions: python: 3.8 pre_build: commands: - echo \u0026#34;Starting pre-build phase\u0026#34; - pip install -r requirements.txt build: commands: - echo \u0026#34;Starting build phase\u0026#34; - chmod +x deploy.sh - ./deploy.sh post_build: commands: - echo \u0026#34;Build completed successfully\u0026#34; artifacts: files: - parameters-*.json - deploy.sh - template.yaml discard-paths: no EOL # Commit và push buildspec file git add buildspec.yml git commit -m \u0026#34;Add buildspec.yml for CodeBuild\u0026#34; git push Nhấn Enter để thực hiện lệnh Bước 5: Tạo CodeBuild project cho sdlf-team Nhập lệnh sau để tạo CodeBuild project cho sdlf-team: aws codebuild create-project \\ --name sdlf-team-build \\ --description \u0026#34;Build project for SDLF Team\u0026#34; \\ --source type=CODECOMMIT,location=https://git-codecommit.$AWS_REGION.amazonaws.com/v1/repos/sdlf-team \\ --artifacts type=NO_ARTIFACTS \\ --environment type=LINUX_CONTAINER,image=aws/codebuild/amazonlinux2-x86_64-standard:3.0,computeType=BUILD_GENERAL1_SMALL \\ --service-role arn:aws:iam::$AWS_ACCOUNT_ID:role/sdlf-codebuild-role Nhấn Enter để thực hiện lệnh Bước 6: Tạo buildspec file và CodeBuild project cho sdlf-dataset Nhập các lệnh sau để tạo buildspec file và CodeBuild project cho sdlf-dataset: cd ~/environment/sdlf-repos/sdlf-dataset cat \u0026gt; buildspec.yml \u0026lt;\u0026lt; \u0026#39;EOL\u0026#39; version: 0.2 phases: install: runtime-versions: python: 3.8 pre_build: commands: - echo \u0026#34;Starting pre-build phase\u0026#34; - pip install -r requirements.txt build: commands: - echo \u0026#34;Starting build phase\u0026#34; - chmod +x deploy.sh - ./deploy.sh post_build: commands: - echo \u0026#34;Build completed successfully\u0026#34; artifacts: files: - parameters-*.json - deploy.sh - template.yaml discard-paths: no EOL # Commit và push buildspec file git add buildspec.yml git commit -m \u0026#34;Add buildspec.yml for CodeBuild\u0026#34; git push # Tạo CodeBuild project aws codebuild create-project \\ --name sdlf-dataset-build \\ --description \u0026#34;Build project for SDLF Dataset\u0026#34; \\ --source type=CODECOMMIT,location=https://git-codecommit.$AWS_REGION.amazonaws.com/v1/repos/sdlf-dataset \\ --artifacts type=NO_ARTIFACTS \\ --environment type=LINUX_CONTAINER,image=aws/codebuild/amazonlinux2-x86_64-standard:3.0,computeType=BUILD_GENERAL1_SMALL \\ --service-role arn:aws:iam::$AWS_ACCOUNT_ID:role/sdlf-codebuild-role Nhấn Enter để thực hiện lệnh Bước 7: Kiểm tra CodeBuild projects trong AWS Management Console Mở tab mới trong trình duyệt (hoặc sử dụng tab đã mở) Truy cập AWS Management Console Tìm kiếm và chọn \u0026ldquo;CodeBuild\u0026rdquo; Bạn sẽ thấy danh sách các CodeBuild projects đã tạo: sdlf-foundations-build sdlf-team-build sdlf-dataset-build Nhấp vào từng project để xem chi tiết cấu hình Xử lý sự cố Vấn đề: Lỗi \u0026ldquo;AccessDeniedException\u0026rdquo; khi tạo CodeBuild project Giải pháp:\nKiểm tra IAM permissions của bạn Đảm bảo IAM role đã được tạo thành công Kiểm tra trust relationship của IAM role Vấn đề: Lỗi \u0026ldquo;ResourceAlreadyExistsException\u0026rdquo; khi tạo CodeBuild project Giải pháp:\nKiểm tra xem project đã tồn tại chưa Sử dụng tên project khác hoặc xóa project hiện tại Trong môi trường production, bạn nên sử dụng IAM role với quyền hạn chế hơn thay vì AdministratorAccess. Chúng ta sử dụng AdministratorAccess trong workshop này để đơn giản hóa quá trình.\nBước tiếp theo Tiếp theo, chúng ta sẽ Thiết lập CodePipeline để tự động hóa toàn bộ quy trình CI/CD.\n"
},
{
	"uri": "/vi/2-foundations/2-configure-parameters/",
	"title": "Cấu hình parameters",
	"tags": [],
	"description": "",
	"content": "Cấu hình parameters Trước khi triển khai SDLF Foundations, chúng ta cần cấu hình các tham số cần thiết để phù hợp với môi trường của bạn.\nCác bước thực hiện Bước 1: Di chuyển đến thư mục sdlf-foundations Trong CloudShell, nhập lệnh sau để di chuyển đến thư mục sdlf-foundations: cd ~/environment/sdlf-workshop/aws-serverless-data-lake-framework/sdlf-foundations Nhấn Enter để thực hiện lệnh Bước 2: Xem file parameters mặc định Nhập lệnh sau để xem nội dung file parameters-dev.json: cat parameters-dev.json Nhấn Enter để thực hiện lệnh Bạn sẽ thấy nội dung của file parameters-dev.json, ví dụ: { \u0026#34;pOrganizationId\u0026#34;: \u0026#34;sdlf\u0026#34;, \u0026#34;pDataLakeName\u0026#34;: \u0026#34;sdlf-datalake\u0026#34;, \u0026#34;pEnvironment\u0026#34;: \u0026#34;dev\u0026#34;, \u0026#34;pRegion\u0026#34;: \u0026#34;us-east-1\u0026#34;, \u0026#34;pAccountId\u0026#34;: \u0026#34;123456789012\u0026#34;, \u0026#34;pLakeFormationAdminRoleName\u0026#34;: \u0026#34;AWSLakeFormationServiceRole\u0026#34;, \u0026#34;pLakeFormationAdminUserName\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;pLakeFormationAdminRoleArn\u0026#34;: \u0026#34;arn:aws:iam::123456789012:role/AWSLakeFormationServiceRole\u0026#34;, \u0026#34;pLakeFormationAdminUserArn\u0026#34;: \u0026#34;arn:aws:iam::123456789012:user/admin\u0026#34;, \u0026#34;pChildAccountId\u0026#34;: \u0026#34;123456789012\u0026#34;, \u0026#34;pSharedAccountId\u0026#34;: \u0026#34;123456789012\u0026#34;, \u0026#34;pSharedAccountRoleName\u0026#34;: \u0026#34;sdlf-crossaccount-role\u0026#34;, \u0026#34;pSharedAccountRoleArn\u0026#34;: \u0026#34;arn:aws:iam::123456789012:role/sdlf-crossaccount-role\u0026#34;, \u0026#34;pArtifactsBucket\u0026#34;: \u0026#34;sdlf-artifacts-123456789012-us-east-1\u0026#34;, \u0026#34;pStagingBucket\u0026#34;: \u0026#34;sdlf-staging-123456789012-us-east-1\u0026#34;, \u0026#34;pCentralBucket\u0026#34;: \u0026#34;sdlf-central-123456789012-us-east-1\u0026#34;, \u0026#34;pDataLakeBucket\u0026#34;: \u0026#34;sdlf-datalake-123456789012-us-east-1\u0026#34;, \u0026#34;pKMSKeyId\u0026#34;: \u0026#34;12345678-1234-1234-1234-123456789012\u0026#34; } Bước 3: Tạo bản sao của file parameters Nhập lệnh sau để tạo bản sao của file parameters-dev.json: cp parameters-dev.json parameters-dev.json.bak Nhấn Enter để thực hiện lệnh Bước 4: Lấy thông tin tài khoản AWS Nhập lệnh sau để lấy ID tài khoản AWS của bạn: export AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) echo \u0026#34;AWS Account ID: $AWS_ACCOUNT_ID\u0026#34; Nhấn Enter để thực hiện lệnh Nhập lệnh sau để lấy region hiện tại: export AWS_REGION=$(aws configure get region) echo \u0026#34;AWS Region: $AWS_REGION\u0026#34; Nhấn Enter để thực hiện lệnh Bước 5: Chỉnh sửa file parameters Nhập lệnh sau để mở file parameters-dev.json trong trình soạn thảo: nano parameters-dev.json Nhấn Enter để thực hiện lệnh Chỉnh sửa các tham số sau:\n\u0026quot;pOrganizationId\u0026quot;: Đặt thành \u0026ldquo;sdlf-workshop\u0026rdquo; \u0026quot;pDataLakeName\u0026quot;: Đặt thành \u0026ldquo;sdlf-datalake\u0026rdquo; \u0026quot;pEnvironment\u0026quot;: Giữ nguyên \u0026ldquo;dev\u0026rdquo; \u0026quot;pRegion\u0026quot;: Đặt thành region hiện tại của bạn (ví dụ: \u0026ldquo;us-east-1\u0026rdquo;) \u0026quot;pAccountId\u0026quot;: Đặt thành ID tài khoản AWS của bạn \u0026quot;pChildAccountId\u0026quot;: Đặt thành ID tài khoản AWS của bạn \u0026quot;pSharedAccountId\u0026quot;: Đặt thành ID tài khoản AWS của bạn Sau khi chỉnh sửa, nhấn Ctrl+X để thoát, nhấn Y để lưu thay đổi, và nhấn Enter để xác nhận tên file\nBước 6: Xác nhận thay đổi Nhập lệnh sau để xem nội dung file parameters-dev.json đã chỉnh sửa: cat parameters-dev.json Nhấn Enter để thực hiện lệnh Đảm bảo các tham số đã được cập nhật chính xác Bước 7: Tạo file cấu hình tự động (tùy chọn) Nếu bạn muốn tự động hóa việc cấu hình parameters, bạn có thể sử dụng lệnh sau: cat \u0026gt; update_parameters.sh \u0026lt;\u0026lt; \u0026#39;EOL\u0026#39; #!/bin/bash ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) REGION=$(aws configure get region) # Tạo bản sao của file parameters cp parameters-dev.json parameters-dev.json.bak # Cập nhật file parameters jq --arg account \u0026#34;$ACCOUNT_ID\u0026#34; --arg region \u0026#34;$REGION\u0026#34; \u0026#39; .pOrganizationId = \u0026#34;sdlf-workshop\u0026#34; | .pDataLakeName = \u0026#34;sdlf-datalake\u0026#34; | .pRegion = $region | .pAccountId = $account | .pChildAccountId = $account | .pSharedAccountId = $account | .pArtifactsBucket = \u0026#34;sdlf-artifacts-\\($account)-\\($region)\u0026#34; | .pStagingBucket = \u0026#34;sdlf-staging-\\($account)-\\($region)\u0026#34; | .pCentralBucket = \u0026#34;sdlf-central-\\($account)-\\($region)\u0026#34; | .pDataLakeBucket = \u0026#34;sdlf-datalake-\\($account)-\\($region)\u0026#34; \u0026#39; parameters-dev.json.bak \u0026gt; parameters-dev.json echo \u0026#34;Parameters updated successfully!\u0026#34; cat parameters-dev.json EOL chmod +x update_parameters.sh ./update_parameters.sh Nhấn Enter để thực hiện lệnh Nếu gặp lỗi \u0026ldquo;jq: command not found\u0026rdquo;, hãy cài đặt jq trước: sudo yum install -y jq Xử lý sự cố Vấn đề: Lỗi khi chỉnh sửa file với nano Giải pháp:\nBạn có thể sử dụng trình soạn thảo khác như vim: vim parameters-dev.json Hoặc sử dụng lệnh sed để thay thế trực tiếp: sed -i \u0026#34;s/\\\u0026#34;pOrganizationId\\\u0026#34;: \\\u0026#34;sdlf\\\u0026#34;/\\\u0026#34;pOrganizationId\\\u0026#34;: \\\u0026#34;sdlf-workshop\\\u0026#34;/\u0026#34; parameters-dev.json Vấn đề: Lỗi \u0026ldquo;jq: command not found\u0026rdquo; khi sử dụng script tự động Giải pháp:\nCài đặt jq: sudo yum install -y jq Hoặc sử dụng phương pháp chỉnh sửa thủ công với nano hoặc vim Đảm bảo bạn đã cập nhật chính xác ID tài khoản AWS và region. Các tham số này rất quan trọng cho việc triển khai SDLF Foundations.\nBước tiếp theo Tiếp theo, chúng ta sẽ Triển khai CloudFormation để tạo cơ sở hạ tầng cho SDLF Foundations.\n"
},
{
	"uri": "/vi/1-prerequisite/2-cloudshell/",
	"title": "Mở AWS CloudShell",
	"tags": [],
	"description": "",
	"content": "Mở AWS CloudShell AWS CloudShell là một shell dựa trên trình duyệt, được tích hợp sẵn các công cụ AWS CLI, giúp bạn thực hiện các lệnh AWS mà không cần cài đặt thêm công cụ trên máy tính cá nhân.\nCác bước thực hiện Bước 1: Tìm biểu tượng CloudShell Trong AWS Management Console, nhìn vào thanh điều hướng phía trên Tìm biểu tượng CloudShell (biểu tượng terminal) ở góc trên bên phải, bên cạnh tên tài khoản của bạn Nhấp vào biểu tượng CloudShell Bước 2: Đợi CloudShell khởi động Một cửa sổ mới sẽ mở ra ở phần dưới của màn hình Đợi CloudShell khởi động (có thể mất vài giây) Bạn sẽ thấy thông báo chào mừng và dấu nhắc lệnh Bước 3: Kiểm tra AWS CLI Khi CloudShell đã sẵn sàng, hãy nhập lệnh sau để kiểm tra phiên bản AWS CLI: aws --version Nhấn Enter để thực hiện lệnh Bạn sẽ thấy kết quả hiển thị phiên bản AWS CLI, ví dụ: aws-cli/2.13.5 Python/3.11.6 Linux/4.14.326-250.539.amzn2.x86_64 exe/x86_64.amzn.2 Bước 4: Kiểm tra quyền truy cập Nhập lệnh sau để xác nhận bạn có quyền truy cập cần thiết: aws sts get-caller-identity Nhấn Enter để thực hiện lệnh Bạn sẽ thấy kết quả hiển thị thông tin về tài khoản AWS của bạn, ví dụ: { \u0026#34;UserId\u0026#34;: \u0026#34;AIDAXXXXXXXXXXXXXXXX\u0026#34;, \u0026#34;Account\u0026#34;: \u0026#34;123456789012\u0026#34;, \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:iam::123456789012:user/your-username\u0026#34; } Các tính năng hữu ích của CloudShell Tải lên và tải xuống file:\nNhấp vào biểu tượng \u0026ldquo;Actions\u0026rdquo; (ba dấu chấm) ở góc trên bên phải của CloudShell Chọn \u0026ldquo;Upload\u0026rdquo; để tải file lên hoặc \u0026ldquo;Download\u0026rdquo; để tải file xuống Thay đổi kích thước cửa sổ:\nNhấp vào biểu tượng \u0026ldquo;Resize\u0026rdquo; (hai mũi tên) để thay đổi kích thước cửa sổ CloudShell Bạn có thể chọn \u0026ldquo;Full screen\u0026rdquo; để mở rộng toàn màn hình Mở tab mới:\nNhấp vào biểu tượng \u0026ldquo;New tab\u0026rdquo; (dấu +) để mở một tab CloudShell mới Xử lý sự cố Vấn đề: CloudShell không khởi động Giải pháp:\nLàm mới trang web Đảm bảo bạn có quyền truy cập vào dịch vụ CloudShell Thử đăng xuất và đăng nhập lại Vấn đề: Lệnh AWS CLI báo lỗi \u0026ldquo;Access Denied\u0026rdquo; Giải pháp:\nKiểm tra IAM permissions của bạn Đảm bảo bạn đang sử dụng tài khoản có quyền AdministratorAccess Kiểm tra lại region hiện tại CloudShell có sẵn trong hầu hết các region AWS, nhưng không phải tất cả. Nếu bạn không thấy biểu tượng CloudShell, hãy thử chuyển sang region khác.\nCloudShell lưu trữ các file của bạn trong một thư mục home có dung lượng 1GB. Dữ liệu này được giữ lại giữa các phiên làm việc nhưng có thể bị xóa sau một thời gian không hoạt động.\nBước tiếp theo Tiếp theo, chúng ta sẽ Chọn region phù hợp để thực hiện workshop.\n"
},
{
	"uri": "/vi/4-team-dataset/2-create-dataset/",
	"title": "Tạo dataset",
	"tags": [],
	"description": "",
	"content": "Tạo dataset Sau khi đã đăng ký team, chúng ta sẽ tạo dataset để thiết lập cấu trúc dữ liệu cho team.\nCác bước thực hiện Bước 1: Lấy thông tin Data Lake Bucket Trong CloudShell, nhập lệnh sau để lấy tên của Data Lake bucket (nếu bạn chưa thực hiện ở bước trước): export DATA_LAKE_BUCKET=$(aws cloudformation describe-stacks \\ --stack-name sdlf-foundations-stack \\ --query \u0026#39;Stacks[0].Outputs[?OutputKey==`DataLakeBucketName`].OutputValue\u0026#39; \\ --output text) echo \u0026#34;Data Lake Bucket: $DATA_LAKE_BUCKET\u0026#34; Nhấn Enter để thực hiện lệnh Bước 2: Tạo dataset trong DynamoDB Nhập lệnh sau để tạo dataset trong bảng DynamoDB sdlf-datasets: aws dynamodb put-item \\ --table-name sdlf-datasets \\ --item \u0026#39;{ \u0026#34;name\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;dataset-a\u0026#34;}, \u0026#34;team\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;team-a\u0026#34;}, \u0026#34;bucket\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;\u0026#39;$DATA_LAKE_BUCKET\u0026#39;\u0026#34;}, \u0026#34;transforms\u0026#34;: {\u0026#34;L\u0026#34;: [ {\u0026#34;S\u0026#34;: \u0026#34;stage-a\u0026#34;}, {\u0026#34;S\u0026#34;: \u0026#34;stage-b\u0026#34;} ]}, \u0026#34;min_items_process\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;1\u0026#34;}, \u0026#34;version\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;1\u0026#34;} }\u0026#39; Nhấn Enter để thực hiện lệnh Nếu lệnh thực hiện thành công, bạn sẽ không thấy output nào Bước 3: Kiểm tra dataset đã tạo Nhập lệnh sau để kiểm tra dataset đã được tạo: aws dynamodb scan --table-name sdlf-datasets Nhấn Enter để thực hiện lệnh Bạn sẽ thấy kết quả JSON với thông tin về dataset đã tạo, ví dụ: { \u0026#34;Items\u0026#34;: [ { \u0026#34;name\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;dataset-a\u0026#34; }, \u0026#34;team\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;team-a\u0026#34; }, \u0026#34;bucket\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;sdlf-datalake-123456789012-us-east-1\u0026#34; }, \u0026#34;transforms\u0026#34;: { \u0026#34;L\u0026#34;: [ { \u0026#34;S\u0026#34;: \u0026#34;stage-a\u0026#34; }, { \u0026#34;S\u0026#34;: \u0026#34;stage-b\u0026#34; } ] }, \u0026#34;min_items_process\u0026#34;: { \u0026#34;N\u0026#34;: \u0026#34;1\u0026#34; }, \u0026#34;version\u0026#34;: { \u0026#34;N\u0026#34;: \u0026#34;1\u0026#34; } } ], \u0026#34;Count\u0026#34;: 1, \u0026#34;ScannedCount\u0026#34;: 1, \u0026#34;ConsumedCapacity\u0026#34;: null } Bước 4: Tạo cấu trúc thư mục cho dataset Nhập lệnh sau để tạo cấu trúc thư mục cho dataset trong S3 bucket: # Tạo thư mục raw aws s3api put-object \\ --bucket $DATA_LAKE_BUCKET \\ --key raw/team-a/dataset-a/ # Tạo thư mục stage aws s3api put-object \\ --bucket $DATA_LAKE_BUCKET \\ --key stage/team-a/dataset-a/ # Tạo thư mục analytics aws s3api put-object \\ --bucket $DATA_LAKE_BUCKET \\ --key analytics/team-a/dataset-a/ Nhấn Enter để thực hiện từng lệnh Bạn sẽ thấy kết quả JSON với thông tin về các objects đã được tạo, bao gồm ETag Bước 5: Kiểm tra cấu trúc thư mục Nhập lệnh sau để kiểm tra cấu trúc thư mục đã tạo: aws s3 ls s3://$DATA_LAKE_BUCKET/ --recursive Nhấn Enter để thực hiện lệnh Bạn sẽ thấy danh sách các thư mục đã tạo, bao gồm: analytics/team-a/dataset-a/ raw/team-a/dataset-a/ stage/team-a/dataset-a/ Bước 6: Tạo Glue Database cho dataset Nhập lệnh sau để tạo Glue Database cho dataset: aws glue create-database \\ --database-input \u0026#39;{ \u0026#34;Name\u0026#34;: \u0026#34;team_a_db\u0026#34;, \u0026#34;Description\u0026#34;: \u0026#34;Database for Team A datasets\u0026#34;, \u0026#34;LocationUri\u0026#34;: \u0026#34;s3://\u0026#39;$DATA_LAKE_BUCKET\u0026#39;/analytics/team-a/\u0026#34; }\u0026#39; Nhấn Enter để thực hiện lệnh Nếu lệnh thực hiện thành công, bạn sẽ không thấy output nào Bước 7: Kiểm tra Glue Database Nhập lệnh sau để kiểm tra Glue Database đã tạo: aws glue get-database --name team_a_db Nhấn Enter để thực hiện lệnh Bạn sẽ thấy kết quả JSON với thông tin về Glue Database đã tạo Xử lý sự cố Vấn đề: Lỗi \u0026ldquo;ResourceNotFoundException\u0026rdquo; khi thao tác với DynamoDB Giải pháp:\nKiểm tra tên bảng chính xác Đảm bảo bảng đã được tạo trong phần SDLF Foundations Kiểm tra region của bảng Vấn đề: Lỗi \u0026ldquo;AccessDeniedException\u0026rdquo; khi tạo Glue Database Giải pháp:\nKiểm tra IAM permissions của bạn Đảm bảo bạn có quyền truy cập vào Glue Kiểm tra Lake Formation permissions Bạn có thể tạo nhiều datasets khác nhau cho cùng một team bằng cách lặp lại các bước trên với tên dataset khác nhau (ví dụ: dataset-b, dataset-c).\nBước tiếp theo Tiếp theo, chúng ta sẽ Thiết lập quyền truy cập để quản lý quyền truy cập vào dữ liệu.\n"
},
{
	"uri": "/vi/7-athena-query/2-create-crawler/",
	"title": "Tạo và chạy Glue Crawler",
	"tags": [],
	"description": "",
	"content": "Tạo và chạy Glue Crawler AWS Glue Crawler quét dữ liệu của bạn trong Amazon S3, phát hiện schema và tạo các bảng trong AWS Glue Data Catalog. Các bảng này sau đó có thể được truy vấn bằng Amazon Athena.\nCác bước thực hiện Bước 1: Mở AWS Glue Console Đăng nhập vào AWS Management Console Tìm kiếm \u0026ldquo;Glue\u0026rdquo; trong thanh tìm kiếm Chọn \u0026ldquo;AWS Glue\u0026rdquo; từ kết quả tìm kiếm Bước 2: Tạo Glue Crawler Trong menu bên trái, chọn \u0026ldquo;Crawlers\u0026rdquo; Nhấp vào nút \u0026ldquo;Create crawler\u0026rdquo; Bước 3: Cấu hình thông tin crawler Nhập thông tin cơ bản: Crawler name: team-a-dataset-a-crawler Mô tả (tùy chọn): Crawler for Team A's dataset A Nhấp vào \u0026ldquo;Next\u0026rdquo; Bước 4: Chọn data source Chọn \u0026ldquo;Data source type\u0026rdquo;: S3 Chọn \u0026ldquo;Crawl data in\u0026rdquo;: Specified path in my account Nhập \u0026ldquo;Include path\u0026rdquo;: s3://\u0026lt;DATA_LAKE_BUCKET\u0026gt;/post-stage/team-a/dataset-a/ (Thay \u0026lt;DATA_LAKE_BUCKET\u0026gt; bằng tên bucket thực tế từ bước trước) Nhấp vào \u0026ldquo;Next\u0026rdquo; Bước 5: Cấu hình IAM role Chọn \u0026ldquo;Create an IAM role\u0026rdquo; Nhập tên role: AWSGlueServiceRole-team-a-crawler Hoặc chọn \u0026ldquo;Choose an existing IAM role\u0026rdquo; và chọn role có tên chứa \u0026ldquo;GlueServiceRole\u0026rdquo; Nhấp vào \u0026ldquo;Next\u0026rdquo; Bước 6: Cấu hình output và scheduling Chọn \u0026ldquo;Target database\u0026rdquo;: Chọn \u0026ldquo;Add database\u0026rdquo; Nhập tên database: team_a_db Nhấp vào \u0026ldquo;Create\u0026rdquo; Nhập \u0026ldquo;Table name prefix\u0026rdquo;: transactions_ Chọn \u0026ldquo;Frequency\u0026rdquo;: Run on demand Nhấp vào \u0026ldquo;Next\u0026rdquo; Bước 7: Xem lại và tạo crawler Xem lại tất cả các cấu hình Nhấp vào \u0026ldquo;Create crawler\u0026rdquo; Bước 8: Chạy crawler Đợi crawler được tạo thành công Chọn crawler \u0026ldquo;team-a-dataset-a-crawler\u0026rdquo; trong danh sách Nhấp vào \u0026ldquo;Run crawler\u0026rdquo; Bước 9: Theo dõi tiến trình crawler Theo dõi trạng thái của crawler trong danh sách Đợi trạng thái chuyển từ \u0026ldquo;Starting\u0026rdquo; sang \u0026ldquo;Running\u0026rdquo; và cuối cùng là \u0026ldquo;Ready\u0026rdquo; Quá trình này có thể mất vài phút Bước 10: Xác nhận tables đã được tạo Trong menu bên trái, chọn \u0026ldquo;Databases\u0026rdquo; Chọn database \u0026ldquo;team_a_db\u0026rdquo; Chọn \u0026ldquo;Tables\u0026rdquo; để xem danh sách tables Xác nhận có table bắt đầu bằng \u0026ldquo;transactions_\u0026rdquo; Xử lý sự cố Vấn đề: Crawler không tìm thấy dữ liệu Giải pháp:\nKiểm tra đường dẫn S3 chính xác Đảm bảo dữ liệu đã được nạp vào bucket Kiểm tra IAM role có đủ quyền truy cập S3 Vấn đề: Crawler chạy thành công nhưng không tạo tables Giải pháp:\nKiểm tra định dạng dữ liệu (CSV, JSON, Parquet, v.v.) Đảm bảo dữ liệu có cấu trúc nhất quán Kiểm tra logs của crawler trong CloudWatch Nếu dữ liệu của bạn được phân vùng (ví dụ: theo ngày), Glue Crawler sẽ tự động phát hiện cấu trúc phân vùng và tạo các cột phân vùng tương ứng trong schema.\nBước tiếp theo Tiếp theo, chúng ta sẽ Thực hiện truy vấn SQL để phân tích dữ liệu trong data lake.\n"
},
{
	"uri": "/vi/5-etl-pipeline/2-setup-step-functions/",
	"title": "Thiết lập Step Functions",
	"tags": [],
	"description": "",
	"content": "Thiết lập Step Functions Sau khi đã tạo các Glue jobs, chúng ta sẽ thiết lập AWS Step Functions để orchestrate workflow của ETL pipeline.\nCác bước thực hiện Bước 1: Tạo định nghĩa state machine Trong CloudShell, nhập lệnh sau để tạo file định nghĩa state machine: cat \u0026gt; state-machine.json \u0026lt;\u0026lt; \u0026#39;EOL\u0026#39; { \u0026#34;Comment\u0026#34;: \u0026#34;ETL Pipeline for team-a dataset-a\u0026#34;, \u0026#34;StartAt\u0026#34;: \u0026#34;Stage A\u0026#34;, \u0026#34;States\u0026#34;: { \u0026#34;Stage A\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:states:::glue:startJobRun.sync\u0026#34;, \u0026#34;Parameters\u0026#34;: { \u0026#34;JobName\u0026#34;: \u0026#34;team-a-dataset-a-stage-a\u0026#34;, \u0026#34;Arguments\u0026#34;: { \u0026#34;--team_name.$\u0026#34;: \u0026#34;$.team\u0026#34;, \u0026#34;--dataset_name.$\u0026#34;: \u0026#34;$.dataset\u0026#34;, \u0026#34;--bucket_name.$\u0026#34;: \u0026#34;$.bucket\u0026#34; } }, \u0026#34;Next\u0026#34;: \u0026#34;Stage B\u0026#34; }, \u0026#34;Stage B\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:states:::glue:startJobRun.sync\u0026#34;, \u0026#34;Parameters\u0026#34;: { \u0026#34;JobName\u0026#34;: \u0026#34;team-a-dataset-a-stage-b\u0026#34;, \u0026#34;Arguments\u0026#34;: { \u0026#34;--team_name.$\u0026#34;: \u0026#34;$.team\u0026#34;, \u0026#34;--dataset_name.$\u0026#34;: \u0026#34;$.dataset\u0026#34;, \u0026#34;--bucket_name.$\u0026#34;: \u0026#34;$.bucket\u0026#34; } }, \u0026#34;End\u0026#34;: true } } } EOL Nhấn Enter để thực hiện lệnh Bước 2: Tạo IAM Role cho Step Functions Nhập lệnh sau để tạo IAM role cho Step Functions: # Tạo trust policy cat \u0026gt; step-functions-trust-policy.json \u0026lt;\u0026lt; \u0026#39;EOL\u0026#39; { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;states.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] } EOL # Tạo IAM role aws iam create-role \\ --role-name sdlf-step-functions-role \\ --assume-role-policy-document file://step-functions-trust-policy.json # Tạo policy cho Step Functions cat \u0026gt; step-functions-policy.json \u0026lt;\u0026lt; \u0026#39;EOL\u0026#39; { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;glue:StartJobRun\u0026#34;, \u0026#34;glue:GetJobRun\u0026#34;, \u0026#34;glue:GetJobRuns\u0026#34;, \u0026#34;glue:BatchStopJobRun\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } EOL # Tạo policy aws iam create-policy \\ --policy-name sdlf-step-functions-policy \\ --policy-document file://step-functions-policy.json # Lấy Account ID export AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) # Gán policy cho role aws iam attach-role-policy \\ --role-name sdlf-step-functions-role \\ --policy-arn arn:aws:iam::$AWS_ACCOUNT_ID:policy/sdlf-step-functions-policy Nhấn Enter để thực hiện từng lệnh Bước 3: Tạo Step Functions state machine Nhập lệnh sau để tạo Step Functions state machine: # Lấy ARN của IAM role export ROLE_ARN=$(aws iam get-role \\ --role-name sdlf-step-functions-role \\ --query \u0026#39;Role.Arn\u0026#39; \\ --output text) # Tạo state machine aws stepfunctions create-state-machine \\ --name team-a-dataset-a-pipeline \\ --definition file://state-machine.json \\ --role-arn $ROLE_ARN Nhấn Enter để thực hiện lệnh Bạn sẽ thấy kết quả JSON với thông tin về state machine đã được tạo, bao gồm ARN của state machine Bước 4: Kiểm tra state machine trong AWS Management Console Mở AWS Management Console và tìm kiếm \u0026ldquo;Step Functions\u0026rdquo; Trong danh sách state machines, tìm \u0026ldquo;team-a-dataset-a-pipeline\u0026rdquo; Nhấp vào state machine để xem chi tiết Bạn sẽ thấy biểu đồ trực quan của workflow, hiển thị các states và transitions Bước 5: Chạy thử state machine Trong trang chi tiết của state machine, nhấp vào nút \u0026ldquo;Start execution\u0026rdquo; Trong hộp thoại \u0026ldquo;Start execution\u0026rdquo;, nhập input JSON sau: { \u0026#34;team\u0026#34;: \u0026#34;team-a\u0026#34;, \u0026#34;dataset\u0026#34;: \u0026#34;dataset-a\u0026#34;, \u0026#34;bucket\u0026#34;: \u0026#34;REPLACE_WITH_YOUR_BUCKET_NAME\u0026#34; } Thay thế \u0026ldquo;REPLACE_WITH_YOUR_BUCKET_NAME\u0026rdquo; bằng tên Data Lake bucket của bạn Nhấp vào \u0026ldquo;Start execution\u0026rdquo; Bạn sẽ thấy execution đang chạy, với biểu đồ trực quan hiển thị tiến trình Execution có thể mất vài phút để hoàn thành. Nếu không có dữ liệu trong thư mục raw/team-a/dataset-a/, Glue job vẫn sẽ chạy nhưng không có dữ liệu nào được xử lý.\nBước 6: Kiểm tra kết quả execution Sau khi execution hoàn thành, bạn sẽ thấy trạng thái của từng state (Succeeded hoặc Failed) Nhấp vào từng state để xem chi tiết Bạn có thể xem input và output của mỗi state Xử lý sự cố Vấn đề: Lỗi \u0026ldquo;AccessDeniedException\u0026rdquo; khi tạo state machine Giải pháp:\nKiểm tra IAM permissions của bạn Đảm bảo IAM role đã được tạo thành công Kiểm tra trust relationship của IAM role Vấn đề: Execution fails với lỗi \u0026ldquo;Glue job not found\u0026rdquo; Giải pháp:\nKiểm tra tên Glue job chính xác Đảm bảo Glue job đã được tạo thành công Kiểm tra IAM role của Step Functions có quyền truy cập Glue không Bạn có thể xem logs của Step Functions executions trong CloudWatch Logs để debug nếu có lỗi xảy ra.\nBước tiếp theo Tiếp theo, chúng ta sẽ Cấu hình EventBridge để tự động kích hoạt ETL pipeline khi có dữ liệu mới.\n"
},
{
	"uri": "/vi/6-data-ingestion/2-upload-data/",
	"title": "Upload dữ liệu",
	"tags": [],
	"description": "",
	"content": "Upload dữ liệu Sau khi đã chuẩn bị dữ liệu mẫu, chúng ta sẽ upload dữ liệu vào data lake để kích hoạt ETL pipeline. Đây là bước quan trọng trong quy trình xử lý dữ liệu, nơi dữ liệu thô được đưa vào hệ thống và bắt đầu quá trình chuyển đổi.\nQuy trình nạp dữ liệu và ETL Khi dữ liệu được upload vào S3 bucket trong thư mục /raw, các sự kiện sau sẽ xảy ra:\nEventBridge phát hiện sự kiện S3 PutObject và kích hoạt Step Functions state machine Step Functions điều phối quy trình ETL thông qua các bước: Kiểm tra metadata của file Chạy Glue job Stage A để xử lý sơ bộ dữ liệu Lưu dữ liệu đã xử lý vào thư mục /stage Chạy Glue job Stage B để xử lý thêm dữ liệu Lưu dữ liệu đã xử lý vào thư mục /analytics Cập nhật metadata và catalog Trong phần này, chúng ta sẽ upload dữ liệu và theo dõi toàn bộ quá trình này.\nCác bước thực hiện Bước 1: Lấy tên của Data Lake bucket Để upload dữ liệu vào S3 bucket, trước tiên chúng ta cần biết chính xác tên của Data Lake bucket đã được tạo trong phần Foundations. Tên bucket này được lưu trong outputs của CloudFormation stack.\nMở AWS CloudShell (nếu chưa mở)\nTrong CloudShell, nhập lệnh sau để lấy tên của Data Lake bucket và lưu vào biến môi trường:\nexport DATA_LAKE_BUCKET=$(aws cloudformation describe-stacks \\ --stack-name sdlf-foundations-stack \\ --query \u0026#39;Stacks[0].Outputs[?OutputKey==`DataLakeBucketName`].OutputValue\u0026#39; \\ --output text) echo \u0026#34;Data Lake Bucket: $DATA_LAKE_BUCKET\u0026#34; Nhấn Enter để thực hiện lệnh Bạn sẽ thấy tên của Data Lake bucket, ví dụ: Data Lake Bucket: sdlf-datalake-123456789012-us-east-1 Lệnh trên thực hiện các bước sau: Sử dụng AWS CLI để truy vấn thông tin của CloudFormation stack sdlf-foundations-stack Lọc kết quả để lấy giá trị của output có key là DataLakeBucketName Lưu giá trị này vào biến môi trường DATA_LAKE_BUCKET Hiển thị tên bucket để xác nhận Biến môi trường DATA_LAKE_BUCKET sẽ được sử dụng trong các lệnh tiếp theo để tránh phải nhập lại tên bucket dài và phức tạp nhiều lần.\nBước 2: Upload dữ liệu legislators vào S3 Bây giờ chúng ta sẽ upload file dữ liệu legislators.json vào S3 bucket trong thư mục /raw. Đường dẫn đầy đủ sẽ là /raw/team-a/dataset-a/legislators/, tuân theo cấu trúc thư mục đã được thiết lập trong SDLF.\nĐảm bảo bạn đang ở thư mục chứa dữ liệu mẫu: cd ~/environment/sample-data Kiểm tra file dữ liệu legislators.json có tồn tại không: ls -la legislators.json Upload file legislators.json vào S3 bucket: # Upload file legislators.json aws s3 cp legislators.json s3://$DATA_LAKE_BUCKET/raw/team-a/dataset-a/legislators/ Nhấn Enter để thực hiện lệnh Bạn sẽ thấy thông báo xác nhận upload thành công, ví dụ: upload: ./legislators.json to s3://sdlf-datalake-123456789012-us-east-1/raw/team-a/dataset-a/legislators/legislators.json Bạn có thể kiểm tra xác nhận file đã được upload lên S3: aws s3 ls s3://$DATA_LAKE_BUCKET/raw/team-a/dataset-a/legislators/ Khi file được upload vào thư mục /raw, EventBridge sẽ tự động phát hiện sự kiện này và kích hoạt Step Functions state machine để bắt đầu quá trình ETL.\nBước 3: Kiểm tra ETL pipeline cho legislators Sau khi upload file dữ liệu, ETL pipeline sẽ được kích hoạt tự động. Chúng ta cần kiểm tra Step Functions để theo dõi quá trình xử lý dữ liệu.\nMở AWS Management Console và tìm kiếm \u0026ldquo;Step Functions\u0026rdquo; trong thanh tìm kiếm\nNhấp vào dịch vụ \u0026ldquo;Step Functions\u0026rdquo; trong kết quả tìm kiếm\nTrong trang Step Functions, bạn sẽ thấy danh sách các state machines. Tìm state machine có tên \u0026ldquo;team-a-dataset-a-pipeline\u0026rdquo;\nNhấp vào state machine để xem chi tiết\nTrong tab \u0026ldquo;Executions\u0026rdquo;, bạn sẽ thấy một execution mới đã được tạo tự động sau khi upload file\nNhấp vào execution để xem chi tiết và theo dõi tiến trình\nBạn sẽ thấy biểu đồ trực quan của quy trình ETL với các bước đang được thực hiện hoặc đã hoàn thành\nĐợi cho đến khi execution hoàn thành (trạng thái \u0026ldquo;Succeeded\u0026rdquo;). Quá trình này có thể mất vài phút\nStep Functions cung cấp giao diện trực quan để theo dõi quá trình ETL. Mỗi bước trong quy trình được hiển thị với trạng thái cụ thể (Running, Succeeded, Failed). Bạn có thể nhấp vào từng bước để xem thông tin chi tiết hơn.\nBước 4: Kiểm tra dữ liệu đã xử lý Sau khi ETL pipeline hoàn thành, dữ liệu sẽ được xử lý và lưu trữ trong các layer khác nhau của data lake. Chúng ta cần kiểm tra các layer này để xác nhận dữ liệu đã được xử lý đúng cách.\nQuay lại CloudShell và đảm bảo biến DATA_LAKE_BUCKET vẫn còn được định nghĩa: echo $DATA_LAKE_BUCKET Nếu biến không còn giá trị, hãy chạy lại lệnh ở Bước 1 để lấy lại tên bucket\nKiểm tra dữ liệu đã được xử lý trong stage layer:\n# Kiểm tra dữ liệu trong stage layer aws s3 ls s3://$DATA_LAKE_BUCKET/stage/team-a/dataset-a/legislators/ Kiểm tra dữ liệu đã được xử lý trong analytics layer: # Kiểm tra dữ liệu trong analytics layer aws s3 ls s3://$DATA_LAKE_BUCKET/analytics/team-a/dataset-a/legislators/ Nhấn Enter để thực hiện từng lệnh Bạn sẽ thấy danh sách các files đã được tạo trong mỗi layer. Các file này có thể có tên khác với file gốc và có thể được lưu trữ dưới dạng Parquet hoặc các định dạng khác tùy thuộc vào cấu hình của ETL pipeline\nĐể xem nội dung của một file cụ thể, bạn có thể sử dụng lệnh sau (thay thế \u0026lt;file_name\u0026gt; bằng tên file thực tế):\n# Tải file về máy để xem nội dung aws s3 cp s3://$DATA_LAKE_BUCKET/analytics/team-a/dataset-a/legislators/\u0026lt;file_name\u0026gt; ./ cat \u0026lt;file_name\u0026gt; Dữ liệu trong stage layer là dữ liệu đã qua bước xử lý đầu tiên (Stage A), trong khi dữ liệu trong analytics layer là dữ liệu đã qua bước xử lý thứ hai (Stage B) và sẵn sàng cho việc phân tích.\nBước 5: Upload dữ liệu transactions vào S3 Sau khi đã kiểm tra quá trình xử lý dữ liệu legislators, chúng ta sẽ tiếp tục với dữ liệu transactions. Quy trình tương tự như với dữ liệu legislators, nhưng chúng ta sẽ upload vào thư mục /raw/team-a/dataset-a/transactions/.\nĐảm bảo bạn vẫn đang ở thư mục chứa dữ liệu mẫu: cd ~/environment/sample-data Kiểm tra file dữ liệu transactions.json có tồn tại không: ls -la transactions.json Upload file transactions.json vào S3 bucket: # Upload file transactions.json aws s3 cp transactions.json s3://$DATA_LAKE_BUCKET/raw/team-a/dataset-a/transactions/ Nhấn Enter để thực hiện lệnh Bạn sẽ thấy thông báo xác nhận upload thành công, ví dụ: upload: ./transactions.json to s3://sdlf-datalake-123456789012-us-east-1/raw/team-a/dataset-a/transactions/transactions.json Bạn có thể kiểm tra xác nhận file đã được upload lên S3: aws s3 ls s3://$DATA_LAKE_BUCKET/raw/team-a/dataset-a/transactions/ Giống như với dữ liệu legislators, việc upload file transactions.json sẽ tự động kích hoạt một execution mới của ETL pipeline. Mỗi file được upload sẽ tạo ra một execution riêng biệt.\nBước 6: Kiểm tra ETL pipeline cho transactions Sau khi upload file transactions.json, chúng ta cần kiểm tra ETL pipeline để đảm bảo quá trình xử lý dữ liệu transactions cũng được kích hoạt và hoạt động đúng cách.\nQuay lại trang Step Functions trong AWS Management Console (hoặc mở lại nếu đã đóng)\nĐi đến state machine \u0026ldquo;team-a-dataset-a-pipeline\u0026rdquo; như đã làm ở Bước 3\nTrong danh sách executions, bạn sẽ thấy một execution mới đã được tạo tự động sau khi upload file transactions.json\nNhấp vào execution mới nhất để xem chi tiết và theo dõi tiến trình\nBạn sẽ thấy biểu đồ trực quan của quy trình ETL tương tự như với dữ liệu legislators\nĐợi cho đến khi execution hoàn thành (trạng thái \u0026ldquo;Succeeded\u0026rdquo;). Quá trình này có thể mất vài phút\nBạn có thể xem thông tin chi tiết của từng bước trong quy trình bằng cách nhấp vào các nút trong biểu đồ\nBạn có thể so sánh các execution khác nhau bằng cách xem thời gian thực hiện và trạng thái của từng bước. Điều này giúp bạn hiểu rõ hơn về hiệu suất của ETL pipeline với các loại dữ liệu khác nhau.\nBước 7: Kiểm tra dữ liệu transactions đã xử lý Sau khi ETL pipeline cho transactions hoàn thành, chúng ta cần kiểm tra dữ liệu đã được xử lý trong các layer khác nhau của data lake, tương tự như đã làm với dữ liệu legislators.\nQuay lại CloudShell và kiểm tra dữ liệu transactions đã được xử lý trong stage layer: # Kiểm tra dữ liệu trong stage layer aws s3 ls s3://$DATA_LAKE_BUCKET/stage/team-a/dataset-a/transactions/ Kiểm tra dữ liệu transactions đã được xử lý trong analytics layer: # Kiểm tra dữ liệu trong analytics layer aws s3 ls s3://$DATA_LAKE_BUCKET/analytics/team-a/dataset-a/transactions/ Nhấn Enter để thực hiện từng lệnh Bạn sẽ thấy danh sách các files đã được tạo trong mỗi layer\nĐể so sánh cấu trúc thư mục giữa các layer, bạn có thể sử dụng lệnh sau:\n# So sánh cấu trúc thư mục giữa các layer echo \u0026#34;=== RAW LAYER ===\u0026#34; aws s3 ls s3://$DATA_LAKE_BUCKET/raw/team-a/dataset-a/ --recursive | head -10 echo \u0026#34;\\n=== STAGE LAYER ===\u0026#34; aws s3 ls s3://$DATA_LAKE_BUCKET/stage/team-a/dataset-a/ --recursive | head -10 echo \u0026#34;\\n=== ANALYTICS LAYER ===\u0026#34; aws s3 ls s3://$DATA_LAKE_BUCKET/analytics/team-a/dataset-a/ --recursive | head -10 Lệnh này sẽ hiển thị cấu trúc thư mục và các file trong mỗi layer, giúp bạn hiểu rõ hơn về cách dữ liệu được tổ chức trong data lake Dữ liệu trong analytics layer thường được tổ chức theo cấu trúc tối ưu cho truy vấn, có thể bao gồm phân vứng theo các trường như ngày, danh mục, v.v. Điều này giúp cải thiện hiệu suất truy vấn khi sử dụng các công cụ phân tích như Athena.\nBước 8: Upload dữ liệu mẫu lớn (tùy chọn) Nếu bạn đã tạo dữ liệu mẫu lớn hơn bằng script Python ở phần trước, bạn có thể upload chúng vào data lake để kiểm tra hiệu suất của ETL pipeline với khối lượng dữ liệu lớn hơn.\nĐảm bảo bạn vẫn đang ở thư mục chứa dữ liệu mẫu: cd ~/environment/sample-data Kiểm tra các file dữ liệu mẫu lớn có tồn tại không: ls -la *_large.json Nếu các file tồn tại, upload chúng vào S3 bucket trong các thư mục riêng để tránh nhầm lẫn với dữ liệu mẫu nhỏ hơn: # Tạo thư mục large_data nếu chưa tồn tại aws s3api put-object --bucket $DATA_LAKE_BUCKET --key raw/team-a/dataset-a/legislators/large_data/ aws s3api put-object --bucket $DATA_LAKE_BUCKET --key raw/team-a/dataset-a/transactions/large_data/ # Upload file legislators_large.json aws s3 cp legislators_large.json s3://$DATA_LAKE_BUCKET/raw/team-a/dataset-a/legislators/large_data/ # Upload file transactions_large.json aws s3 cp transactions_large.json s3://$DATA_LAKE_BUCKET/raw/team-a/dataset-a/transactions/large_data/ Nhấn Enter để thực hiện từng lệnh Bạn sẽ thấy thông báo xác nhận upload thành công\nSau khi upload, bạn có thể kiểm tra Step Functions để theo dõi các execution mới được tạo cho các file dữ liệu lớn này\nViệc xử lý dữ liệu lớn có thể mất nhiều thời gian hơn và có thể phát sinh thêm chi phí AWS. Đảm bảo bạn hiểu rõ về các giới hạn và chi phí liên quan trước khi upload dữ liệu lớn vào môi trường sản xuất.\nBước 9: Kiểm tra CloudWatch Logs Mở AWS Management Console và tìm kiếm \u0026ldquo;CloudWatch\u0026rdquo; Trong menu bên trái, chọn \u0026ldquo;Logs\u0026rdquo; \u0026gt; \u0026ldquo;Log groups\u0026rdquo; Tìm log groups liên quan đến Glue jobs: /aws-glue/jobs/team-a-dataset-a-stage-a /aws-glue/jobs/team-a-dataset-a-stage-b Nhấp vào mỗi log group để xem chi tiết Chọn log stream mới nhất để xem logs Bạn sẽ thấy logs chi tiết của quá trình xử lý dữ liệu Xử lý sự cố Vấn đề: Lỗi \u0026ldquo;AccessDeniedException\u0026rdquo; khi upload dữ liệu Giải pháp:\nKiểm tra IAM permissions của bạn Đảm bảo tên bucket chính xác Kiểm tra bucket policy Vấn đề: ETL pipeline không được kích hoạt tự động Giải pháp:\nKiểm tra EventBridge rule đã được cấu hình đúng chưa Kiểm tra prefix trong event pattern Kiểm tra IAM role của EventBridge Vấn đề: Glue job fails Giải pháp:\nKiểm tra CloudWatch Logs để xem lỗi chi tiết Đảm bảo định dạng dữ liệu đúng Kiểm tra IAM role của Glue job Bạn có thể sử dụng lệnh aws s3 ls s3://$DATA_LAKE_BUCKET/ --recursive để xem tất cả các files trong S3 bucket.\nTổng kết Chúc mừng! Bạn đã hoàn thành việc nạp và xử lý dữ liệu trong SDLF, bao gồm:\nChuẩn bị dữ liệu mẫu Upload dữ liệu vào S3 bucket Kích hoạt và theo dõi ETL pipeline Kiểm tra dữ liệu đã xử lý Dữ liệu đã được xử lý qua các stages và lưu trữ trong analytics layer, sẵn sàng cho việc truy vấn và phân tích.\nBước tiếp theo Tiếp theo, chúng ta sẽ Truy vấn dữ liệu với Athena để phân tích dữ liệu đã được xử lý.\n"
},
{
	"uri": "/vi/5-etl-pipeline/3-configure-eventbridge/",
	"title": "Cấu hình EventBridge",
	"tags": [],
	"description": "",
	"content": "Cấu hình EventBridge Bước cuối cùng trong việc triển khai ETL Pipeline là cấu hình Amazon EventBridge để tự động kích hoạt Step Functions state machine khi có dữ liệu mới được upload vào S3 bucket.\nCác bước thực hiện Bước 1: Lấy ARN của Step Functions state machine Trong CloudShell, nhập lệnh sau để lấy ARN của Step Functions state machine: export STATE_MACHINE_ARN=$(aws stepfunctions list-state-machines \\ --query \u0026#39;stateMachines[?name==`team-a-dataset-a-pipeline`].stateMachineArn\u0026#39; \\ --output text) echo \u0026#34;State Machine ARN: $STATE_MACHINE_ARN\u0026#34; Nhấn Enter để thực hiện lệnh Bạn sẽ thấy ARN của state machine, ví dụ: State Machine ARN: arn:aws:states:us-east-1:123456789012:stateMachine:team-a-dataset-a-pipeline Bước 2: Lấy tên của Data Lake bucket Nhập lệnh sau để lấy tên của Data Lake bucket: export DATA_LAKE_BUCKET=$(aws cloudformation describe-stacks \\ --stack-name sdlf-foundations-stack \\ --query \u0026#39;Stacks[0].Outputs[?OutputKey==`DataLakeBucketName`].OutputValue\u0026#39; \\ --output text) echo \u0026#34;Data Lake Bucket: $DATA_LAKE_BUCKET\u0026#34; Nhấn Enter để thực hiện lệnh Bước 3: Tạo IAM Role cho EventBridge Nhập lệnh sau để tạo IAM role cho EventBridge: # Tạo trust policy cat \u0026gt; eventbridge-trust-policy.json \u0026lt;\u0026lt; \u0026#39;EOL\u0026#39; { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;events.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] } EOL # Tạo IAM role aws iam create-role \\ --role-name sdlf-eventbridge-role \\ --assume-role-policy-document file://eventbridge-trust-policy.json # Tạo policy cho EventBridge cat \u0026gt; eventbridge-policy.json \u0026lt;\u0026lt; \u0026#39;EOL\u0026#39; { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;states:StartExecution\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;STATE_MACHINE_ARN_PLACEHOLDER\u0026#34; } ] } EOL # Thay thế placeholder bằng ARN thực tế sed -i \u0026#34;s|STATE_MACHINE_ARN_PLACEHOLDER|$STATE_MACHINE_ARN|g\u0026#34; eventbridge-policy.json # Tạo policy aws iam create-policy \\ --policy-name sdlf-eventbridge-policy \\ --policy-document file://eventbridge-policy.json # Lấy Account ID export AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) # Gán policy cho role aws iam attach-role-policy \\ --role-name sdlf-eventbridge-role \\ --policy-arn arn:aws:iam::$AWS_ACCOUNT_ID:policy/sdlf-eventbridge-policy Nhấn Enter để thực hiện từng lệnh Bước 4: Tạo EventBridge rule Mở AWS Management Console và tìm kiếm \u0026ldquo;EventBridge\u0026rdquo; Trong menu bên trái, chọn \u0026ldquo;Rules\u0026rdquo; Nhấp vào nút \u0026ldquo;Create rule\u0026rdquo; Nhập thông tin cơ bản: Name: team-a-dataset-a-s3-trigger Description: Trigger ETL pipeline when new data is uploaded to S3 Event bus: default Trong phần \u0026ldquo;Rule type\u0026rdquo;, chọn \u0026ldquo;Rule with an event pattern\u0026rdquo; Trong phần \u0026ldquo;Event pattern\u0026rdquo;, chọn: Event source: AWS services AWS service: Simple Storage Service (S3) Event type: Amazon S3 Event Notification Specific event(s): Object Created Specific bucket(s) by name: Nhập tên Data Lake bucket của bạn Specific prefix(es): raw/team-a/dataset-a/ Nhấp vào \u0026ldquo;Next\u0026rdquo; Trong phần \u0026ldquo;Select targets\u0026rdquo;: Target type: AWS service Select a target: Step Functions state machine State machine: team-a-dataset-a-pipeline Execution role: Use existing role Existing role: sdlf-eventbridge-role Trong phần \u0026ldquo;Configure input\u0026rdquo;: Input type: Input Transformer Input Path: {\u0026#34;bucket\u0026#34;: \u0026#34;$.detail.bucket.name\u0026#34;, \u0026#34;team\u0026#34;: \u0026#34;team-a\u0026#34;, \u0026#34;dataset\u0026#34;: \u0026#34;dataset-a\u0026#34;} Template: {\u0026#34;bucket\u0026#34;: \u0026lt;bucket\u0026gt;, \u0026#34;team\u0026#34;: \u0026lt;team\u0026gt;, \u0026#34;dataset\u0026#34;: \u0026lt;dataset\u0026gt;} Nhấp vào \u0026ldquo;Next\u0026rdquo; Nhấp vào \u0026ldquo;Next\u0026rdquo; lại Nhấp vào \u0026ldquo;Create rule\u0026rdquo; Bước 5: Kiểm tra EventBridge rule Trong AWS Management Console, trang EventBridge, chọn \u0026ldquo;Rules\u0026rdquo; từ menu bên trái Bạn sẽ thấy rule \u0026ldquo;team-a-dataset-a-s3-trigger\u0026rdquo; trong danh sách Nhấp vào rule để xem chi tiết Bước 6: Kiểm tra tự động kích hoạt Trong CloudShell, nhập lệnh sau để tạo một file test và upload lên S3: # Tạo file test echo \u0026#39;{\u0026#34;test\u0026#34;: \u0026#34;data\u0026#34;}\u0026#39; \u0026gt; test.json # Upload file lên S3 aws s3 cp test.json s3://$DATA_LAKE_BUCKET/raw/team-a/dataset-a/ Nhấn Enter để thực hiện lệnh Mở AWS Management Console và tìm kiếm \u0026ldquo;Step Functions\u0026rdquo; Trong danh sách state machines, tìm \u0026ldquo;team-a-dataset-a-pipeline\u0026rdquo; Nhấp vào state machine để xem chi tiết Trong tab \u0026ldquo;Executions\u0026rdquo;, bạn sẽ thấy một execution mới đã được tạo tự động Nhấp vào execution để xem chi tiết và theo dõi tiến trình Xử lý sự cố Vấn đề: EventBridge rule không kích hoạt Step Functions Giải pháp:\nKiểm tra event pattern đã được cấu hình đúng chưa Đảm bảo IAM role có quyền kích hoạt Step Functions Kiểm tra S3 bucket và prefix đã được cấu hình đúng chưa Vấn đề: Lỗi \u0026ldquo;AccessDeniedException\u0026rdquo; khi EventBridge kích hoạt Step Functions Giải pháp:\nKiểm tra IAM role của EventBridge Đảm bảo policy đã được gán cho role Kiểm tra resource ARN trong policy Bạn có thể xem CloudWatch Logs để debug các vấn đề với EventBridge rules. Nhấp vào \u0026ldquo;Monitoring\u0026rdquo; tab trong trang chi tiết của rule để xem metrics.\nTổng kết Chúc mừng! Bạn đã hoàn thành việc triển khai ETL Pipeline trong SDLF, bao gồm:\nTạo Glue jobs để xử lý và chuyển đổi dữ liệu Thiết lập Step Functions để orchestrate workflow Cấu hình EventBridge để tự động kích hoạt pipeline khi có dữ liệu mới ETL Pipeline này sẽ tự động xử lý dữ liệu khi có dữ liệu mới được upload vào S3 bucket, chuyển đổi dữ liệu qua các stages và lưu trữ kết quả trong analytics layer.\nBước tiếp theo Tiếp theo, chúng ta sẽ Nạp và xử lý dữ liệu để kiểm tra ETL pipeline đã triển khai.\n"
},
{
	"uri": "/vi/1-prerequisite/3-region-selection/",
	"title": "Chọn region phù hợp",
	"tags": [],
	"description": "",
	"content": "Chọn region phù hợp Việc chọn đúng AWS region là rất quan trọng để đảm bảo tất cả các dịch vụ cần thiết đều có sẵn và để tối ưu hóa hiệu suất cũng như chi phí.\nCác bước thực hiện Bước 1: Xác định region hiện tại Nhìn vào góc trên bên phải của AWS Management Console Bạn sẽ thấy region hiện tại được hiển thị (ví dụ: \u0026ldquo;N. Virginia\u0026rdquo; hoặc \u0026ldquo;us-east-1\u0026rdquo;) Bước 2: Thay đổi region (nếu cần) Nhấp vào menu dropdown hiển thị region hiện tại Một danh sách các region sẽ hiện ra Chọn một trong các region được khuyến nghị: US East (N. Virginia) - us-east-1 US West (Oregon) - us-west-2 Europe (Ireland) - eu-west-1 Asia Pacific (Singapore) - ap-southeast-1 Bước 3: Xác nhận region trong CloudShell Trong CloudShell, nhập lệnh sau để xác nhận region hiện tại: aws configure get region Nhấn Enter để thực hiện lệnh Kết quả sẽ hiển thị region hiện tại, ví dụ: us-east-1 Bước 4: Thiết lập region mặc định (nếu cần) Nếu region hiển thị không phải là region bạn muốn sử dụng, hãy thiết lập region mặc định bằng lệnh: aws configure set region us-east-1 Thay thế us-east-1 bằng region bạn muốn sử dụng Nhấn Enter để thực hiện lệnh Kiểm tra lại region đã được thiết lập: aws configure get region Các region được khuyến nghị Tên region Mã region Vị trí US East (N. Virginia) us-east-1 Bắc Virginia, Hoa Kỳ US West (Oregon) us-west-2 Oregon, Hoa Kỳ Europe (Ireland) eu-west-1 Ireland Asia Pacific (Singapore) ap-southeast-1 Singapore Xử lý sự cố Vấn đề: Region không thay đổi trong CloudShell sau khi thay đổi trong Console Giải pháp:\nĐóng và mở lại CloudShell Sử dụng lệnh aws configure set region như hướng dẫn ở trên Vấn đề: Một số dịch vụ không có sẵn trong region đã chọn Giải pháp:\nChuyển sang region khác được khuyến nghị Ưu tiên sử dụng us-east-1 (N. Virginia) vì đây là region có đầy đủ nhất các dịch vụ AWS Đảm bảo bạn sử dụng cùng một region cho toàn bộ workshop. Việc thay đổi region giữa chừng có thể gây ra lỗi và tăng chi phí do chuyển dữ liệu giữa các region.\nNếu bạn đang ở khu vực châu Á - Thái Bình Dương, hãy cân nhắc sử dụng region ap-southeast-1 (Singapore) để giảm độ trễ. Tuy nhiên, hãy đảm bảo tất cả các dịch vụ cần thiết đều có sẵn trong region này.\nBước tiếp theo Bây giờ bạn đã hoàn thành các bước chuẩn bị, chúng ta sẽ tiếp tục với Triển khai SDLF Foundations để bắt đầu xây dựng data lake.\n"
},
{
	"uri": "/vi/3-cicd-pipeline/3-setup-codepipeline/",
	"title": "Thiết lập CodePipeline",
	"tags": [],
	"description": "",
	"content": "Thiết lập CodePipeline Sau khi đã cấu hình CodeBuild, chúng ta sẽ thiết lập CodePipeline để tự động hóa toàn bộ quy trình CI/CD.\nCác bước thực hiện Bước 1: Tạo S3 bucket cho artifacts Trong CloudShell, nhập lệnh sau để tạo S3 bucket cho artifacts: # Lấy account ID và region export AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) export AWS_REGION=$(aws configure get region) # Tạo S3 bucket aws s3 mb s3://sdlf-codepipeline-artifacts-$AWS_ACCOUNT_ID-$AWS_REGION --region $AWS_REGION # Bật versioning cho bucket aws s3api put-bucket-versioning \\ --bucket sdlf-codepipeline-artifacts-$AWS_ACCOUNT_ID-$AWS_REGION \\ --versioning-configuration Status=Enabled Nhấn Enter để thực hiện từng lệnh Bước 2: Tạo IAM Role cho CodePipeline Nhập lệnh sau để tạo IAM role cho CodePipeline: # Tạo trust policy cat \u0026gt; codepipeline-trust-policy.json \u0026lt;\u0026lt; \u0026#39;EOL\u0026#39; { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;codepipeline.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] } EOL # Tạo IAM role aws iam create-role \\ --role-name sdlf-codepipeline-role \\ --assume-role-policy-document file://codepipeline-trust-policy.json # Gán AdministratorAccess policy (chỉ cho workshop, không khuyến nghị cho production) aws iam attach-role-policy \\ --role-name sdlf-codepipeline-role \\ --policy-arn arn:aws:iam::aws:policy/AdministratorAccess Nhấn Enter để thực hiện từng lệnh Bước 3: Tạo CodePipeline cho sdlf-foundations Nhập lệnh sau để tạo file cấu hình pipeline: cat \u0026gt; sdlf-foundations-pipeline.json \u0026lt;\u0026lt; EOL { \u0026#34;pipeline\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;sdlf-foundations-pipeline\u0026#34;, \u0026#34;roleArn\u0026#34;: \u0026#34;arn:aws:iam::$AWS_ACCOUNT_ID:role/sdlf-codepipeline-role\u0026#34;, \u0026#34;artifactStore\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;S3\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;sdlf-codepipeline-artifacts-$AWS_ACCOUNT_ID-$AWS_REGION\u0026#34; }, \u0026#34;stages\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Source\u0026#34;, \u0026#34;actions\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Source\u0026#34;, \u0026#34;actionTypeId\u0026#34;: { \u0026#34;category\u0026#34;: \u0026#34;Source\u0026#34;, \u0026#34;owner\u0026#34;: \u0026#34;AWS\u0026#34;, \u0026#34;provider\u0026#34;: \u0026#34;CodeCommit\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1\u0026#34; }, \u0026#34;configuration\u0026#34;: { \u0026#34;RepositoryName\u0026#34;: \u0026#34;sdlf-foundations\u0026#34;, \u0026#34;BranchName\u0026#34;: \u0026#34;master\u0026#34; }, \u0026#34;outputArtifacts\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;SourceCode\u0026#34; } ], \u0026#34;region\u0026#34;: \u0026#34;$AWS_REGION\u0026#34; } ] }, { \u0026#34;name\u0026#34;: \u0026#34;Build\u0026#34;, \u0026#34;actions\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;BuildAction\u0026#34;, \u0026#34;actionTypeId\u0026#34;: { \u0026#34;category\u0026#34;: \u0026#34;Build\u0026#34;, \u0026#34;owner\u0026#34;: \u0026#34;AWS\u0026#34;, \u0026#34;provider\u0026#34;: \u0026#34;CodeBuild\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1\u0026#34; }, \u0026#34;configuration\u0026#34;: { \u0026#34;ProjectName\u0026#34;: \u0026#34;sdlf-foundations-build\u0026#34; }, \u0026#34;inputArtifacts\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;SourceCode\u0026#34; } ], \u0026#34;region\u0026#34;: \u0026#34;$AWS_REGION\u0026#34; } ] } ] } } EOL Nhấn Enter để thực hiện lệnh Nhập lệnh sau để tạo CodePipeline: aws codepipeline create-pipeline --cli-input-json file://sdlf-foundations-pipeline.json Nhấn Enter để thực hiện lệnh Bạn sẽ thấy kết quả JSON với thông tin về pipeline đã được tạo Bước 4: Tạo CodePipeline cho sdlf-team Nhập lệnh sau để tạo file cấu hình pipeline cho sdlf-team: cat \u0026gt; sdlf-team-pipeline.json \u0026lt;\u0026lt; EOL { \u0026#34;pipeline\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;sdlf-team-pipeline\u0026#34;, \u0026#34;roleArn\u0026#34;: \u0026#34;arn:aws:iam::$AWS_ACCOUNT_ID:role/sdlf-codepipeline-role\u0026#34;, \u0026#34;artifactStore\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;S3\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;sdlf-codepipeline-artifacts-$AWS_ACCOUNT_ID-$AWS_REGION\u0026#34; }, \u0026#34;stages\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Source\u0026#34;, \u0026#34;actions\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Source\u0026#34;, \u0026#34;actionTypeId\u0026#34;: { \u0026#34;category\u0026#34;: \u0026#34;Source\u0026#34;, \u0026#34;owner\u0026#34;: \u0026#34;AWS\u0026#34;, \u0026#34;provider\u0026#34;: \u0026#34;CodeCommit\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1\u0026#34; }, \u0026#34;configuration\u0026#34;: { \u0026#34;RepositoryName\u0026#34;: \u0026#34;sdlf-team\u0026#34;, \u0026#34;BranchName\u0026#34;: \u0026#34;master\u0026#34; }, \u0026#34;outputArtifacts\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;SourceCode\u0026#34; } ], \u0026#34;region\u0026#34;: \u0026#34;$AWS_REGION\u0026#34; } ] }, { \u0026#34;name\u0026#34;: \u0026#34;Build\u0026#34;, \u0026#34;actions\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;BuildAction\u0026#34;, \u0026#34;actionTypeId\u0026#34;: { \u0026#34;category\u0026#34;: \u0026#34;Build\u0026#34;, \u0026#34;owner\u0026#34;: \u0026#34;AWS\u0026#34;, \u0026#34;provider\u0026#34;: \u0026#34;CodeBuild\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1\u0026#34; }, \u0026#34;configuration\u0026#34;: { \u0026#34;ProjectName\u0026#34;: \u0026#34;sdlf-team-build\u0026#34; }, \u0026#34;inputArtifacts\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;SourceCode\u0026#34; } ], \u0026#34;region\u0026#34;: \u0026#34;$AWS_REGION\u0026#34; } ] } ] } } EOL # Tạo CodePipeline aws codepipeline create-pipeline --cli-input-json file://sdlf-team-pipeline.json Nhấn Enter để thực hiện lệnh Bước 5: Tạo CodePipeline cho sdlf-dataset Nhập lệnh sau để tạo file cấu hình pipeline cho sdlf-dataset: cat \u0026gt; sdlf-dataset-pipeline.json \u0026lt;\u0026lt; EOL { \u0026#34;pipeline\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;sdlf-dataset-pipeline\u0026#34;, \u0026#34;roleArn\u0026#34;: \u0026#34;arn:aws:iam::$AWS_ACCOUNT_ID:role/sdlf-codepipeline-role\u0026#34;, \u0026#34;artifactStore\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;S3\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;sdlf-codepipeline-artifacts-$AWS_ACCOUNT_ID-$AWS_REGION\u0026#34; }, \u0026#34;stages\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Source\u0026#34;, \u0026#34;actions\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Source\u0026#34;, \u0026#34;actionTypeId\u0026#34;: { \u0026#34;category\u0026#34;: \u0026#34;Source\u0026#34;, \u0026#34;owner\u0026#34;: \u0026#34;AWS\u0026#34;, \u0026#34;provider\u0026#34;: \u0026#34;CodeCommit\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1\u0026#34; }, \u0026#34;configuration\u0026#34;: { \u0026#34;RepositoryName\u0026#34;: \u0026#34;sdlf-dataset\u0026#34;, \u0026#34;BranchName\u0026#34;: \u0026#34;master\u0026#34; }, \u0026#34;outputArtifacts\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;SourceCode\u0026#34; } ], \u0026#34;region\u0026#34;: \u0026#34;$AWS_REGION\u0026#34; } ] }, { \u0026#34;name\u0026#34;: \u0026#34;Build\u0026#34;, \u0026#34;actions\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;BuildAction\u0026#34;, \u0026#34;actionTypeId\u0026#34;: { \u0026#34;category\u0026#34;: \u0026#34;Build\u0026#34;, \u0026#34;owner\u0026#34;: \u0026#34;AWS\u0026#34;, \u0026#34;provider\u0026#34;: \u0026#34;CodeBuild\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1\u0026#34; }, \u0026#34;configuration\u0026#34;: { \u0026#34;ProjectName\u0026#34;: \u0026#34;sdlf-dataset-build\u0026#34; }, \u0026#34;inputArtifacts\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;SourceCode\u0026#34; } ], \u0026#34;region\u0026#34;: \u0026#34;$AWS_REGION\u0026#34; } ] } ] } } EOL # Tạo CodePipeline aws codepipeline create-pipeline --cli-input-json file://sdlf-dataset-pipeline.json Nhấn Enter để thực hiện lệnh Bước 6: Kiểm tra CodePipeline trong AWS Management Console Mở tab mới trong trình duyệt (hoặc sử dụng tab đã mở) Truy cập AWS Management Console Tìm kiếm và chọn \u0026ldquo;CodePipeline\u0026rdquo; Bạn sẽ thấy danh sách các pipelines đã tạo: sdlf-foundations-pipeline sdlf-team-pipeline sdlf-dataset-pipeline Nhấp vào từng pipeline để xem chi tiết Bạn sẽ thấy các stages của pipeline (Source và Build) và trạng thái của chúng Xử lý sự cố Vấn đề: Lỗi \u0026ldquo;AccessDeniedException\u0026rdquo; khi tạo CodePipeline Giải pháp:\nKiểm tra IAM permissions của bạn Đảm bảo IAM role đã được tạo thành công Kiểm tra trust relationship của IAM role Vấn đề: Lỗi \u0026ldquo;InvalidS3LocationException\u0026rdquo; khi tạo CodePipeline Giải pháp:\nKiểm tra xem S3 bucket đã được tạo thành công chưa Đảm bảo tên bucket chính xác Kiểm tra region của bucket Bạn có thể theo dõi tiến trình của pipeline bằng cách nhấp vào pipeline trong AWS Management Console. Mỗi khi có thay đổi trong repository, pipeline sẽ tự động chạy.\nBước tiếp theo Tiếp theo, chúng ta sẽ Kiểm tra pipeline để đảm bảo CI/CD pipeline hoạt động như mong đợi.\n"
},
{
	"uri": "/vi/4-team-dataset/3-configure-permissions/",
	"title": "Thiết lập quyền truy cập",
	"tags": [],
	"description": "",
	"content": "Thiết lập quyền truy cập Sau khi đã tạo team và dataset, chúng ta cần thiết lập quyền truy cập để quản lý quyền truy cập vào dữ liệu.\nCác bước thực hiện Bước 1: Thiết lập Lake Formation permissions Mở AWS Management Console và tìm kiếm \u0026ldquo;Lake Formation\u0026rdquo; Trong menu bên trái, chọn \u0026ldquo;Permissions\u0026rdquo; \u0026gt; \u0026ldquo;Data lake permissions\u0026rdquo; Nhấp vào nút \u0026ldquo;Grant\u0026rdquo; Trong phần \u0026ldquo;Principals\u0026rdquo;, chọn \u0026ldquo;IAM users and roles\u0026rdquo; Chọn role \u0026ldquo;sdlf-team-a-role\u0026rdquo; từ danh sách Trong phần \u0026ldquo;LF-Tags or catalog resources\u0026rdquo;, chọn \u0026ldquo;Named data catalog resources\u0026rdquo; Trong phần \u0026ldquo;Databases\u0026rdquo;, chọn \u0026ldquo;team_a_db\u0026rdquo; từ danh sách Trong phần \u0026ldquo;Tables\u0026rdquo;, chọn \u0026ldquo;All tables\u0026rdquo; Trong phần \u0026ldquo;Table permissions\u0026rdquo;, chọn các quyền sau: Select Describe Alter Drop Delete Insert Nhấp vào nút \u0026ldquo;Grant\u0026rdquo; để cấp quyền Bước 2: Thiết lập S3 bucket policy Trong CloudShell, nhập lệnh sau để lấy tên của Data Lake bucket (nếu bạn chưa thực hiện ở bước trước): export DATA_LAKE_BUCKET=$(aws cloudformation describe-stacks \\ --stack-name sdlf-foundations-stack \\ --query \u0026#39;Stacks[0].Outputs[?OutputKey==`DataLakeBucketName`].OutputValue\u0026#39; \\ --output text) echo \u0026#34;Data Lake Bucket: $DATA_LAKE_BUCKET\u0026#34; Nhấn Enter để thực hiện lệnh Nhập lệnh sau để lấy ARN của IAM role: export TEAM_ROLE_ARN=$(aws iam get-role \\ --role-name sdlf-team-a-role \\ --query \u0026#39;Role.Arn\u0026#39; \\ --output text) echo \u0026#34;Team Role ARN: $TEAM_ROLE_ARN\u0026#34; Nhấn Enter để thực hiện lệnh Nhập lệnh sau để tạo bucket policy: cat \u0026gt; bucket-policy.json \u0026lt;\u0026lt; EOL { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: \u0026#34;$TEAM_ROLE_ARN\u0026#34; }, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:ListBucket\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::$DATA_LAKE_BUCKET\u0026#34;, \u0026#34;arn:aws:s3:::$DATA_LAKE_BUCKET/raw/team-a/*\u0026#34;, \u0026#34;arn:aws:s3:::$DATA_LAKE_BUCKET/stage/team-a/*\u0026#34;, \u0026#34;arn:aws:s3:::$DATA_LAKE_BUCKET/analytics/team-a/*\u0026#34; ] } ] } EOL Nhấn Enter để thực hiện lệnh Nhập lệnh sau để áp dụng bucket policy: aws s3api put-bucket-policy \\ --bucket $DATA_LAKE_BUCKET \\ --policy file://bucket-policy.json Nhấn Enter để thực hiện lệnh Nếu lệnh thực hiện thành công, bạn sẽ không thấy output nào Bước 3: Kiểm tra bucket policy Nhập lệnh sau để kiểm tra bucket policy đã được áp dụng: aws s3api get-bucket-policy --bucket $DATA_LAKE_BUCKET Nhấn Enter để thực hiện lệnh Bạn sẽ thấy kết quả JSON với thông tin về bucket policy đã được áp dụng Bước 4: Kiểm tra Lake Formation permissions Trong AWS Management Console, tìm kiếm \u0026ldquo;Lake Formation\u0026rdquo; Trong menu bên trái, chọn \u0026ldquo;Permissions\u0026rdquo; \u0026gt; \u0026ldquo;Data lake permissions\u0026rdquo; Tìm kiếm role \u0026ldquo;sdlf-team-a-role\u0026rdquo; trong danh sách Kiểm tra xem role đã được cấp quyền truy cập vào database \u0026ldquo;team_a_db\u0026rdquo; chưa Xử lý sự cố Vấn đề: Lỗi \u0026ldquo;AccessDeniedException\u0026rdquo; khi thiết lập Lake Formation permissions Giải pháp:\nKiểm tra IAM permissions của bạn Đảm bảo bạn có quyền truy cập vào Lake Formation Kiểm tra xem bạn có quyền cấp permissions không Vấn đề: Lỗi \u0026ldquo;MalformedPolicy\u0026rdquo; khi áp dụng bucket policy Giải pháp:\nKiểm tra cú pháp của policy Đảm bảo ARN của role và bucket là chính xác Kiểm tra xem policy có vượt quá kích thước tối đa không Trong môi trường production, bạn nên áp dụng nguyên tắc least privilege (quyền tối thiểu cần thiết) khi thiết lập quyền truy cập. Trong workshop này, chúng ta cấp quyền rộng hơn để đơn giản hóa quá trình.\nTổng kết Chúc mừng! Bạn đã hoàn thành việc đăng ký team và dataset trong SDLF, bao gồm:\nĐăng ký team trong DynamoDB Tạo dataset trong DynamoDB Tạo cấu trúc thư mục trong S3 Tạo Glue Database Thiết lập quyền truy cập thông qua Lake Formation và S3 bucket policy Các bước này đã thiết lập cấu trúc dữ liệu và quyền truy cập cần thiết cho SDLF.\nBước tiếp theo Tiếp theo, chúng ta sẽ Triển khai ETL Pipeline để xử lý và chuyển đổi dữ liệu.\n"
},
{
	"uri": "/vi/7-athena-query/3-run-queries/",
	"title": "Thực hiện truy vấn SQL",
	"tags": [],
	"description": "",
	"content": "Thực hiện truy vấn SQL Sau khi đã thiết lập Athena Workgroup và tạo tables bằng Glue Crawler, bây giờ chúng ta có thể sử dụng SQL để truy vấn và phân tích dữ liệu trong data lake.\nCác bước thực hiện Bước 1: Mở Amazon Athena Console Đăng nhập vào AWS Management Console Tìm kiếm \u0026ldquo;Athena\u0026rdquo; trong thanh tìm kiếm Chọn \u0026ldquo;Amazon Athena\u0026rdquo; từ kết quả tìm kiếm Bước 2: Chọn Workgroup Trong menu bên trái, chọn \u0026ldquo;Workgroups\u0026rdquo; Tìm và chọn \u0026ldquo;analytics-team-workgroup\u0026rdquo; từ danh sách Nhấp vào \u0026ldquo;Switch workgroup\u0026rdquo; để chuyển sang workgroup này Bước 3: Chọn Database và Table Trong menu bên trái, chọn \u0026ldquo;Query editor\u0026rdquo; Từ dropdown \u0026ldquo;Database\u0026rdquo;, chọn \u0026ldquo;team_a_db\u0026rdquo; Bạn sẽ thấy danh sách tables trong database, bao gồm table \u0026ldquo;transactions_\u0026hellip;\u0026rdquo; đã được tạo bởi Glue Crawler Bước 4: Xem schema của table Nhấp vào biểu tượng \u0026ldquo;\u0026hellip;\u0026rdquo; bên cạnh tên table Chọn \u0026ldquo;Preview table\u0026rdquo; Hoặc mở rộng tên table để xem danh sách các cột và kiểu dữ liệu Bước 5: Thực hiện truy vấn cơ bản Trong Query editor, nhập truy vấn SQL sau: SELECT * FROM \u0026#34;team_a_db\u0026#34;.\u0026#34;transactions_YYYYMMDD_HHMMSS\u0026#34; LIMIT 10; (Thay \u0026ldquo;transactions_YYYYMMDD_HHMMSS\u0026rdquo; bằng tên table thực tế)\nNhấp vào \u0026ldquo;Run query\u0026rdquo; hoặc nhấn Ctrl+Enter Xem kết quả truy vấn ở phần dưới màn hình Bước 6: Thực hiện truy vấn tổng hợp theo merchant_category Trong Query editor, nhập truy vấn SQL sau: SELECT merchant_category, COUNT(*) as transaction_count, SUM(amount) as total_amount, AVG(amount) as avg_amount FROM \u0026#34;team_a_db\u0026#34;.\u0026#34;transactions_YYYYMMDD_HHMMSS\u0026#34; GROUP BY merchant_category ORDER BY total_amount DESC; (Thay \u0026ldquo;transactions_YYYYMMDD_HHMMSS\u0026rdquo; bằng tên table thực tế)\nNhấp vào \u0026ldquo;Run query\u0026rdquo; hoặc nhấn Ctrl+Enter Bước 7: Thực hiện truy vấn phân tích theo customer_id Trong Query editor, nhập truy vấn SQL sau: SELECT customer_id, COUNT(*) as transaction_count, SUM(amount) as total_spent, AVG(amount) as avg_transaction, MIN(transaction_date) as first_transaction, MAX(transaction_date) as last_transaction FROM \u0026#34;team_a_db\u0026#34;.\u0026#34;transactions_YYYYMMDD_HHMMSS\u0026#34; GROUP BY customer_id ORDER BY total_spent DESC LIMIT 20; (Thay \u0026ldquo;transactions_YYYYMMDD_HHMMSS\u0026rdquo; bằng tên table thực tế)\nNhấp vào \u0026ldquo;Run query\u0026rdquo; hoặc nhấn Ctrl+Enter Bước 8: Lưu truy vấn Sau khi thực hiện truy vấn thành công, nhấp vào \u0026ldquo;Save as\u0026rdquo; Nhập tên cho truy vấn, ví dụ: \u0026ldquo;Customer Spending Analysis\u0026rdquo; Nhấp vào \u0026ldquo;Save\u0026rdquo; Bước 9: Xem lịch sử truy vấn Trong menu bên trái, chọn \u0026ldquo;Recent queries\u0026rdquo; Xem danh sách các truy vấn đã thực hiện gần đây Nhấp vào một truy vấn để mở lại trong Query editor Các truy vấn SQL hữu ích Truy vấn 1: Phân tích giao dịch theo ngày SELECT DATE(transaction_date) as tx_date, COUNT(*) as transaction_count, SUM(amount) as total_amount, AVG(amount) as avg_amount FROM \u0026#34;team_a_db\u0026#34;.\u0026#34;transactions_YYYYMMDD_HHMMSS\u0026#34; GROUP BY DATE(transaction_date) ORDER BY tx_date DESC; Truy vấn 2: Top 10 khách hàng có chi tiêu cao nhất SELECT customer_id, COUNT(*) as transaction_count, SUM(amount) as total_spent FROM \u0026#34;team_a_db\u0026#34;.\u0026#34;transactions_YYYYMMDD_HHMMSS\u0026#34; GROUP BY customer_id ORDER BY total_spent DESC LIMIT 10; Truy vấn 3: Phân tích giao dịch theo giờ trong ngày SELECT EXTRACT(HOUR FROM transaction_date) as hour_of_day, COUNT(*) as transaction_count, SUM(amount) as total_amount FROM \u0026#34;team_a_db\u0026#34;.\u0026#34;transactions_YYYYMMDD_HHMMSS\u0026#34; GROUP BY EXTRACT(HOUR FROM transaction_date) ORDER BY hour_of_day; Xử lý sự cố Vấn đề: Lỗi \u0026ldquo;Table not found\u0026rdquo; Giải pháp:\nKiểm tra tên database và table chính xác Đảm bảo Glue Crawler đã chạy thành công Kiểm tra quyền truy cập vào Glue Data Catalog Vấn đề: Lỗi \u0026ldquo;Access Denied\u0026rdquo; khi truy vấn Giải pháp:\nKiểm tra IAM permissions Đảm bảo bạn có quyền truy cập vào S3 bucket chứa dữ liệu Kiểm tra Lake Formation permissions nếu được sử dụng Để tối ưu chi phí khi sử dụng Athena, hãy giới hạn số lượng dữ liệu được quét bằng cách sử dụng WHERE clause, LIMIT, và chỉ chọn các cột cần thiết thay vì sử dụng SELECT *.\nBước tiếp theo Tiếp theo, chúng ta sẽ Tạo và sử dụng Views để đơn giản hóa các truy vấn phức tạp.\n"
},
{
	"uri": "/vi/2-foundations/3-deploy-cloudformation/",
	"title": "Triển khai CloudFormation",
	"tags": [],
	"description": "",
	"content": "Triển khai CloudFormation Sau khi đã cấu hình parameters, chúng ta sẽ triển khai SDLF Foundations bằng AWS CloudFormation.\nCác bước thực hiện Bước 1: Kiểm tra script triển khai Trong CloudShell, đảm bảo bạn đang ở thư mục sdlf-foundations: cd ~/environment/sdlf-workshop/aws-serverless-data-lake-framework/sdlf-foundations Nhập lệnh sau để xem nội dung script triển khai: cat deploy.sh Nhấn Enter để thực hiện lệnh Bạn sẽ thấy nội dung của script deploy.sh, đây là script sẽ triển khai SDLF Foundations thông qua CloudFormation Bước 2: Cấp quyền thực thi cho script Nhập lệnh sau để cấp quyền thực thi cho script deploy.sh: chmod +x deploy.sh Nhấn Enter để thực hiện lệnh Bước 3: Chạy script triển khai Nhập lệnh sau để chạy script triển khai: ./deploy.sh Nhấn Enter để thực hiện lệnh Script sẽ bắt đầu triển khai SDLF Foundations. Quá trình này có thể mất khoảng 15-20 phút.\nBạn sẽ thấy các thông báo về tiến trình triển khai, ví dụ:\nDeploying SDLF Foundations... Creating CloudFormation stack: sdlf-foundations-stack Waiting for stack creation to complete... Bước 4: Theo dõi tiến trình triển khai Trong khi đợi script hoàn thành, bạn có thể mở tab mới trong CloudShell để theo dõi tiến trình triển khai: aws cloudformation describe-stacks --stack-name sdlf-foundations-stack --query \u0026#39;Stacks[0].StackStatus\u0026#39; Nhấn Enter để thực hiện lệnh Bạn sẽ thấy trạng thái hiện tại của stack, ví dụ:\n\u0026quot;CREATE_IN_PROGRESS\u0026quot;: Stack đang được tạo \u0026quot;CREATE_COMPLETE\u0026quot;: Stack đã được tạo thành công Bạn cũng có thể theo dõi tiến trình triển khai thông qua AWS Management Console:\nMở tab mới trong trình duyệt Truy cập AWS Management Console Tìm kiếm và chọn \u0026ldquo;CloudFormation\u0026rdquo; Chọn stack \u0026ldquo;sdlf-foundations-stack\u0026rdquo; trong danh sách Xem tab \u0026ldquo;Events\u0026rdquo; để theo dõi các sự kiện triển khai Bước 5: Xác nhận triển khai thành công Sau khi script hoàn thành, bạn sẽ thấy thông báo thành công: SDLF Foundations deployment completed successfully! Để xác nhận stack đã được tạo thành công, nhập lệnh: aws cloudformation describe-stacks --stack-name sdlf-foundations-stack --query \u0026#39;Stacks[0].StackStatus\u0026#39; Nhấn Enter để thực hiện lệnh Kết quả sẽ hiển thị \u0026quot;CREATE_COMPLETE\u0026quot; nếu stack đã được tạo thành công Xử lý sự cố Vấn đề: Stack creation fails với lỗi \u0026ldquo;Access Denied\u0026rdquo; Giải pháp:\nKiểm tra IAM permissions của bạn Đảm bảo bạn đang sử dụng tài khoản có quyền AdministratorAccess Kiểm tra lại region và account ID trong file parameters-dev.json Vấn đề: Stack creation fails với lỗi \u0026ldquo;Resource already exists\u0026rdquo; Giải pháp:\nKiểm tra xem bạn đã có stack với tên tương tự chưa Thay đổi tên bucket trong file parameters-dev.json vì bucket names phải là duy nhất trên toàn cầu Xóa stack hiện tại (nếu có) và thử lại Vấn đề: Script deploy.sh không thực thi Giải pháp:\nĐảm bảo bạn đã cấp quyền thực thi: chmod +x deploy.sh Thử chạy với bash: bash deploy.sh Quá trình triển khai có thể mất khoảng 15-20 phút. Đừng dừng script giữa chừng vì điều này có thể dẫn đến tình trạng stack không hoàn chỉnh.\nNếu bạn gặp lỗi trong quá trình triển khai, hãy kiểm tra CloudWatch Logs để biết thêm chi tiết. Các logs thường được lưu trong log group có tên bắt đầu bằng \u0026ldquo;/aws/cloudformation/\u0026rdquo;.\nBước tiếp theo Tiếp theo, chúng ta sẽ Kiểm tra kết quả triển khai để đảm bảo SDLF Foundations đã được triển khai thành công.\n"
},
{
	"uri": "/vi/2-foundations/4-verify-deployment/",
	"title": "Kiểm tra kết quả triển khai",
	"tags": [],
	"description": "",
	"content": "Kiểm tra kết quả triển khai Sau khi triển khai SDLF Foundations thành công, chúng ta cần kiểm tra các tài nguyên đã được tạo để đảm bảo mọi thứ hoạt động như mong đợi.\nCác bước thực hiện Bước 1: Kiểm tra CloudFormation Outputs Trong CloudShell, nhập lệnh sau để xem outputs của CloudFormation stack: aws cloudformation describe-stacks --stack-name sdlf-foundations-stack --query \u0026#39;Stacks[0].Outputs\u0026#39; Nhấn Enter để thực hiện lệnh Bạn sẽ thấy danh sách các outputs quan trọng, bao gồm:\nDataLakeBucketName: Tên của S3 bucket chính cho data lake ArtifactsBucketName: Tên của S3 bucket chứa artifacts StagingBucketName: Tên của S3 bucket cho staging data CentralBucketName: Tên của S3 bucket trung tâm Và các outputs khác Lưu lại các giá trị này vì chúng sẽ được sử dụng trong các phần tiếp theo\nBước 2: Kiểm tra S3 Buckets Nhập lệnh sau để lấy tên của Data Lake bucket: export DATA_LAKE_BUCKET=$(aws cloudformation describe-stacks --stack-name sdlf-foundations-stack --query \u0026#39;Stacks[0].Outputs[?OutputKey==`DataLakeBucketName`].OutputValue\u0026#39; --output text) echo \u0026#34;Data Lake Bucket: $DATA_LAKE_BUCKET\u0026#34; Nhấn Enter để thực hiện lệnh Kiểm tra xem bucket đã được tạo thành công: aws s3 ls s3://$DATA_LAKE_BUCKET Nhấn Enter để thực hiện lệnh Bạn sẽ thấy danh sách các thư mục trong bucket (có thể trống nếu bucket mới được tạo) Bước 3: Kiểm tra DynamoDB Tables Nhập lệnh sau để liệt kê các DynamoDB tables đã được tạo: aws dynamodb list-tables Nhấn Enter để thực hiện lệnh Bạn sẽ thấy danh sách các tables, bao gồm: sdlf-metadata sdlf-teams sdlf-datasets sdlf-pipeline-execution Và các tables khác Bước 4: Kiểm tra IAM Roles Nhập lệnh sau để liệt kê các IAM roles liên quan đến SDLF: aws iam list-roles --query \u0026#39;Roles[?contains(RoleName, `sdlf`)].RoleName\u0026#39; Nhấn Enter để thực hiện lệnh Bạn sẽ thấy danh sách các IAM roles đã được tạo cho SDLF Bước 5: Kiểm tra Lake Formation Nhập lệnh sau để kiểm tra Lake Formation Data Lake Settings: aws lakeformation get-data-lake-settings Nhấn Enter để thực hiện lệnh Bạn sẽ thấy thông tin về Data Lake Settings, bao gồm các admins và các cài đặt khác Bước 6: Kiểm tra qua AWS Management Console Mở tab mới trong trình duyệt Truy cập AWS Management Console Tìm kiếm và chọn \u0026ldquo;S3\u0026rdquo; Kiểm tra xem các buckets sau đã được tạo: sdlf-datalake-[account-id]-[region] sdlf-artifacts-[account-id]-[region] sdlf-staging-[account-id]-[region] sdlf-central-[account-id]-[region] Tìm kiếm và chọn \u0026ldquo;DynamoDB\u0026rdquo; Kiểm tra xem các tables đã được liệt kê ở Bước 3 đã được tạo Tìm kiếm và chọn \u0026ldquo;Lake Formation\u0026rdquo; Kiểm tra xem Data Lake đã được thiết lập Xử lý sự cố Vấn đề: Không thấy một số tài nguyên đã được tạo Giải pháp:\nKiểm tra lại region hiện tại, đảm bảo bạn đang ở cùng region với nơi triển khai SDLF Kiểm tra CloudFormation stack status để đảm bảo triển khai đã hoàn tất Kiểm tra CloudWatch Logs để tìm lỗi trong quá trình triển khai Vấn đề: Lỗi khi truy cập S3 buckets Giải pháp:\nKiểm tra IAM permissions của bạn Đảm bảo tên bucket chính xác (phân biệt chữ hoa chữ thường) Kiểm tra bucket policy và ACLs Lưu lại các giá trị từ CloudFormation outputs (đặc biệt là tên các S3 buckets) vì chúng sẽ được sử dụng nhiều lần trong các phần tiếp theo của workshop.\nTổng kết Chúc mừng! Bạn đã triển khai thành công SDLF Foundations, bao gồm:\nCác S3 buckets cho data lake DynamoDB tables cho metadata IAM roles và policies Lake Formation settings Các tài nguyên khác cần thiết cho SDLF Các tài nguyên này tạo thành cơ sở hạ tầng cơ bản cho Serverless Data Lake Framework, và bạn sẽ xây dựng trên nền tảng này trong các phần tiếp theo.\nBước tiếp theo Tiếp theo, chúng ta sẽ Thiết lập CI/CD Pipeline để tự động hóa việc triển khai các thành phần khác của SDLF.\n"
},
{
	"uri": "/vi/3-cicd-pipeline/4-test-pipeline/",
	"title": "Kiểm tra pipeline",
	"tags": [],
	"description": "",
	"content": "Kiểm tra pipeline Sau khi đã thiết lập CodePipeline, chúng ta cần kiểm tra để đảm bảo pipeline hoạt động như mong đợi.\nCác bước thực hiện Bước 1: Kiểm tra trạng thái pipeline Trong AWS Management Console, tìm kiếm và chọn \u0026ldquo;CodePipeline\u0026rdquo; Bạn sẽ thấy danh sách các pipelines đã tạo Kiểm tra trạng thái của từng pipeline: sdlf-foundations-pipeline sdlf-team-pipeline sdlf-dataset-pipeline Nếu pipeline đang chạy, bạn sẽ thấy các stages đang trong trạng thái \u0026ldquo;In progress\u0026rdquo; hoặc \u0026ldquo;Succeeded\u0026rdquo; Nếu pipeline chưa chạy, bạn có thể nhấp vào \u0026ldquo;Release change\u0026rdquo; để kích hoạt pipeline thủ công Bước 2: Kích hoạt pipeline thủ công Trong danh sách pipelines, chọn sdlf-foundations-pipeline Nhấp vào nút \u0026ldquo;Release change\u0026rdquo; ở góc trên bên phải Xác nhận bằng cách nhấp vào \u0026ldquo;Release\u0026rdquo; Pipeline sẽ bắt đầu chạy, bắt đầu từ stage Source Lặp lại các bước trên cho sdlf-team-pipeline và sdlf-dataset-pipeline Bước 3: Theo dõi tiến trình pipeline Nhấp vào pipeline sdlf-foundations-pipeline để xem chi tiết Bạn sẽ thấy tiến trình của từng stage: Source: Lấy mã nguồn từ CodeCommit Build: Build và triển khai mã nguồn Nhấp vào \u0026ldquo;Details\u0026rdquo; trong stage Build để xem logs của CodeBuild Đợi cho đến khi tất cả các stages chuyển sang trạng thái \u0026ldquo;Succeeded\u0026rdquo; (màu xanh lá) Bước 4: Kiểm tra kết quả triển khai Sau khi pipeline hoàn thành, kiểm tra xem các tài nguyên đã được triển khai thành công chưa Trong CloudShell, nhập lệnh sau để kiểm tra CloudFormation stacks: aws cloudformation list-stacks \\ --stack-status-filter CREATE_COMPLETE UPDATE_COMPLETE \\ --query \u0026#34;StackSummaries[?contains(StackName, \u0026#39;sdlf\u0026#39;)].StackName\u0026#34; Nhấn Enter để thực hiện lệnh Bạn sẽ thấy danh sách các stacks đã được triển khai Bước 5: Thực hiện thay đổi để kiểm tra tự động hóa Trong CloudShell, nhập các lệnh sau để thực hiện thay đổi trong repository: cd ~/environment/sdlf-repos/sdlf-foundations # Thêm một file mới echo \u0026#34;# SDLF Workshop\u0026#34; \u0026gt; README.md # Commit và push thay đổi git add README.md git commit -m \u0026#34;Add README file\u0026#34; git push Nhấn Enter để thực hiện từng lệnh Bước 6: Kiểm tra pipeline tự động chạy Quay lại AWS Management Console, trang CodePipeline Chọn sdlf-foundations-pipeline Bạn sẽ thấy pipeline đã tự động bắt đầu chạy sau khi có thay đổi trong repository Đợi cho đến khi pipeline hoàn thành Bước 7: Kiểm tra lịch sử thực thi Trong trang chi tiết của pipeline, nhấp vào tab \u0026ldquo;History\u0026rdquo; Bạn sẽ thấy lịch sử các lần thực thi của pipeline, bao gồm cả lần thực thi tự động vừa rồi Nhấp vào một execution để xem chi tiết Xử lý sự cố Vấn đề: Pipeline không tự động chạy khi có thay đổi Giải pháp:\nKiểm tra cấu hình source stage của pipeline Đảm bảo webhook đã được thiết lập đúng Thử kích hoạt pipeline thủ công Vấn đề: Build stage fails Giải pháp:\nKiểm tra logs của CodeBuild để xác định lỗi Đảm bảo buildspec.yml đã được cấu hình đúng Kiểm tra IAM permissions của CodeBuild role Bạn có thể sử dụng CloudWatch Logs để xem logs chi tiết của CodeBuild. Nhấp vào \u0026ldquo;Details\u0026rdquo; trong stage Build và sau đó nhấp vào \u0026ldquo;View logs in CloudWatch\u0026rdquo;.\nTổng kết Chúc mừng! Bạn đã thiết lập thành công CI/CD pipeline cho SDLF, bao gồm:\nTạo CodeCommit repositories để lưu trữ mã nguồn Cấu hình CodeBuild để build và test mã nguồn Thiết lập CodePipeline để tự động hóa quy trình CI/CD Kiểm tra pipeline hoạt động như mong đợi Với CI/CD pipeline này, bạn có thể dễ dàng triển khai và cập nhật các thành phần của SDLF mỗi khi có thay đổi trong mã nguồn.\nBước tiếp theo Tiếp theo, chúng ta sẽ Đăng ký Team và Dataset để thiết lập cấu trúc dữ liệu và quyền truy cập.\n"
},
{
	"uri": "/vi/7-athena-query/4-create-views/",
	"title": "Tạo và sử dụng Views",
	"tags": [],
	"description": "",
	"content": "Tạo và sử dụng Views Views trong Athena giúp đơn giản hóa các truy vấn phức tạp và cho phép tái sử dụng logic truy vấn. Views cũng có thể được sử dụng để giới hạn quyền truy cập vào dữ liệu nhạy cảm.\nCác bước thực hiện Bước 1: Mở Amazon Athena Console Đăng nhập vào AWS Management Console Tìm kiếm \u0026ldquo;Athena\u0026rdquo; trong thanh tìm kiếm Chọn \u0026ldquo;Amazon Athena\u0026rdquo; từ kết quả tìm kiếm Đảm bảo bạn đang sử dụng workgroup \u0026ldquo;analytics-team-workgroup\u0026rdquo; Bước 2: Tạo view cơ bản Trong Query editor, nhập truy vấn SQL sau để tạo view cơ bản: CREATE OR REPLACE VIEW \u0026#34;team_a_db\u0026#34;.\u0026#34;transactions_view\u0026#34; AS SELECT transaction_id, customer_id, amount, currency, transaction_date, merchant_category FROM \u0026#34;team_a_db\u0026#34;.\u0026#34;transactions_YYYYMMDD_HHMMSS\u0026#34;; (Thay \u0026ldquo;transactions_YYYYMMDD_HHMMSS\u0026rdquo; bằng tên table thực tế)\nNhấp vào \u0026ldquo;Run query\u0026rdquo; hoặc nhấn Ctrl+Enter Bước 3: Xác nhận view đã được tạo Trong danh sách tables bên trái, nhấp vào biểu tượng làm mới Xác nhận \u0026ldquo;transactions_view\u0026rdquo; xuất hiện trong danh sách Mở rộng view để xem schema Bước 4: Truy vấn view Trong Query editor, nhập truy vấn SQL sau: SELECT * FROM \u0026#34;team_a_db\u0026#34;.\u0026#34;transactions_view\u0026#34; LIMIT 10; Nhấp vào \u0026ldquo;Run query\u0026rdquo; hoặc nhấn Ctrl+Enter Xem kết quả truy vấn ở phần dưới màn hình Bước 5: Tạo view tổng hợp Trong Query editor, nhập truy vấn SQL sau để tạo view tổng hợp: CREATE OR REPLACE VIEW \u0026#34;team_a_db\u0026#34;.\u0026#34;daily_transactions_summary\u0026#34; AS SELECT DATE(transaction_date) as tx_date, merchant_category, COUNT(*) as transaction_count, SUM(amount) as total_amount, AVG(amount) as avg_amount FROM \u0026#34;team_a_db\u0026#34;.\u0026#34;transactions_YYYYMMDD_HHMMSS\u0026#34; GROUP BY DATE(transaction_date), merchant_category; (Thay \u0026ldquo;transactions_YYYYMMDD_HHMMSS\u0026rdquo; bằng tên table thực tế)\nNhấp vào \u0026ldquo;Run query\u0026rdquo; hoặc nhấn Ctrl+Enter Bước 6: Truy vấn view tổng hợp Trong Query editor, nhập truy vấn SQL sau: SELECT * FROM \u0026#34;team_a_db\u0026#34;.\u0026#34;daily_transactions_summary\u0026#34; ORDER BY tx_date DESC, total_amount DESC LIMIT 20; Nhấp vào \u0026ldquo;Run query\u0026rdquo; hoặc nhấn Ctrl+Enter Xem kết quả truy vấn ở phần dưới màn hình Bước 7: Tạo view cho phân tích khách hàng Trong Query editor, nhập truy vấn SQL sau: CREATE OR REPLACE VIEW \u0026#34;team_a_db\u0026#34;.\u0026#34;customer_spending_analysis\u0026#34; AS SELECT customer_id, COUNT(*) as transaction_count, SUM(amount) as total_spent, AVG(amount) as avg_transaction, MIN(transaction_date) as first_transaction, MAX(transaction_date) as last_transaction FROM \u0026#34;team_a_db\u0026#34;.\u0026#34;transactions_YYYYMMDD_HHMMSS\u0026#34; GROUP BY customer_id; (Thay \u0026ldquo;transactions_YYYYMMDD_HHMMSS\u0026rdquo; bằng tên table thực tế)\nNhấp vào \u0026ldquo;Run query\u0026rdquo; hoặc nhấn Ctrl+Enter Bước 8: Truy vấn view phân tích khách hàng Trong Query editor, nhập truy vấn SQL sau: SELECT * FROM \u0026#34;team_a_db\u0026#34;.\u0026#34;customer_spending_analysis\u0026#34; ORDER BY total_spent DESC LIMIT 10; Nhấp vào \u0026ldquo;Run query\u0026rdquo; hoặc nhấn Ctrl+Enter Xem kết quả truy vấn ở phần dưới màn hình Bước 9: Xem thông tin về view Trong Query editor, nhập truy vấn SQL sau: SHOW CREATE VIEW \u0026#34;team_a_db\u0026#34;.\u0026#34;transactions_view\u0026#34;; Nhấp vào \u0026ldquo;Run query\u0026rdquo; hoặc nhấn Ctrl+Enter Xem định nghĩa SQL của view Lợi ích của việc sử dụng Views Đơn giản hóa truy vấn phức tạp: Ẩn logic phức tạp đằng sau view đơn giản Tái sử dụng logic: Định nghĩa logic một lần và sử dụng nhiều lần Bảo mật dữ liệu: Giới hạn quyền truy cập vào các cột nhạy cảm Chuẩn hóa dữ liệu: Đảm bảo tính nhất quán trong cách dữ liệu được truy vấn Tối ưu hiệu suất: Giảm lượng dữ liệu được quét bằng cách chỉ chọn các cột cần thiết Xử lý sự cố Vấn đề: Lỗi \u0026ldquo;View already exists\u0026rdquo; Giải pháp:\nSử dụng CREATE OR REPLACE VIEW thay vì CREATE VIEW Hoặc xóa view hiện có trước bằng DROP VIEW Vấn đề: Lỗi khi truy vấn view Giải pháp:\nKiểm tra định nghĩa view bằng SHOW CREATE VIEW Đảm bảo table gốc vẫn tồn tại và có thể truy cập Kiểm tra quyền truy cập vào view và table gốc Views trong Athena là metadata-only và không lưu trữ dữ liệu. Mỗi khi bạn truy vấn một view, Athena sẽ thực hiện truy vấn định nghĩa view trên dữ liệu gốc.\nBước tiếp theo Tiếp theo, chúng ta sẽ Export kết quả truy vấn để lưu và chia sẻ kết quả phân tích.\n"
},
{
	"uri": "/vi/7-athena-query/5-export-results/",
	"title": "Export kết quả truy vấn",
	"tags": [],
	"description": "",
	"content": "Export kết quả truy vấn Sau khi thực hiện các truy vấn phân tích, bạn có thể muốn lưu kết quả để chia sẻ với người khác hoặc sử dụng trong các công cụ phân tích khác. Amazon Athena cho phép bạn export kết quả truy vấn theo nhiều định dạng khác nhau.\nCác bước thực hiện Bước 1: Thực hiện truy vấn để export Mở Amazon Athena Console Đảm bảo bạn đang sử dụng workgroup \u0026ldquo;analytics-team-workgroup\u0026rdquo; Trong Query editor, nhập một truy vấn có kết quả bạn muốn export, ví dụ: SELECT * FROM \u0026#34;team_a_db\u0026#34;.\u0026#34;daily_transactions_summary\u0026#34; ORDER BY tx_date DESC, total_amount DESC LIMIT 100; Nhấp vào \u0026ldquo;Run query\u0026rdquo; hoặc nhấn Ctrl+Enter Đợi truy vấn hoàn thành và hiển thị kết quả Bước 2: Export kết quả dưới dạng CSV Sau khi truy vấn hoàn thành, nhấp vào biểu tượng \u0026ldquo;Download results\u0026rdquo; (hình tải xuống) ở góc trên bên phải của bảng kết quả Chọn \u0026ldquo;CSV\u0026rdquo; File CSV sẽ được tải xuống máy tính của bạn Bước 3: Export kết quả dưới dạng JSON Sau khi truy vấn hoàn thành, nhấp vào biểu tượng \u0026ldquo;Download results\u0026rdquo; (hình tải xuống) ở góc trên bên phải của bảng kết quả Chọn \u0026ldquo;JSON\u0026rdquo; File JSON sẽ được tải xuống máy tính của bạn Bước 4: Xem kết quả truy vấn trong S3 Mở AWS Management Console và tìm kiếm \u0026ldquo;S3\u0026rdquo; Chọn \u0026ldquo;Amazon S3\u0026rdquo; từ kết quả tìm kiếm Tìm và chọn bucket data lake của bạn (được sử dụng khi thiết lập workgroup) Điều hướng đến thư mục \u0026ldquo;athena-results/analytics-team/\u0026rdquo; Bạn sẽ thấy các file kết quả truy vấn được lưu trữ ở đây, được tổ chức theo ngày Bước 5: Tạo truy vấn lưu kết quả vào bảng mới Trong Query editor, nhập truy vấn SQL sau để lưu kết quả vào bảng mới: CREATE TABLE \u0026#34;team_a_db\u0026#34;.\u0026#34;monthly_transaction_summary\u0026#34; WITH ( format = \u0026#39;PARQUET\u0026#39;, parquet_compression = \u0026#39;SNAPPY\u0026#39;, external_location = \u0026#39;s3://\u0026lt;DATA_LAKE_BUCKET\u0026gt;/analytics/monthly_summary/\u0026#39; ) AS SELECT DATE_TRUNC(\u0026#39;month\u0026#39;, transaction_date) as month, merchant_category, COUNT(*) as transaction_count, SUM(amount) as total_amount, AVG(amount) as avg_amount FROM \u0026#34;team_a_db\u0026#34;.\u0026#34;transactions_YYYYMMDD_HHMMSS\u0026#34; GROUP BY DATE_TRUNC(\u0026#39;month\u0026#39;, transaction_date), merchant_category; (Thay \u0026ldquo;\u0026lt;DATA_LAKE_BUCKET\u0026gt;\u0026rdquo; bằng tên bucket thực tế và \u0026ldquo;transactions_YYYYMMDD_HHMMSS\u0026rdquo; bằng tên table thực tế)\nNhấp vào \u0026ldquo;Run query\u0026rdquo; hoặc nhấn Ctrl+Enter Bước 6: Xác nhận bảng mới đã được tạo Trong danh sách tables bên trái, nhấp vào biểu tượng làm mới Xác nhận \u0026ldquo;monthly_transaction_summary\u0026rdquo; xuất hiện trong danh sách Truy vấn bảng mới để xác nhận dữ liệu: SELECT * FROM \u0026#34;team_a_db\u0026#34;.\u0026#34;monthly_transaction_summary\u0026#34; ORDER BY month DESC, total_amount DESC; Bước 7: Xem dữ liệu Parquet trong S3 Mở AWS Management Console và tìm kiếm \u0026ldquo;S3\u0026rdquo; Chọn \u0026ldquo;Amazon S3\u0026rdquo; từ kết quả tìm kiếm Tìm và chọn bucket data lake của bạn Điều hướng đến thư mục \u0026ldquo;analytics/monthly_summary/\u0026rdquo; Bạn sẽ thấy các file Parquet chứa dữ liệu của bảng mới Bước 8: Thiết lập export tự động vào S3 Trong CloudShell, chạy lệnh sau để tạo một EventBridge rule để lưu tất cả kết quả truy vấn: # Lấy DataLakeBucketName từ CloudFormation outputs DATA_LAKE_BUCKET=$(aws cloudformation describe-stacks \\ --stack-name sdlf-foundations-stack \\ --query \u0026#39;Stacks[0].Outputs[?OutputKey==`DataLakeBucketName`].OutputValue\u0026#39; \\ --output text) # Tạo IAM role cho Athena để ghi kết quả vào S3 aws iam create-role \\ --role-name AthenaQueryResultsExportRole \\ --assume-role-policy-document \u0026#39;{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;athena.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] }\u0026#39; # Tạo policy cho role aws iam put-role-policy \\ --role-name AthenaQueryResultsExportRole \\ --policy-name S3Access \\ --policy-document \u0026#39;{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:ListBucket\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::\u0026#39;$DATA_LAKE_BUCKET\u0026#39;\u0026#34;, \u0026#34;arn:aws:s3:::\u0026#39;$DATA_LAKE_BUCKET\u0026#39;/athena-exports/*\u0026#34; ] } ] }\u0026#39; # Cập nhật cấu hình workgroup để tự động export kết quả aws athena update-work-group \\ --work-group analytics-team-workgroup \\ --configuration \u0026#39;{ \u0026#34;ResultConfiguration\u0026#34;: { \u0026#34;OutputLocation\u0026#34;: \u0026#34;s3://\u0026#39;$DATA_LAKE_BUCKET\u0026#39;/athena-results/analytics-team/\u0026#34;, \u0026#34;EncryptionConfiguration\u0026#34;: { \u0026#34;EncryptionOption\u0026#34;: \u0026#34;SSE_S3\u0026#34; } }, \u0026#34;EnforceWorkGroupConfiguration\u0026#34;: true, \u0026#34;PublishCloudwatchMetricsEnabled\u0026#34;: true, \u0026#34;BytesScannedCutoffPerQuery\u0026#34;: 1073741824 }\u0026#39; Nhấn Enter để thực hiện lệnh Các định dạng export hỗ trợ Amazon Athena hỗ trợ export kết quả truy vấn trong các định dạng sau:\nCSV (Comma-Separated Values):\nPhổ biến và dễ sử dụng với Excel, Google Sheets Tốt cho dữ liệu dạng bảng đơn giản JSON (JavaScript Object Notation):\nTốt cho dữ liệu có cấu trúc phức tạp Dễ dàng phân tích bằng các ngôn ngữ lập trình Parquet:\nĐịnh dạng cột hiệu quả về lưu trữ và hiệu suất Tốt cho phân tích dữ liệu lớn Xử lý sự cố Vấn đề: Không thể tải xuống kết quả truy vấn Giải pháp:\nKiểm tra kích thước kết quả (giới hạn tải xuống là 100MB) Sử dụng LIMIT để giảm kích thước kết quả Lưu kết quả vào bảng mới và truy vấn từng phần Vấn đề: Lỗi khi tạo bảng từ kết quả truy vấn Giải pháp:\nKiểm tra quyền truy cập vào S3 location Đảm bảo đường dẫn S3 không tồn tại hoặc trống Kiểm tra cú pháp SQL của truy vấn CTAS Khi làm việc với dữ liệu lớn, hãy sử dụng định dạng Parquet thay vì CSV hoặc JSON. Parquet tiết kiệm không gian lưu trữ và cải thiện hiệu suất truy vấn đáng kể.\nTổng kết Trong phần này, bạn đã học cách:\nExport kết quả truy vấn Athena dưới dạng CSV và JSON Xem kết quả truy vấn được lưu trữ trong S3 Lưu kết quả truy vấn vào bảng mới với định dạng Parquet Thiết lập export tự động cho workgroup Bạn đã hoàn thành phần truy vấn dữ liệu với Amazon Athena. Giờ đây, bạn có thể sử dụng Athena để phân tích dữ liệu trong data lake và chia sẻ kết quả với team của mình.\nBước tiếp theo Tiếp theo, chúng ta sẽ Giám sát và xử lý sự cố để thiết lập monitoring và alerting cho toàn bộ hệ thống.\n"
},
{
	"uri": "/vi/1-prerequisite/",
	"title": "Các bước chuẩn bị",
	"tags": [],
	"description": "",
	"content": "Tổng quan Trước khi bắt đầu triển khai Serverless Data Lake Framework (SDLF), chúng ta cần chuẩn bị môi trường làm việc trên AWS. Phần này sẽ hướng dẫn bạn đăng nhập vào AWS Management Console, mở AWS CloudShell và chọn region phù hợp.\nTài liệu chính thức về SDLF\nYêu cầu Tài khoản AWS với quyền AdministratorAccess Trình duyệt web hiện đại (Chrome, Firefox, Safari, Edge) Kết nối internet ổn định Các bước thực hiện Đăng nhập vào AWS Management Console Mở AWS CloudShell Chọn region phù hợp Workshop này được thiết kế để thực hiện trên AWS CloudShell, giúp bạn không cần cài đặt bất kỳ công cụ nào trên máy tính cá nhân.\nĐảm bảo bạn sử dụng cùng một region cho toàn bộ workshop để tránh các vấn đề về tương thích và chi phí chuyển dữ liệu giữa các region.\nBước tiếp theo Tiếp theo, chúng ta sẽ Đăng nhập vào AWS Management Console để bắt đầu workshop.\n"
},
{
	"uri": "/vi/2-foundations/",
	"title": "Triển khai SDLF Foundations",
	"tags": [],
	"description": "",
	"content": "Tổng quan SDLF Foundations là cơ sở hạ tầng cơ bản cần thiết để triển khai Serverless Data Lake Framework. Phần này sẽ thiết lập các thành phần cốt lõi của data lake.\nTài liệu chính thức về SDLF Foundations\nCác thành phần chính S3 Buckets: Lưu trữ dữ liệu raw, processed và curated Glue Data Catalog: Quản lý metadata và schema Lake Formation: Quản lý quyền truy cập dữ liệu IAM Roles: Phân quyền cho các dịch vụ KMS Keys: Mã hóa dữ liệu CloudWatch: Giám sát và logging Các bước thực hiện Clone repository SDLF Cấu hình parameters Triển khai CloudFormation Kiểm tra kết quả triển khai Quá trình triển khai SDLF Foundations có thể mất khoảng 15-20 phút. Đây là thời gian tốt để tìm hiểu thêm về kiến trúc của SDLF.\nĐảm bảo bạn đang sử dụng region đã chọn ở phần trước. Việc thay đổi region giữa chừng có thể gây ra lỗi.\nBước tiếp theo Tiếp theo, chúng ta sẽ Clone repository SDLF để bắt đầu triển khai.\n"
},
{
	"uri": "/vi/3-cicd-pipeline/",
	"title": "Thiết lập CI/CD Pipeline",
	"tags": [],
	"description": "",
	"content": "Tổng quan CI/CD Pipeline là thành phần quan trọng để tự động hóa việc triển khai và cập nhật SDLF. Pipeline này sẽ sử dụng AWS CodeCommit, CodeBuild và CodePipeline.\nTài liệu chính thức về SDLF CI/CD\nCác thành phần chính CodeCommit: Repository lưu trữ mã nguồn CodeBuild: Build và test code CodePipeline: Orchestrate quy trình CI/CD CloudFormation: Deploy infrastructure S3 Artifacts: Lưu trữ build artifacts Các bước thực hiện Tạo CodeCommit repositories Cấu hình CodeBuild Thiết lập CodePipeline Kiểm tra pipeline CI/CD Pipeline sẽ giúp tự động hóa việc triển khai các thành phần của SDLF mỗi khi có thay đổi trong mã nguồn.\nĐảm bảo bạn đã hoàn thành phần SDLF Foundations trước khi bắt đầu phần này.\nBước tiếp theo Tiếp theo, chúng ta sẽ Tạo CodeCommit repositories để lưu trữ mã nguồn của SDLF.\n"
},
{
	"uri": "/vi/4-team-dataset/",
	"title": "Đăng ký Team và Dataset",
	"tags": [],
	"description": "",
	"content": "Tổng quan Trong phần này, chúng ta sẽ đăng ký team và dataset trong SDLF để thiết lập cấu trúc dữ liệu và quyền truy cập.\nTài liệu chính thức về SDLF Teams và Datasets\nCác thành phần chính Team: Nhóm làm việc với các quyền truy cập cụ thể Dataset: Bộ dữ liệu được quản lý bởi team Permissions: Quyền truy cập vào dữ liệu Data Catalog: Metadata về dữ liệu Các bước thực hiện Đăng ký team Tạo dataset Thiết lập quyền truy cập Việc đăng ký team và dataset là bước quan trọng để thiết lập cấu trúc dữ liệu và quyền truy cập trong SDLF.\nĐảm bảo bạn đã hoàn thành phần CI/CD Pipeline trước khi bắt đầu phần này.\nBước tiếp theo Tiếp theo, chúng ta sẽ Đăng ký team để bắt đầu thiết lập cấu trúc dữ liệu.\n"
},
{
	"uri": "/vi/5-etl-pipeline/",
	"title": "Triển khai ETL Pipeline",
	"tags": [],
	"description": "",
	"content": "Tổng quan ETL Pipeline là thành phần xử lý và chuyển đổi dữ liệu trong SDLF. Pipeline này sẽ sử dụng AWS Glue để extract, transform và load dữ liệu.\nTài liệu chính thức về SDLF ETL Pipeline\nCác thành phần chính Glue Jobs: Xử lý và chuyển đổi dữ liệu Step Functions: Orchestrate workflow EventBridge: Trigger events S3 Buckets: Lưu trữ dữ liệu raw, stage, analytics Glue Data Catalog: Quản lý metadata Các bước thực hiện Tạo Glue jobs Thiết lập Step Functions Cấu hình EventBridge ETL Pipeline sẽ tự động xử lý dữ liệu khi có dữ liệu mới được upload vào S3 bucket.\nĐảm bảo bạn đã hoàn thành phần Team và Dataset trước khi bắt đầu phần này.\nBước tiếp theo Tiếp theo, chúng ta sẽ Tạo Glue jobs để xử lý và chuyển đổi dữ liệu.\n"
},
{
	"uri": "/vi/6-data-ingestion/",
	"title": "Nạp và xử lý dữ liệu",
	"tags": [],
	"description": "",
	"content": "Tổng quan Trong phần này, chúng ta sẽ nạp dữ liệu vào data lake và theo dõi quá trình xử lý thông qua ETL pipeline đã thiết lập. Đây là một phần quan trọng trong quy trình làm việc với data lake, nơi dữ liệu thô được đưa vào hệ thống và được xử lý qua nhiều giai đoạn để trở thành dữ liệu có giá trị cho phân tích.\nTài liệu chính thức về SDLF Data Ingestion\nQuy trình xử lý dữ liệu trong SDLF Khi dữ liệu được nạp vào data lake, quy trình xử lý sẽ diễn ra như sau:\nRaw Layer: Dữ liệu thô được upload vào S3 bucket trong thư mục /raw EventBridge Rule: Phát hiện sự kiện S3 PutObject và kích hoạt Step Functions state machine Step Functions: Điều phối quy trình ETL thông qua nhiều giai đoạn Stage A: Dữ liệu được xử lý sơ bộ và lưu vào thư mục /stage Stage B: Dữ liệu được xử lý thêm và lưu vào thư mục /analytics Catalog: Dữ liệu được đăng ký vào AWS Glue Data Catalog để sẵn sàng cho truy vấn Các bước thực hiện Chuẩn bị dữ liệu mẫu - Tạo dữ liệu JSON mẫu để nạp vào data lake Upload dữ liệu - Đưa dữ liệu vào S3 và theo dõi quá trình xử lý Khi dữ liệu được upload vào S3 bucket, EventBridge rule sẽ tự động kích hoạt ETL pipeline để xử lý dữ liệu. Bạn không cần phải thực hiện thêm bất kỳ hành động nào để bắt đầu quá trình xử lý.\nĐảm bảo bạn đã hoàn thành phần ETL Pipeline (Phần 5) trước khi bắt đầu phần này. Nếu ETL pipeline chưa được thiết lập đúng cách, quá trình xử lý dữ liệu sẽ không hoạt động.\nLợi ích của quy trình nạp dữ liệu tự động Giảm thời gian xử lý: Dữ liệu được xử lý tự động ngay khi được nạp vào hệ thống Nhất quán: Áp dụng cùng một quy trình xử lý cho tất cả dữ liệu Khả năng mở rộng: Dễ dàng xử lý khối lượng dữ liệu lớn Theo dõi: Có thể theo dõi trạng thái xử lý của từng file dữ liệu Tái sử dụng: Các thành phần xử lý có thể được tái sử dụng cho nhiều loại dữ liệu khác nhau Bước tiếp theo Tiếp theo, chúng ta sẽ Chuẩn bị dữ liệu mẫu để nạp vào data lake.\n"
},
{
	"uri": "/vi/7-athena-query/",
	"title": "Truy vấn dữ liệu với Athena",
	"tags": [],
	"description": "",
	"content": "Tổng quan Amazon Athena là dịch vụ truy vấn tương tác giúp bạn phân tích dữ liệu trực tiếp trong Amazon S3 bằng SQL tiêu chuẩn mà không cần di chuyển dữ liệu. Trong phần này, chúng ta sẽ sử dụng Athena để truy vấn và phân tích dữ liệu đã được xử lý trong data lake.\nTài liệu chính thức về Amazon Athena\nLợi ích của Amazon Athena Serverless: Không cần quản lý infrastructure Pay-per-query: Chỉ trả tiền cho lượng dữ liệu được quét Tương thích với SQL: Sử dụng ANSI SQL tiêu chuẩn Tích hợp với AWS Glue Data Catalog: Tự động phát hiện schema Hiệu suất cao: Sử dụng Presto engine cho truy vấn nhanh Các bước thực hiện Thiết lập Athena Workgroup - Cấu hình môi trường làm việc cho Athena Tạo và chạy Glue Crawler - Phát hiện schema của dữ liệu Thực hiện truy vấn SQL - Truy vấn và phân tích dữ liệu Tạo và sử dụng Views - Đơn giản hóa truy vấn phức tạp Export kết quả truy vấn - Lưu và chia sẻ kết quả phân tích Amazon Athena tính phí dựa trên lượng dữ liệu được quét trong mỗi truy vấn. Để tối ưu chi phí, hãy sử dụng các kỹ thuật như partition pruning, column projection và nén dữ liệu.\nBước tiếp theo Hãy bắt đầu với việc Thiết lập Athena Workgroup để chuẩn bị môi trường truy vấn.\n"
},
{
	"uri": "/vi/8-monitoring/",
	"title": "Giám sát và xử lý sự cố",
	"tags": [],
	"description": "",
	"content": "Thiết lập CloudWatch Dashboard Bước 1: Tạo CloudWatch Dashboard Đăng nhập vào AWS Management Console Tìm kiếm \u0026ldquo;CloudWatch\u0026rdquo; và chọn dịch vụ CloudWatch Trong menu bên trái, chọn \u0026ldquo;Dashboards\u0026rdquo; Nhấp vào \u0026ldquo;Create dashboard\u0026rdquo; Nhập tên dashboard: SDLF-Monitoring-Dashboard Nhấp vào \u0026ldquo;Create dashboard\u0026rdquo; Bước 2: Thêm widget giám sát S3 Trong màn hình \u0026ldquo;Add to this dashboard\u0026rdquo;, chọn \u0026ldquo;Line\u0026rdquo; Tìm kiếm \u0026ldquo;S3\u0026rdquo; trong danh sách metrics Chọn \u0026ldquo;BucketSizeBytes\u0026rdquo; và \u0026ldquo;NumberOfObjects\u0026rdquo; Chọn bucket data lake của bạn Nhấp vào \u0026ldquo;Create widget\u0026rdquo; Bước 3: Thêm widget giám sát Lambda Nhấp vào \u0026ldquo;Add widget\u0026rdquo; Chọn \u0026ldquo;Line\u0026rdquo; Tìm kiếm \u0026ldquo;Lambda\u0026rdquo; trong danh sách metrics Chọn \u0026ldquo;Invocations\u0026rdquo;, \u0026ldquo;Errors\u0026rdquo;, \u0026ldquo;Duration\u0026rdquo; và \u0026ldquo;Throttles\u0026rdquo; Chọn tất cả các hàm Lambda có tên bắt đầu bằng \u0026ldquo;sdlf-\u0026rdquo; Nhấp vào \u0026ldquo;Create widget\u0026rdquo; Bước 4: Thêm widget giám sát Glue Nhấp vào \u0026ldquo;Add widget\u0026rdquo; Chọn \u0026ldquo;Line\u0026rdquo; Tìm kiếm \u0026ldquo;Glue\u0026rdquo; trong danh sách metrics Chọn \u0026ldquo;glue.driver.aggregate.numCompletedTasks\u0026rdquo; và \u0026ldquo;glue.driver.aggregate.numFailedTasks\u0026rdquo; Chọn tất cả các Glue jobs có tên bắt đầu bằng \u0026ldquo;sdlf-\u0026rdquo; Nhấp vào \u0026ldquo;Create widget\u0026rdquo; Bước 5: Thêm widget giám sát Step Functions Nhấp vào \u0026ldquo;Add widget\u0026rdquo; Chọn \u0026ldquo;Line\u0026rdquo; Tìm kiếm \u0026ldquo;Step Functions\u0026rdquo; trong danh sách metrics Chọn \u0026ldquo;ExecutionsFailed\u0026rdquo;, \u0026ldquo;ExecutionsStarted\u0026rdquo; và \u0026ldquo;ExecutionsSucceeded\u0026rdquo; Chọn tất cả các state machines có tên bắt đầu bằng \u0026ldquo;sdlf-\u0026rdquo; Nhấp vào \u0026ldquo;Create widget\u0026rdquo; Bước 6: Thêm widget logs Nhấp vào \u0026ldquo;Add widget\u0026rdquo; Chọn \u0026ldquo;Logs\u0026rdquo; Chọn \u0026ldquo;Create logs widget\u0026rdquo; Trong \u0026ldquo;Select log group(s)\u0026rdquo;, tìm và chọn các log groups có tên bắt đầu bằng \u0026ldquo;/aws/lambda/sdlf-\u0026rdquo; Trong \u0026ldquo;Log query\u0026rdquo;, nhập: fields @timestamp, @message | filter @message like \u0026#34;ERROR\u0026#34; or @message like \u0026#34;FAILED\u0026#34; | sort @timestamp desc | limit 100 Nhấp vào \u0026ldquo;Create widget\u0026rdquo; Bước 7: Lưu dashboard Nhấp vào \u0026ldquo;Save dashboard\u0026rdquo; ở góc trên bên phải Xác nhận lưu dashboard Thiết lập CloudWatch Alarms Bước 1: Tạo alarm cho Lambda errors Trong CloudWatch, chọn \u0026ldquo;Alarms\u0026rdquo; từ menu bên trái Nhấp vào \u0026ldquo;Create alarm\u0026rdquo; Nhấp vào \u0026ldquo;Select metric\u0026rdquo; Tìm kiếm \u0026ldquo;Lambda\u0026rdquo; và chọn \u0026ldquo;By Function Name\u0026rdquo; Chọn metric \u0026ldquo;Errors\u0026rdquo; cho các hàm Lambda có tên bắt đầu bằng \u0026ldquo;sdlf-\u0026rdquo; Nhấp vào \u0026ldquo;Select metric\u0026rdquo; Cấu hình điều kiện: Chọn \u0026ldquo;Static\u0026rdquo; Chọn \u0026ldquo;Greater than or equal to\u0026rdquo; Nhập \u0026ldquo;1\u0026rdquo; cho threshold Cấu hình Actions: Chọn \u0026ldquo;In alarm\u0026rdquo; Chọn \u0026ldquo;Create new topic\u0026rdquo; Nhập tên topic: SDLF-Alerts Nhập email của bạn Nhấp vào \u0026ldquo;Create topic\u0026rdquo; Nhập tên alarm: SDLF-Lambda-Errors Nhấp vào \u0026ldquo;Create alarm\u0026rdquo; Bước 2: Tạo alarm cho Glue job failures Trong CloudWatch, chọn \u0026ldquo;Alarms\u0026rdquo; từ menu bên trái Nhấp vào \u0026ldquo;Create alarm\u0026rdquo; Nhấp vào \u0026ldquo;Select metric\u0026rdquo; Tìm kiếm \u0026ldquo;Glue\u0026rdquo; và chọn \u0026ldquo;Job Metrics\u0026rdquo; Chọn metric \u0026ldquo;glue.driver.aggregate.numFailedTasks\u0026rdquo; cho các Glue jobs có tên bắt đầu bằng \u0026ldquo;sdlf-\u0026rdquo; Nhấp vào \u0026ldquo;Select metric\u0026rdquo; Cấu hình điều kiện: Chọn \u0026ldquo;Static\u0026rdquo; Chọn \u0026ldquo;Greater than or equal to\u0026rdquo; Nhập \u0026ldquo;1\u0026rdquo; cho threshold Cấu hình Actions: Chọn \u0026ldquo;In alarm\u0026rdquo; Chọn topic \u0026ldquo;SDLF-Alerts\u0026rdquo; đã tạo ở bước trước Nhập tên alarm: SDLF-Glue-Failures Nhấp vào \u0026ldquo;Create alarm\u0026rdquo; Bước 3: Tạo alarm cho Step Functions failures Trong CloudWatch, chọn \u0026ldquo;Alarms\u0026rdquo; từ menu bên trái Nhấp vào \u0026ldquo;Create alarm\u0026rdquo; Nhấp vào \u0026ldquo;Select metric\u0026rdquo; Tìm kiếm \u0026ldquo;Step Functions\u0026rdquo; và chọn \u0026ldquo;Execution Metrics\u0026rdquo; Chọn metric \u0026ldquo;ExecutionsFailed\u0026rdquo; cho các state machines có tên bắt đầu bằng \u0026ldquo;sdlf-\u0026rdquo; Nhấp vào \u0026ldquo;Select metric\u0026rdquo; Cấu hình điều kiện: Chọn \u0026ldquo;Static\u0026rdquo; Chọn \u0026ldquo;Greater than or equal to\u0026rdquo; Nhập \u0026ldquo;1\u0026rdquo; cho threshold Cấu hình Actions: Chọn \u0026ldquo;In alarm\u0026rdquo; Chọn topic \u0026ldquo;SDLF-Alerts\u0026rdquo; đã tạo ở bước trước Nhập tên alarm: SDLF-StepFunctions-Failures Nhấp vào \u0026ldquo;Create alarm\u0026rdquo; Thiết lập CloudWatch Logs Insights Bước 1: Tạo truy vấn để phân tích lỗi Lambda Trong CloudWatch, chọn \u0026ldquo;Logs Insights\u0026rdquo; từ menu bên trái Trong \u0026ldquo;Select log group(s)\u0026rdquo;, tìm và chọn các log groups có tên bắt đầu bằng \u0026ldquo;/aws/lambda/sdlf-\u0026rdquo; Trong trường truy vấn, nhập: fields @timestamp, @message | filter @message like \u0026#34;ERROR\u0026#34; or @message like \u0026#34;Exception\u0026#34; | parse @message \u0026#34;ERROR * \u0026#34; as error_message | stats count(*) as error_count by error_message | sort error_count desc Chọn khoảng thời gian phù hợp (ví dụ: 1 giờ qua) Nhấp vào \u0026ldquo;Run query\u0026rdquo; Xem kết quả phân tích lỗi Bước 2: Tạo truy vấn để theo dõi thời gian xử lý Trong CloudWatch, chọn \u0026ldquo;Logs Insights\u0026rdquo; từ menu bên trái Trong \u0026ldquo;Select log group(s)\u0026rdquo;, tìm và chọn các log groups có tên bắt đầu bằng \u0026ldquo;/aws/lambda/sdlf-\u0026rdquo; Trong trường truy vấn, nhập: fields @timestamp, @message | filter @message like \u0026#34;Processing time\u0026#34; | parse @message \u0026#34;Processing time: * ms\u0026#34; as processing_time | stats avg(processing_time) as avg_time, max(processing_time) as max_time by bin(30m) | sort @timestamp desc Chọn khoảng thời gian phù hợp (ví dụ: 24 giờ qua) Nhấp vào \u0026ldquo;Run query\u0026rdquo; Xem kết quả phân tích thời gian xử lý Thiết lập EventBridge Rule cho thông báo Bước 1: Tạo EventBridge Rule Mở AWS Management Console và tìm kiếm \u0026ldquo;EventBridge\u0026rdquo; Chọn \u0026ldquo;Amazon EventBridge\u0026rdquo; từ kết quả tìm kiếm Trong menu bên trái, chọn \u0026ldquo;Rules\u0026rdquo; Nhấp vào \u0026ldquo;Create rule\u0026rdquo; Nhập tên rule: SDLF-Pipeline-Notifications Chọn \u0026ldquo;Rule with an event pattern\u0026rdquo; Nhấp vào \u0026ldquo;Next\u0026rdquo; Trong \u0026ldquo;Event pattern\u0026rdquo;, chọn \u0026ldquo;Custom pattern\u0026rdquo; Nhập pattern sau: { \u0026#34;source\u0026#34;: [\u0026#34;aws.states\u0026#34;], \u0026#34;detail-type\u0026#34;: [\u0026#34;Step Functions Execution Status Change\u0026#34;], \u0026#34;detail\u0026#34;: { \u0026#34;status\u0026#34;: [\u0026#34;FAILED\u0026#34;, \u0026#34;SUCCEEDED\u0026#34;], \u0026#34;stateMachineArn\u0026#34;: [{ \u0026#34;prefix\u0026#34;: \u0026#34;arn:aws:states:YOUR_REGION:YOUR_ACCOUNT_ID:stateMachine:sdlf-\u0026#34; }] } } (Thay YOUR_REGION và YOUR_ACCOUNT_ID bằng giá trị thực tế) Nhấp vào \u0026ldquo;Next\u0026rdquo; Trong \u0026ldquo;Target 1\u0026rdquo;, chọn \u0026ldquo;SNS topic\u0026rdquo; Chọn topic \u0026ldquo;SDLF-Alerts\u0026rdquo; đã tạo trước đó Nhấp vào \u0026ldquo;Next\u0026rdquo; Nhấp vào \u0026ldquo;Next\u0026rdquo; trong màn hình tags Nhấp vào \u0026ldquo;Create rule\u0026rdquo; Xử lý sự cố thường gặp Sự cố 1: Lambda timeout Mở AWS Management Console và tìm kiếm \u0026ldquo;Lambda\u0026rdquo; Chọn hàm Lambda đang gặp sự cố Chọn tab \u0026ldquo;Configuration\u0026rdquo; Chọn \u0026ldquo;General configuration\u0026rdquo; Nhấp vào \u0026ldquo;Edit\u0026rdquo; Tăng giá trị \u0026ldquo;Timeout\u0026rdquo; (ví dụ: từ 3 phút lên 5 phút) Nhấp vào \u0026ldquo;Save\u0026rdquo; Sự cố 2: Glue job fails Mở AWS Management Console và tìm kiếm \u0026ldquo;Glue\u0026rdquo; Chọn \u0026ldquo;Jobs\u0026rdquo; từ menu bên trái Tìm và chọn Glue job đang gặp sự cố Chọn tab \u0026ldquo;History\u0026rdquo; Chọn lần chạy gần nhất bị lỗi Xem \u0026ldquo;Error logs\u0026rdquo; để xác định nguyên nhân Dựa vào lỗi, thực hiện các biện pháp khắc phục: Nếu lỗi \u0026ldquo;Memory limit exceeded\u0026rdquo;: Tăng memory cho job Nếu lỗi \u0026ldquo;Access denied\u0026rdquo;: Kiểm tra IAM permissions Nếu lỗi \u0026ldquo;Schema mismatch\u0026rdquo;: Kiểm tra schema của dữ liệu Sự cố 3: Step Functions execution fails Mở AWS Management Console và tìm kiếm \u0026ldquo;Step Functions\u0026rdquo; Chọn state machine đang gặp sự cố Tìm và chọn execution bị lỗi Xem \u0026ldquo;Execution event history\u0026rdquo; để xác định bước bị lỗi Nhấp vào bước bị lỗi để xem chi tiết Dựa vào lỗi, thực hiện các biện pháp khắc phục: Nếu lỗi từ Lambda: Kiểm tra CloudWatch Logs của Lambda Nếu lỗi từ Glue: Kiểm tra Glue job logs Nếu lỗi từ S3: Kiểm tra quyền truy cập S3 Bước tiếp theo Tiếp theo, chúng ta sẽ Dọn dẹp tài nguyên để tránh phát sinh chi phí không cần thiết.\n"
},
{
	"uri": "/vi/9-cleanup/",
	"title": "Dọn dẹp tài nguyên",
	"tags": [],
	"description": "",
	"content": "Xóa dữ liệu trong S3 buckets Bước 1: Xóa dữ liệu trong S3 buckets Mở AWS Management Console và tìm kiếm \u0026ldquo;S3\u0026rdquo; Chọn \u0026ldquo;Amazon S3\u0026rdquo; từ kết quả tìm kiếm Tìm bucket data lake của bạn (có tên bắt đầu bằng \u0026ldquo;sdlf-\u0026rdquo;) Chạy lệnh sau trong CloudShell để xóa tất cả dữ liệu trong bucket: # Lấy tên bucket từ CloudFormation outputs DATA_LAKE_BUCKET=$(aws cloudformation describe-stacks \\ --stack-name sdlf-foundations-stack \\ --query \u0026#39;Stacks[0].Outputs[?OutputKey==`DataLakeBucketName`].OutputValue\u0026#39; \\ --output text) # Xóa tất cả objects trong bucket echo \u0026#34;Xóa tất cả objects trong bucket $DATA_LAKE_BUCKET\u0026#34; aws s3 rm s3://$DATA_LAKE_BUCKET --recursive # Xóa tất cả objects trong artifact bucket ARTIFACT_BUCKET=$(aws cloudformation describe-stacks \\ --stack-name sdlf-cicd-stack \\ --query \u0026#39;Stacks[0].Outputs[?OutputKey==`ArtifactBucketName`].OutputValue\u0026#39; \\ --output text 2\u0026gt;/dev/null) if [ ! -z \u0026#34;$ARTIFACT_BUCKET\u0026#34; ]; then echo \u0026#34;Xóa tất cả objects trong bucket $ARTIFACT_BUCKET\u0026#34; aws s3 rm s3://$ARTIFACT_BUCKET --recursive fi Xóa CloudWatch Dashboards và Alarms Bước 1: Xóa CloudWatch Dashboards Mở AWS Management Console và tìm kiếm \u0026ldquo;CloudWatch\u0026rdquo; Chọn \u0026ldquo;CloudWatch\u0026rdquo; từ kết quả tìm kiếm Trong menu bên trái, chọn \u0026ldquo;Dashboards\u0026rdquo; Tìm và chọn dashboard \u0026ldquo;SDLF-Monitoring-Dashboard\u0026rdquo; Nhấp vào \u0026ldquo;Actions\u0026rdquo; và chọn \u0026ldquo;Delete\u0026rdquo; Xác nhận xóa dashboard Bước 2: Xóa CloudWatch Alarms Trong CloudWatch, chọn \u0026ldquo;Alarms\u0026rdquo; từ menu bên trái Tìm và chọn các alarms có tên bắt đầu bằng \u0026ldquo;SDLF-\u0026rdquo; (có thể chọn nhiều alarm bằng cách giữ Ctrl và nhấp chuột) Nhấp vào \u0026ldquo;Actions\u0026rdquo; và chọn \u0026ldquo;Delete\u0026rdquo; Xác nhận xóa alarms Bước 3: Xóa SNS Topics Mở AWS Management Console và tìm kiếm \u0026ldquo;SNS\u0026rdquo; Chọn \u0026ldquo;Simple Notification Service\u0026rdquo; từ kết quả tìm kiếm Trong menu bên trái, chọn \u0026ldquo;Topics\u0026rdquo; Tìm và chọn topic \u0026ldquo;SDLF-Alerts\u0026rdquo; Nhấp vào \u0026ldquo;Delete\u0026rdquo; Nhập \u0026ldquo;delete me\u0026rdquo; vào trường xác nhận Nhấp vào \u0026ldquo;Delete\u0026rdquo; Xóa EventBridge Rules Bước 1: Xóa EventBridge Rules Mở AWS Management Console và tìm kiếm \u0026ldquo;EventBridge\u0026rdquo; Chọn \u0026ldquo;Amazon EventBridge\u0026rdquo; từ kết quả tìm kiếm Trong menu bên trái, chọn \u0026ldquo;Rules\u0026rdquo; Tìm và chọn rule \u0026ldquo;SDLF-Pipeline-Notifications\u0026rdquo; Nhấp vào \u0026ldquo;Delete\u0026rdquo; Xác nhận xóa rule Xóa CloudFormation Stacks Bước 1: Xóa ETL Pipeline Stack Mở AWS Management Console và tìm kiếm \u0026ldquo;CloudFormation\u0026rdquo; Chọn \u0026ldquo;CloudFormation\u0026rdquo; từ kết quả tìm kiếm Tìm stack có tên \u0026ldquo;sdlf-team-a-dataset-a-glue-stack\u0026rdquo; hoặc tương tự Chọn stack và nhấp vào \u0026ldquo;Delete\u0026rdquo; Xác nhận xóa stack Đợi stack bị xóa hoàn toàn (trạng thái \u0026ldquo;DELETE_COMPLETE\u0026rdquo;) Bước 2: Xóa Dataset Stack Tìm stack có tên \u0026ldquo;sdlf-team-a-dataset-a-stack\u0026rdquo; hoặc tương tự Chọn stack và nhấp vào \u0026ldquo;Delete\u0026rdquo; Xác nhận xóa stack Đợi stack bị xóa hoàn toàn (trạng thái \u0026ldquo;DELETE_COMPLETE\u0026rdquo;) Bước 3: Xóa Team Stack Tìm stack có tên \u0026ldquo;sdlf-team-a-stack\u0026rdquo; hoặc tương tự Chọn stack và nhấp vào \u0026ldquo;Delete\u0026rdquo; Xác nhận xóa stack Đợi stack bị xóa hoàn toàn (trạng thái \u0026ldquo;DELETE_COMPLETE\u0026rdquo;) Bước 4: Xóa CI/CD Stack Tìm stack có tên \u0026ldquo;sdlf-cicd-stack\u0026rdquo; Chọn stack và nhấp vào \u0026ldquo;Delete\u0026rdquo; Xác nhận xóa stack Đợi stack bị xóa hoàn toàn (trạng thái \u0026ldquo;DELETE_COMPLETE\u0026rdquo;) Bước 5: Xóa Foundations Stack Tìm stack có tên \u0026ldquo;sdlf-foundations-stack\u0026rdquo; Chọn stack và nhấp vào \u0026ldquo;Delete\u0026rdquo; Xác nhận xóa stack Đợi stack bị xóa hoàn toàn (trạng thái \u0026ldquo;DELETE_COMPLETE\u0026rdquo;) Xóa CodeCommit Repositories Bước 1: Xóa CodeCommit Repositories Mở AWS Management Console và tìm kiếm \u0026ldquo;CodeCommit\u0026rdquo; Chọn \u0026ldquo;AWS CodeCommit\u0026rdquo; từ kết quả tìm kiếm Tìm và chọn repository có tên bắt đầu bằng \u0026ldquo;sdlf-\u0026rdquo; Nhấp vào \u0026ldquo;Delete repository\u0026rdquo; Nhập tên repository để xác nhận Nhấp vào \u0026ldquo;Delete\u0026rdquo; Lặp lại cho tất cả các repositories có tên bắt đầu bằng \u0026ldquo;sdlf-\u0026rdquo; Xóa IAM Roles và Policies Bước 1: Xóa IAM Roles Mở AWS Management Console và tìm kiếm \u0026ldquo;IAM\u0026rdquo; Chọn \u0026ldquo;IAM\u0026rdquo; từ kết quả tìm kiếm Trong menu bên trái, chọn \u0026ldquo;Roles\u0026rdquo; Tìm và chọn các roles có tên chứa \u0026ldquo;sdlf\u0026rdquo; hoặc \u0026ldquo;SDLF\u0026rdquo; Nhấp vào \u0026ldquo;Delete\u0026rdquo; Nhập tên role để xác nhận Nhấp vào \u0026ldquo;Delete\u0026rdquo; Lặp lại cho tất cả các roles có tên chứa \u0026ldquo;sdlf\u0026rdquo; hoặc \u0026ldquo;SDLF\u0026rdquo; Bước 2: Xóa IAM Policies Trong IAM, chọn \u0026ldquo;Policies\u0026rdquo; từ menu bên trái Tìm và chọn các policies có tên chứa \u0026ldquo;sdlf\u0026rdquo; hoặc \u0026ldquo;SDLF\u0026rdquo; Nhấp vào \u0026ldquo;Actions\u0026rdquo; và chọn \u0026ldquo;Delete\u0026rdquo; Nhập tên policy để xác nhận Nhấp vào \u0026ldquo;Delete\u0026rdquo; Lặp lại cho tất cả các policies có tên chứa \u0026ldquo;sdlf\u0026rdquo; hoặc \u0026ldquo;SDLF\u0026rdquo; Xóa Glue Databases và Tables Bước 1: Xóa Glue Databases và Tables Mở AWS Management Console và tìm kiếm \u0026ldquo;Glue\u0026rdquo; Chọn \u0026ldquo;AWS Glue\u0026rdquo; từ kết quả tìm kiếm Trong menu bên trái, chọn \u0026ldquo;Databases\u0026rdquo; Tìm và chọn database \u0026ldquo;team_a_db\u0026rdquo; hoặc tương tự Nhấp vào \u0026ldquo;Actions\u0026rdquo; và chọn \u0026ldquo;Delete database\u0026rdquo; Chọn \u0026ldquo;Delete all tables in the database\u0026rdquo; nếu có Nhấp vào \u0026ldquo;Delete\u0026rdquo; Xóa Athena Workgroups Bước 1: Xóa Athena Workgroups Mở AWS Management Console và tìm kiếm \u0026ldquo;Athena\u0026rdquo; Chọn \u0026ldquo;Amazon Athena\u0026rdquo; từ kết quả tìm kiếm Trong menu bên trái, chọn \u0026ldquo;Workgroups\u0026rdquo; Tìm và chọn workgroup \u0026ldquo;analytics-team-workgroup\u0026rdquo; Nhấp vào \u0026ldquo;Actions\u0026rdquo; và chọn \u0026ldquo;Delete workgroup\u0026rdquo; Nhập tên workgroup để xác nhận Nhấp vào \u0026ldquo;Delete\u0026rdquo; Xác nhận tất cả tài nguyên đã được xóa Bước 1: Kiểm tra CloudFormation Stacks Mở AWS Management Console và tìm kiếm \u0026ldquo;CloudFormation\u0026rdquo; Chọn \u0026ldquo;CloudFormation\u0026rdquo; từ kết quả tìm kiếm Xác nhận không còn stack nào có tên bắt đầu bằng \u0026ldquo;sdlf-\u0026rdquo; (trừ những stack có trạng thái \u0026ldquo;DELETE_COMPLETE\u0026rdquo;) Bước 2: Kiểm tra S3 Buckets Mở AWS Management Console và tìm kiếm \u0026ldquo;S3\u0026rdquo; Chọn \u0026ldquo;Amazon S3\u0026rdquo; từ kết quả tìm kiếm Xác nhận không còn bucket nào có tên bắt đầu bằng \u0026ldquo;sdlf-\u0026rdquo; Bước 3: Kiểm tra Lambda Functions Mở AWS Management Console và tìm kiếm \u0026ldquo;Lambda\u0026rdquo; Chọn \u0026ldquo;AWS Lambda\u0026rdquo; từ kết quả tìm kiếm Xác nhận không còn function nào có tên bắt đầu bằng \u0026ldquo;sdlf-\u0026rdquo; Bước 4: Kiểm tra Glue Jobs Mở AWS Management Console và tìm kiếm \u0026ldquo;Glue\u0026rdquo; Chọn \u0026ldquo;AWS Glue\u0026rdquo; từ kết quả tìm kiếm Trong menu bên trái, chọn \u0026ldquo;Jobs\u0026rdquo; Xác nhận không còn job nào có tên bắt đầu bằng \u0026ldquo;sdlf-\u0026rdquo; Script tự động dọn dẹp Nếu bạn muốn tự động hóa quá trình dọn dẹp, bạn có thể sử dụng script sau trong CloudShell:\n#!/bin/bash echo \u0026#34;Bắt đầu quá trình dọn dẹp tài nguyên SDLF...\u0026#34; # Xóa dữ liệu trong S3 buckets echo \u0026#34;Xóa dữ liệu trong S3 buckets...\u0026#34; DATA_LAKE_BUCKET=$(aws cloudformation describe-stacks \\ --stack-name sdlf-foundations-stack \\ --query \u0026#39;Stacks[0].Outputs[?OutputKey==`DataLakeBucketName`].OutputValue\u0026#39; \\ --output text 2\u0026gt;/dev/null) if [ ! -z \u0026#34;$DATA_LAKE_BUCKET\u0026#34; ]; then echo \u0026#34;Xóa tất cả objects trong bucket $DATA_LAKE_BUCKET\u0026#34; aws s3 rm s3://$DATA_LAKE_BUCKET --recursive fi ARTIFACT_BUCKET=$(aws cloudformation describe-stacks \\ --stack-name sdlf-cicd-stack \\ --query \u0026#39;Stacks[0].Outputs[?OutputKey==`ArtifactBucketName`].OutputValue\u0026#39; \\ --output text 2\u0026gt;/dev/null) if [ ! -z \u0026#34;$ARTIFACT_BUCKET\u0026#34; ]; then echo \u0026#34;Xóa tất cả objects trong bucket $ARTIFACT_BUCKET\u0026#34; aws s3 rm s3://$ARTIFACT_BUCKET --recursive fi # Xóa CloudWatch Dashboards echo \u0026#34;Xóa CloudWatch Dashboards...\u0026#34; aws cloudwatch delete-dashboards --dashboard-names \u0026#34;SDLF-Monitoring-Dashboard\u0026#34; # Xóa CloudWatch Alarms echo \u0026#34;Xóa CloudWatch Alarms...\u0026#34; for alarm in $(aws cloudwatch describe-alarms --query \u0026#34;MetricAlarms[?starts_with(AlarmName, \u0026#39;SDLF-\u0026#39;)].AlarmName\u0026#34; --output text); do echo \u0026#34;Xóa alarm $alarm\u0026#34; aws cloudwatch delete-alarms --alarm-names \u0026#34;$alarm\u0026#34; done # Xóa SNS Topics echo \u0026#34;Xóa SNS Topics...\u0026#34; for topic in $(aws sns list-topics --query \u0026#34;Topics[?contains(TopicArn, \u0026#39;SDLF-Alerts\u0026#39;)].TopicArn\u0026#34; --output text); do echo \u0026#34;Xóa topic $topic\u0026#34; aws sns delete-topic --topic-arn \u0026#34;$topic\u0026#34; done # Xóa EventBridge Rules echo \u0026#34;Xóa EventBridge Rules...\u0026#34; for rule in $(aws events list-rules --query \u0026#34;Rules[?Name==\u0026#39;SDLF-Pipeline-Notifications\u0026#39;].Name\u0026#34; --output text); do echo \u0026#34;Xóa rule $rule\u0026#34; aws events remove-targets --rule \u0026#34;$rule\u0026#34; --ids \u0026#34;1\u0026#34; aws events delete-rule --name \u0026#34;$rule\u0026#34; done # Xóa CloudFormation Stacks echo \u0026#34;Xóa CloudFormation Stacks...\u0026#34; # Xóa ETL Pipeline Stacks for stack in $(aws cloudformation list-stacks --stack-status-filter CREATE_COMPLETE UPDATE_COMPLETE --query \u0026#34;StackSummaries[?contains(StackName, \u0026#39;sdlf\u0026#39;) \u0026amp;\u0026amp; contains(StackName, \u0026#39;glue\u0026#39;)].StackName\u0026#34; --output text); do echo \u0026#34;Xóa stack $stack\u0026#34; aws cloudformation delete-stack --stack-name \u0026#34;$stack\u0026#34; done # Đợi ETL Pipeline Stacks bị xóa echo \u0026#34;Đợi ETL Pipeline Stacks bị xóa...\u0026#34; sleep 60 # Xóa Dataset Stacks for stack in $(aws cloudformation list-stacks --stack-status-filter CREATE_COMPLETE UPDATE_COMPLETE --query \u0026#34;StackSummaries[?contains(StackName, \u0026#39;sdlf\u0026#39;) \u0026amp;\u0026amp; contains(StackName, \u0026#39;dataset\u0026#39;)].StackName\u0026#34; --output text); do echo \u0026#34;Xóa stack $stack\u0026#34; aws cloudformation delete-stack --stack-name \u0026#34;$stack\u0026#34; done # Đợi Dataset Stacks bị xóa echo \u0026#34;Đợi Dataset Stacks bị xóa...\u0026#34; sleep 60 # Xóa Team Stacks for stack in $(aws cloudformation list-stacks --stack-status-filter CREATE_COMPLETE UPDATE_COMPLETE --query \u0026#34;StackSummaries[?contains(StackName, \u0026#39;sdlf\u0026#39;) \u0026amp;\u0026amp; contains(StackName, \u0026#39;team\u0026#39;)].StackName\u0026#34; --output text); do echo \u0026#34;Xóa stack $stack\u0026#34; aws cloudformation delete-stack --stack-name \u0026#34;$stack\u0026#34; done # Đợi Team Stacks bị xóa echo \u0026#34;Đợi Team Stacks bị xóa...\u0026#34; sleep 60 # Xóa CI/CD Stack if aws cloudformation describe-stacks --stack-name sdlf-cicd-stack \u0026amp;\u0026gt;/dev/null; then echo \u0026#34;Xóa stack sdlf-cicd-stack\u0026#34; aws cloudformation delete-stack --stack-name sdlf-cicd-stack # Đợi CI/CD Stack bị xóa echo \u0026#34;Đợi CI/CD Stack bị xóa...\u0026#34; sleep 60 fi # Xóa Foundations Stack if aws cloudformation describe-stacks --stack-name sdlf-foundations-stack \u0026amp;\u0026gt;/dev/null; then echo \u0026#34;Xóa stack sdlf-foundations-stack\u0026#34; aws cloudformation delete-stack --stack-name sdlf-foundations-stack fi # Xóa CodeCommit Repositories echo \u0026#34;Xóa CodeCommit Repositories...\u0026#34; for repo in $(aws codecommit list-repositories --query \u0026#34;repositories[?contains(repositoryName, \u0026#39;sdlf\u0026#39;)].repositoryName\u0026#34; --output text); do echo \u0026#34;Xóa repository $repo\u0026#34; aws codecommit delete-repository --repository-name \u0026#34;$repo\u0026#34; done # Xóa Glue Databases echo \u0026#34;Xóa Glue Databases...\u0026#34; for db in $(aws glue get-databases --query \u0026#34;DatabaseList[?contains(Name, \u0026#39;team_\u0026#39;)].Name\u0026#34; --output text); do echo \u0026#34;Xóa database $db\u0026#34; aws glue delete-database --name \u0026#34;$db\u0026#34; done # Xóa Athena Workgroups echo \u0026#34;Xóa Athena Workgroups...\u0026#34; if aws athena get-work-group --work-group analytics-team-workgroup \u0026amp;\u0026gt;/dev/null; then echo \u0026#34;Xóa workgroup analytics-team-workgroup\u0026#34; aws athena delete-work-group --work-group analytics-team-workgroup --recursive-delete-option fi echo \u0026#34;Quá trình dọn dẹp đã hoàn tất!\u0026#34; Để sử dụng script này:\nMở CloudShell Tạo file script: nano cleanup-sdlf.sh Sao chép và dán script vào editor Nhấn Ctrl+X, sau đó Y và Enter để lưu Cấp quyền thực thi cho script: chmod +x cleanup-sdlf.sh Chạy script: ./cleanup-sdlf.sh Script này sẽ xóa tất cả tài nguyên liên quan đến SDLF. Đảm bảo bạn đã sao lưu bất kỳ dữ liệu quan trọng nào trước khi chạy script.\nKết thúc Workshop Chúc mừng! Bạn đã hoàn thành workshop về Serverless Data Lake Framework (SDLF). Trong workshop này, bạn đã:\nThiết lập cơ sở hạ tầng cho data lake serverless Triển khai CI/CD pipeline cho quản lý mã nguồn Đăng ký team và dataset Triển khai ETL pipeline Nạp và xử lý dữ liệu Truy vấn dữ liệu với Athena Thiết lập giám sát và cảnh báo Dọn dẹp tài nguyên Những kiến thức và kỹ năng bạn đã học trong workshop này có thể được áp dụng để xây dựng các data lake serverless cho các dự án thực tế.\n"
},
{
	"uri": "/vi/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/vi/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]